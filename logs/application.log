2025-01-15T08:47:34.156+03:00  INFO 16796 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 16796 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T08:47:34.160+03:00  INFO 16796 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T08:47:34.250+03:00  INFO 16796 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T08:47:34.250+03:00  INFO 16796 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T08:47:35.647+03:00  INFO 16796 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T08:47:35.737+03:00  INFO 16796 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 77 ms. Found 5 JPA repository interfaces.
2025-01-15T08:47:36.673+03:00  INFO 16796 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T08:47:36.695+03:00  INFO 16796 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T08:47:36.696+03:00  INFO 16796 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T08:47:36.763+03:00  INFO 16796 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T08:47:36.763+03:00  INFO 16796 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2512 ms
2025-01-15T08:47:37.013+03:00  INFO 16796 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T08:47:37.082+03:00  INFO 16796 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T08:47:37.148+03:00  INFO 16796 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T08:47:37.567+03:00  INFO 16796 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T08:47:37.601+03:00  INFO 16796 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T08:47:37.969+03:00  INFO 16796 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@354fe22
2025-01-15T08:47:37.971+03:00  INFO 16796 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T08:47:39.089+03:00  INFO 16796 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T08:47:39.093+03:00  INFO 16796 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T08:47:40.532+03:00  WARN 16796 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T08:47:41.242+03:00  INFO 16796 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T08:47:41.327+03:00  INFO 16796 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T08:47:41.374+03:00  INFO 16796 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [${kafka.bootstrap.server}]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T08:47:41.423+03:00  INFO 16796 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T08:47:41.593+03:00  INFO 16796 --- [kafka] [restartedMain] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T08:47:41.593+03:00  INFO 16796 --- [kafka] [restartedMain] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T08:47:41.593+03:00  INFO 16796 --- [kafka] [restartedMain] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T08:47:41.594+03:00  INFO 16796 --- [kafka] [restartedMain] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T08:47:41.595+03:00  INFO 16796 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T08:47:41.612+03:00  WARN 16796 --- [kafka] [restartedMain] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.kafka.config.internalKafkaListenerEndpointRegistry'
2025-01-15T08:47:41.617+03:00  INFO 16796 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T08:47:41.631+03:00  INFO 16796 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T08:47:41.654+03:00  INFO 16796 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T08:47:41.677+03:00  INFO 16796 --- [kafka] [restartedMain] .s.b.a.l.ConditionEvaluationReportLogger : 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2025-01-15T08:47:41.726+03:00 ERROR 16796 --- [kafka] [restartedMain] o.s.boot.SpringApplication               : Application run failed

org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.kafka.config.internalKafkaListenerEndpointRegistry'
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:288) ~[spring-context-6.1.12.jar:6.1.12]
	at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:469) ~[spring-context-6.1.12.jar:6.1.12]
	at java.base/java.lang.Iterable.forEach(Iterable.java:75) ~[na:na]
	at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:257) ~[spring-context-6.1.12.jar:6.1.12]
	at org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:202) ~[spring-context-6.1.12.jar:6.1.12]
	at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:990) ~[spring-context-6.1.12.jar:6.1.12]
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:628) ~[spring-context-6.1.12.jar:6.1.12]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146) ~[spring-boot-3.3.3.jar:3.3.3]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754) ~[spring-boot-3.3.3.jar:3.3.3]
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456) ~[spring-boot-3.3.3.jar:3.3.3]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335) ~[spring-boot-3.3.3.jar:3.3.3]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363) ~[spring-boot-3.3.3.jar:3.3.3]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1352) ~[spring-boot-3.3.3.jar:3.3.3]
	at com.demo.kafka.KafkaApplication.main(KafkaApplication.java:12) ~[classes/:na]
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[na:na]
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50) ~[spring-boot-devtools-3.3.3.jar:3.3.3]
Caused by: org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.<init>(LegacyKafkaConsumer.java:264) ~[kafka-clients-3.7.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.ConsumerDelegateCreator.create(ConsumerDelegateCreator.java:65) ~[kafka-clients-3.7.1.jar:na]
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:600) ~[kafka-clients-3.7.1.jar:na]
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:595) ~[kafka-clients-3.7.1.jar:na]
	at org.springframework.kafka.core.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer.<init>(DefaultKafkaConsumerFactory.java:506) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createRawConsumer(DefaultKafkaConsumerFactory.java:485) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createKafkaConsumer(DefaultKafkaConsumerFactory.java:462) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createConsumerWithAdjustedProperties(DefaultKafkaConsumerFactory.java:439) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createKafkaConsumer(DefaultKafkaConsumerFactory.java:406) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createConsumer(DefaultKafkaConsumerFactory.java:367) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.<init>(KafkaMessageListenerContainer.java:866) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer.doStart(KafkaMessageListenerContainer.java:379) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.AbstractMessageListenerContainer.start(AbstractMessageListenerContainer.java:519) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.ConcurrentMessageListenerContainer.doStart(ConcurrentMessageListenerContainer.java:255) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.AbstractMessageListenerContainer.start(AbstractMessageListenerContainer.java:519) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.config.KafkaListenerEndpointRegistry.startIfNecessary(KafkaListenerEndpointRegistry.java:436) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.config.KafkaListenerEndpointRegistry.start(KafkaListenerEndpointRegistry.java:382) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:285) ~[spring-context-6.1.12.jar:6.1.12]
	... 16 common frames omitted
Caused by: org.apache.kafka.common.config.ConfigException: Invalid url in bootstrap.servers: ${kafka.bootstrap.server}
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:73) ~[kafka-clients-3.7.1.jar:na]
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:62) ~[kafka-clients-3.7.1.jar:na]
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:58) ~[kafka-clients-3.7.1.jar:na]
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.<init>(LegacyKafkaConsumer.java:183) ~[kafka-clients-3.7.1.jar:na]
	... 33 common frames omitted

2025-01-15T08:51:34.752+03:00  INFO 17908 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 17908 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T08:51:34.758+03:00  INFO 17908 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T08:51:34.895+03:00  INFO 17908 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T08:51:34.895+03:00  INFO 17908 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T08:51:36.583+03:00  INFO 17908 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T08:51:36.717+03:00  INFO 17908 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 116 ms. Found 5 JPA repository interfaces.
2025-01-15T08:51:38.202+03:00  INFO 17908 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T08:51:38.233+03:00  INFO 17908 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T08:51:38.234+03:00  INFO 17908 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T08:51:38.328+03:00  INFO 17908 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T08:51:38.329+03:00  INFO 17908 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 3433 ms
2025-01-15T08:51:38.796+03:00  INFO 17908 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T08:51:38.937+03:00  INFO 17908 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T08:51:39.069+03:00  INFO 17908 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T08:51:39.771+03:00  INFO 17908 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T08:51:39.852+03:00  INFO 17908 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T08:51:40.605+03:00  INFO 17908 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@6cdc4bdc
2025-01-15T08:51:40.609+03:00  INFO 17908 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T08:51:42.654+03:00  INFO 17908 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T08:51:42.659+03:00  INFO 17908 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T08:51:44.024+03:00  WARN 17908 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T08:51:45.159+03:00  INFO 17908 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T08:51:45.413+03:00  INFO 17908 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T08:51:45.529+03:00  INFO 17908 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T08:51:45.643+03:00  INFO 17908 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T08:51:46.271+03:00  INFO 17908 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T08:51:46.272+03:00  INFO 17908 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T08:51:46.272+03:00  INFO 17908 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736920306268
2025-01-15T08:51:46.284+03:00  INFO 17908 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T08:51:46.329+03:00  INFO 17908 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 12.747 seconds (process running for 14.353)
2025-01-15T08:51:47.366+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T08:51:47.367+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T08:51:47.373+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T08:51:47.459+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-4396670b-969c-41d5-8395-c407a06f3418
2025-01-15T08:51:47.459+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T08:51:50.464+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=31, memberId='consumer-my-group-1-4396670b-969c-41d5-8395-c407a06f3418', protocol='range'}
2025-01-15T08:51:50.472+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 31: {consumer-my-group-1-4396670b-969c-41d5-8395-c407a06f3418=Assignment(partitions=[test-topic-0])}
2025-01-15T08:51:50.485+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=31, memberId='consumer-my-group-1-4396670b-969c-41d5-8395-c407a06f3418', protocol='range'}
2025-01-15T08:51:50.486+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T08:51:50.489+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T08:51:50.503+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T08:51:50.506+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T09:00:47.577+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T09:01:16.146+03:00  WARN 17908 --- [kafka] [scheduling-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42P01
2025-01-15T09:01:16.146+03:00 ERROR 17908 --- [kafka] [scheduling-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: relation "topics" does not exist
  Position: 48
2025-01-15T09:01:16.157+03:00 ERROR 17908 --- [kafka] [scheduling-1] o.s.s.s.TaskUtils$LoggingErrorHandler    : Unexpected error occurred in scheduled task

org.springframework.dao.InvalidDataAccessResourceUsageException: JDBC exception executing SQL [select t1_0.id,t1_0.description,t1_0.name from topics t1_0] [ERROR: relation "topics" does not exist
  Position: 48] [n/a]; SQL [n/a]
	at org.springframework.orm.jpa.vendor.HibernateJpaDialect.convertHibernateAccessException(HibernateJpaDialect.java:277) ~[spring-orm-6.1.12.jar:6.1.12]
	at org.springframework.orm.jpa.vendor.HibernateJpaDialect.translateExceptionIfPossible(HibernateJpaDialect.java:241) ~[spring-orm-6.1.12.jar:6.1.12]
	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.translateExceptionIfPossible(AbstractEntityManagerFactoryBean.java:550) ~[spring-orm-6.1.12.jar:6.1.12]
	at org.springframework.dao.support.ChainedPersistenceExceptionTranslator.translateExceptionIfPossible(ChainedPersistenceExceptionTranslator.java:61) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.dao.support.DataAccessUtils.translateIfNecessary(DataAccessUtils.java:335) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:160) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.data.jpa.repository.support.CrudMethodMetadataPostProcessor$CrudMethodMetadataPopulatingMethodInterceptor.invoke(CrudMethodMetadataPostProcessor.java:165) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:223) ~[spring-aop-6.1.12.jar:6.1.12]
	at jdk.proxy4/jdk.proxy4.$Proxy130.findAll(Unknown Source) ~[na:na]
	at com.demo.kafka.config.KafkaTopicScheduler.checkAndSubscribeTopics(KafkaTopicScheduler.java:25) ~[classes/:na]
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[na:na]
	at org.springframework.scheduling.support.ScheduledMethodRunnable.runInternal(ScheduledMethodRunnable.java:130) ~[spring-context-6.1.12.jar:6.1.12]
	at org.springframework.scheduling.support.ScheduledMethodRunnable.lambda$run$2(ScheduledMethodRunnable.java:124) ~[spring-context-6.1.12.jar:6.1.12]
	at io.micrometer.observation.Observation.observe(Observation.java:499) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.scheduling.support.ScheduledMethodRunnable.run(ScheduledMethodRunnable.java:124) ~[spring-context-6.1.12.jar:6.1.12]
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54) ~[spring-context-6.1.12.jar:6.1.12]
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[na:na]
	at java.base/java.util.concurrent.FutureTask.runAndReset$$$capture(FutureTask.java:358) ~[na:na]
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java) ~[na:na]
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) ~[na:na]
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[na:na]
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [select t1_0.id,t1_0.description,t1_0.name from topics t1_0] [ERROR: relation "topics" does not exist
  Position: 48] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.DeferredResultSetAccess.executeQuery(DeferredResultSetAccess.java:264) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.DeferredResultSetAccess.getResultSet(DeferredResultSetAccess.java:167) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.JdbcValuesResultSetImpl.advanceNext(JdbcValuesResultSetImpl.java:265) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.JdbcValuesResultSetImpl.processNext(JdbcValuesResultSetImpl.java:145) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.AbstractJdbcValues.next(AbstractJdbcValues.java:19) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.internal.RowProcessingStateStandardImpl.next(RowProcessingStateStandardImpl.java:67) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.spi.ListResultsConsumer.consume(ListResultsConsumer.java:204) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.spi.ListResultsConsumer.consume(ListResultsConsumer.java:33) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.JdbcSelectExecutorStandardImpl.doExecuteQuery(JdbcSelectExecutorStandardImpl.java:211) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.JdbcSelectExecutorStandardImpl.executeQuery(JdbcSelectExecutorStandardImpl.java:83) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.spi.JdbcSelectExecutor.list(JdbcSelectExecutor.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.spi.JdbcSelectExecutor.list(JdbcSelectExecutor.java:65) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.ConcreteSqmSelectQueryPlan.lambda$new$2(ConcreteSqmSelectQueryPlan.java:139) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.ConcreteSqmSelectQueryPlan.withCacheableSqmInterpretation(ConcreteSqmSelectQueryPlan.java:382) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.ConcreteSqmSelectQueryPlan.performList(ConcreteSqmSelectQueryPlan.java:302) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.QuerySqmImpl.doList(QuerySqmImpl.java:526) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractSelectionQuery.list(AbstractSelectionQuery.java:423) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.Query.getResultList(Query.java:120) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.springframework.data.jpa.repository.support.SimpleJpaRepository.findAll(SimpleJpaRepository.java:386) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[na:na]
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:355) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:277) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:170) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:158) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:516) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:628) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:173) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:148) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:70) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:379) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:119) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:138) ~[spring-tx-6.1.12.jar:6.1.12]
	... 22 common frames omitted
Caused by: org.postgresql.util.PSQLException: ERROR: relation "topics" does not exist
  Position: 48
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:137) ~[postgresql-42.7.4.jar:42.7.4]
	at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeQuery(ProxyPreparedStatement.java:52) ~[HikariCP-5.1.0.jar:na]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeQuery(HikariProxyPreparedStatement.java) ~[HikariCP-5.1.0.jar:na]
	at org.hibernate.sql.results.jdbc.internal.DeferredResultSetAccess.executeQuery(DeferredResultSetAccess.java:246) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 59 common frames omitted

2025-01-15T09:01:46.135+03:00  WARN 17908 --- [kafka] [scheduling-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42P01
2025-01-15T09:01:46.135+03:00 ERROR 17908 --- [kafka] [scheduling-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: relation "topics" does not exist
  Position: 48
2025-01-15T09:01:46.137+03:00 ERROR 17908 --- [kafka] [scheduling-1] o.s.s.s.TaskUtils$LoggingErrorHandler    : Unexpected error occurred in scheduled task

org.springframework.dao.InvalidDataAccessResourceUsageException: JDBC exception executing SQL [select t1_0.id,t1_0.description,t1_0.name from topics t1_0] [ERROR: relation "topics" does not exist
  Position: 48] [n/a]; SQL [n/a]
	at org.springframework.orm.jpa.vendor.HibernateJpaDialect.convertHibernateAccessException(HibernateJpaDialect.java:277) ~[spring-orm-6.1.12.jar:6.1.12]
	at org.springframework.orm.jpa.vendor.HibernateJpaDialect.translateExceptionIfPossible(HibernateJpaDialect.java:241) ~[spring-orm-6.1.12.jar:6.1.12]
	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.translateExceptionIfPossible(AbstractEntityManagerFactoryBean.java:550) ~[spring-orm-6.1.12.jar:6.1.12]
	at org.springframework.dao.support.ChainedPersistenceExceptionTranslator.translateExceptionIfPossible(ChainedPersistenceExceptionTranslator.java:61) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.dao.support.DataAccessUtils.translateIfNecessary(DataAccessUtils.java:335) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:160) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.data.jpa.repository.support.CrudMethodMetadataPostProcessor$CrudMethodMetadataPopulatingMethodInterceptor.invoke(CrudMethodMetadataPostProcessor.java:165) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:223) ~[spring-aop-6.1.12.jar:6.1.12]
	at jdk.proxy4/jdk.proxy4.$Proxy130.findAll(Unknown Source) ~[na:na]
	at com.demo.kafka.config.KafkaTopicScheduler.checkAndSubscribeTopics(KafkaTopicScheduler.java:25) ~[classes/:na]
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[na:na]
	at org.springframework.scheduling.support.ScheduledMethodRunnable.runInternal(ScheduledMethodRunnable.java:130) ~[spring-context-6.1.12.jar:6.1.12]
	at org.springframework.scheduling.support.ScheduledMethodRunnable.lambda$run$2(ScheduledMethodRunnable.java:124) ~[spring-context-6.1.12.jar:6.1.12]
	at io.micrometer.observation.Observation.observe(Observation.java:499) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.scheduling.support.ScheduledMethodRunnable.run(ScheduledMethodRunnable.java:124) ~[spring-context-6.1.12.jar:6.1.12]
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54) ~[spring-context-6.1.12.jar:6.1.12]
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[na:na]
	at java.base/java.util.concurrent.FutureTask.runAndReset$$$capture(FutureTask.java:358) ~[na:na]
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java) ~[na:na]
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) ~[na:na]
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[na:na]
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [select t1_0.id,t1_0.description,t1_0.name from topics t1_0] [ERROR: relation "topics" does not exist
  Position: 48] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.DeferredResultSetAccess.executeQuery(DeferredResultSetAccess.java:264) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.DeferredResultSetAccess.getResultSet(DeferredResultSetAccess.java:167) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.JdbcValuesResultSetImpl.advanceNext(JdbcValuesResultSetImpl.java:265) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.JdbcValuesResultSetImpl.processNext(JdbcValuesResultSetImpl.java:145) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.AbstractJdbcValues.next(AbstractJdbcValues.java:19) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.internal.RowProcessingStateStandardImpl.next(RowProcessingStateStandardImpl.java:67) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.spi.ListResultsConsumer.consume(ListResultsConsumer.java:204) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.spi.ListResultsConsumer.consume(ListResultsConsumer.java:33) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.JdbcSelectExecutorStandardImpl.doExecuteQuery(JdbcSelectExecutorStandardImpl.java:211) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.JdbcSelectExecutorStandardImpl.executeQuery(JdbcSelectExecutorStandardImpl.java:83) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.spi.JdbcSelectExecutor.list(JdbcSelectExecutor.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.spi.JdbcSelectExecutor.list(JdbcSelectExecutor.java:65) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.ConcreteSqmSelectQueryPlan.lambda$new$2(ConcreteSqmSelectQueryPlan.java:139) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.ConcreteSqmSelectQueryPlan.withCacheableSqmInterpretation(ConcreteSqmSelectQueryPlan.java:382) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.ConcreteSqmSelectQueryPlan.performList(ConcreteSqmSelectQueryPlan.java:302) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.QuerySqmImpl.doList(QuerySqmImpl.java:526) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractSelectionQuery.list(AbstractSelectionQuery.java:423) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.Query.getResultList(Query.java:120) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.springframework.data.jpa.repository.support.SimpleJpaRepository.findAll(SimpleJpaRepository.java:386) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[na:na]
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:355) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:277) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:170) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:158) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:516) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:628) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:173) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:148) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:70) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:379) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:119) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:138) ~[spring-tx-6.1.12.jar:6.1.12]
	... 22 common frames omitted
Caused by: org.postgresql.util.PSQLException: ERROR: relation "topics" does not exist
  Position: 48
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:137) ~[postgresql-42.7.4.jar:42.7.4]
	at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeQuery(ProxyPreparedStatement.java:52) ~[HikariCP-5.1.0.jar:na]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeQuery(HikariProxyPreparedStatement.java) ~[HikariCP-5.1.0.jar:na]
	at org.hibernate.sql.results.jdbc.internal.DeferredResultSetAccess.executeQuery(DeferredResultSetAccess.java:246) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 59 common frames omitted

2025-01-15T09:07:46.161+03:00  INFO 17908 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T09:07:46.162+03:00  INFO 17908 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:07:46.198+03:00  INFO 17908 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:07:46.198+03:00  INFO 17908 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:07:46.198+03:00  INFO 17908 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736921266198
2025-01-15T09:07:46.199+03:00  INFO 17908 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T09:07:46.234+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:07:46.234+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T09:07:46.238+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T09:07:46.291+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-21faa754-c7aa-47b7-aa58-26007273ae9f
2025-01-15T09:07:46.291+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T09:07:49.321+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=1, memberId='consumer-dynamic-group-2-21faa754-c7aa-47b7-aa58-26007273ae9f', protocol='range'}
2025-01-15T09:07:49.321+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 1: {consumer-dynamic-group-2-21faa754-c7aa-47b7-aa58-26007273ae9f=Assignment(partitions=[test-topic-0])}
2025-01-15T09:07:49.366+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=1, memberId='consumer-dynamic-group-2-21faa754-c7aa-47b7-aa58-26007273ae9f', protocol='range'}
2025-01-15T09:07:49.366+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T09:07:49.367+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T09:07:49.375+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Found no committed offset for partition test-topic-0
2025-01-15T09:07:49.379+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Found no committed offset for partition test-topic-0
2025-01-15T09:07:49.418+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.SubscriptionState    : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Resetting offset for partition test-topic-0 to position FetchPosition{offset=7, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}.
2025-01-15T09:07:49.453+03:00  INFO 17908 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T09:10:47.562+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T09:15:00.819+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T09:15:00.819+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T09:15:00.820+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-4396670b-969c-41d5-8395-c407a06f3418 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T09:15:00.820+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T09:15:00.820+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T09:15:00.820+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T09:15:00.822+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T09:15:00.822+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T09:15:00.834+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T09:15:00.835+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T09:15:00.835+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T09:15:00.835+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T09:15:00.844+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T09:15:00.845+03:00  INFO 17908 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T09:15:00.860+03:00  INFO 17908 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T09:15:00.865+03:00  INFO 17908 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T09:15:00.885+03:00  INFO 17908 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T09:16:29.877+03:00  INFO 10376 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 10376 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T09:16:29.881+03:00  INFO 10376 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T09:16:29.982+03:00  INFO 10376 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T09:16:29.983+03:00  INFO 10376 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T09:16:31.130+03:00  INFO 10376 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T09:16:31.223+03:00  INFO 10376 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 80 ms. Found 5 JPA repository interfaces.
2025-01-15T09:16:32.152+03:00  INFO 10376 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T09:16:32.172+03:00  INFO 10376 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T09:16:32.172+03:00  INFO 10376 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T09:16:32.227+03:00  INFO 10376 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T09:16:32.227+03:00  INFO 10376 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2243 ms
2025-01-15T09:16:32.461+03:00  INFO 10376 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T09:16:32.539+03:00  INFO 10376 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T09:16:32.591+03:00  INFO 10376 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T09:16:32.981+03:00  INFO 10376 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T09:16:33.011+03:00  INFO 10376 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T09:16:33.362+03:00  INFO 10376 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@70eb084e
2025-01-15T09:16:33.364+03:00  INFO 10376 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T09:16:34.443+03:00  INFO 10376 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T09:16:34.446+03:00  INFO 10376 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T09:16:35.277+03:00  WARN 10376 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T09:16:35.877+03:00  INFO 10376 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T09:16:35.987+03:00  INFO 10376 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T09:16:36.039+03:00  INFO 10376 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T09:16:36.131+03:00  INFO 10376 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:16:36.462+03:00  INFO 10376 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:16:36.462+03:00  INFO 10376 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:16:36.462+03:00  INFO 10376 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736921796460
2025-01-15T09:16:36.468+03:00  INFO 10376 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T09:16:36.500+03:00  INFO 10376 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.319 seconds (process running for 8.183)
2025-01-15T09:16:36.724+03:00  INFO 10376 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T09:16:36.726+03:00  INFO 10376 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:16:36.777+03:00  INFO 10376 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:16:36.778+03:00  INFO 10376 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:16:36.778+03:00  INFO 10376 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736921796777
2025-01-15T09:16:36.778+03:00  INFO 10376 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T09:16:37.217+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:16:37.217+03:00  INFO 10376 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:16:37.219+03:00  INFO 10376 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T09:16:37.219+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T09:16:37.224+03:00  INFO 10376 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T09:16:37.224+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T09:16:37.292+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-b9aacc4c-5c0a-4784-b472-30b0f7271276
2025-01-15T09:16:37.293+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T09:16:37.294+03:00  INFO 10376 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-73519bee-5151-44b3-9aa2-196336b0b6f9
2025-01-15T09:16:37.295+03:00  INFO 10376 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T09:16:40.300+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=33, memberId='consumer-my-group-1-b9aacc4c-5c0a-4784-b472-30b0f7271276', protocol='range'}
2025-01-15T09:16:40.301+03:00  INFO 10376 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=3, memberId='consumer-dynamic-group-2-73519bee-5151-44b3-9aa2-196336b0b6f9', protocol='range'}
2025-01-15T09:16:40.309+03:00  INFO 10376 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 3: {consumer-dynamic-group-2-73519bee-5151-44b3-9aa2-196336b0b6f9=Assignment(partitions=[test-topic-0])}
2025-01-15T09:16:40.309+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 33: {consumer-my-group-1-b9aacc4c-5c0a-4784-b472-30b0f7271276=Assignment(partitions=[test-topic-0])}
2025-01-15T09:16:40.324+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=33, memberId='consumer-my-group-1-b9aacc4c-5c0a-4784-b472-30b0f7271276', protocol='range'}
2025-01-15T09:16:40.325+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T09:16:40.325+03:00  INFO 10376 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=3, memberId='consumer-dynamic-group-2-73519bee-5151-44b3-9aa2-196336b0b6f9', protocol='range'}
2025-01-15T09:16:40.325+03:00  INFO 10376 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T09:16:40.328+03:00  INFO 10376 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T09:16:40.328+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T09:16:40.337+03:00  INFO 10376 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T09:16:40.347+03:00  INFO 10376 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T09:16:40.347+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T09:16:40.349+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T09:16:49.632+03:00  INFO 10376 --- [kafka] [http-nio-8080-exec-2] o.apache.coyote.http11.Http11Processor   : Error parsing HTTP request header
 Note: further occurrences of HTTP request parsing errors will be logged at DEBUG level.

java.lang.IllegalArgumentException: Invalid character found in the request target [/api/kafka/send/test-topic?message={%0A%20%20%22payload%22:%20{%0A%20%20%20%20%22op%22:%20%22c%22,%20%0A%20%20%20%20%22before%22:%20null,%0A%20%20%20%20%22after%22:%20{%0A%20%20%20%20%20%20%22id%22:%201,%0A%20%20%20%20%20%20%22name%22:%20%22John%20Doe%22,%0A%20%20%20%20%20%20%22email%22:%20%22john.doe@example.com%22%0A%20%20%20%20},%0A%20%20%20%20%22source%22:%20{%0A%20%20%20%20%20%20%22version%22:%20%222.2.0.Final%22,%0A%20%20%20%20%20%20%22connector%22:%20%22mysql%22,%0A%20%20%20%20%20%20%22name%22:%20%22dbserver1%22,%0A%20%20%20%20%20%20%22ts_ms%22:%201673626800000,%0A%20%20%20%20%20%20%22snapshot%22:%20%22false%22,%0A%20%20%20%20%20%20%22db%22:%20%22exampledb%22,%0A%20%20%20%20%20%20%22sequence%22:%20null,%0A%20%20%20%20%20%20%22table%22:%20%22users%22,%0A%20%20%20%20%20%20%22server_id%22:%20223344,%0A%20%20%20%20%20%20%22gtid%22:%20null,%0A%20%20%20%20%20%20%22file%22:%20%22binlog.000003%22,%0A%20%20%20%20%20%20%22pos%22:%20156,%0A%20%20%20%20%20%20%22row%22:%200,%0A%20%20%20%20%20%20%22thread%22:%20null,%0A%20%20%20%20%20%20%22query%22:%20null%0A%20%20%20%20},%0A%20%20%20%20%22ts_ms%22:%201673626800500,%0A%20%20%20%20%22transaction%22:%20null%0A%20%20}%0A}%0A ]. The valid characters are defined in RFC 7230 and RFC 3986
	at org.apache.coyote.http11.Http11InputBuffer.parseRequestLine(Http11InputBuffer.java:482) ~[tomcat-embed-core-10.1.28.jar:10.1.28]
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:257) ~[tomcat-embed-core-10.1.28.jar:10.1.28]
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63) ~[tomcat-embed-core-10.1.28.jar:10.1.28]
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:904) ~[tomcat-embed-core-10.1.28.jar:10.1.28]
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1741) ~[tomcat-embed-core-10.1.28.jar:10.1.28]
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52) ~[tomcat-embed-core-10.1.28.jar:10.1.28]
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1190) ~[tomcat-embed-core-10.1.28.jar:10.1.28]
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659) ~[tomcat-embed-core-10.1.28.jar:10.1.28]
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:63) ~[tomcat-embed-core-10.1.28.jar:10.1.28]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]

2025-01-15T09:18:18.618+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T09:18:18.619+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T09:18:18.620+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-b9aacc4c-5c0a-4784-b472-30b0f7271276 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T09:18:18.622+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T09:18:18.622+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T09:18:18.622+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T09:18:18.624+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T09:18:18.625+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T09:18:18.717+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T09:18:18.717+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T09:18:18.717+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T09:18:18.717+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T09:18:18.727+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T09:18:18.728+03:00  INFO 10376 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T09:18:18.743+03:00  INFO 10376 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T09:18:18.747+03:00  INFO 10376 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T09:18:18.766+03:00  INFO 10376 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T09:18:22.691+03:00  INFO 8572 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 8572 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T09:18:22.694+03:00  INFO 8572 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T09:18:22.772+03:00  INFO 8572 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T09:18:22.772+03:00  INFO 8572 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T09:18:23.981+03:00  INFO 8572 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T09:18:24.059+03:00  INFO 8572 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 68 ms. Found 5 JPA repository interfaces.
2025-01-15T09:18:25.193+03:00  INFO 8572 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T09:18:25.215+03:00  INFO 8572 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T09:18:25.216+03:00  INFO 8572 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T09:18:25.278+03:00  INFO 8572 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T09:18:25.279+03:00  INFO 8572 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2505 ms
2025-01-15T09:18:25.575+03:00  INFO 8572 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T09:18:25.661+03:00  INFO 8572 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T09:18:25.719+03:00  INFO 8572 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T09:18:26.165+03:00  INFO 8572 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T09:18:26.204+03:00  INFO 8572 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T09:18:26.579+03:00  INFO 8572 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@6cdc4bdc
2025-01-15T09:18:26.582+03:00  INFO 8572 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T09:18:27.770+03:00  INFO 8572 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T09:18:27.775+03:00  INFO 8572 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T09:18:28.698+03:00  WARN 8572 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T09:18:29.366+03:00  INFO 8572 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T09:18:29.465+03:00  INFO 8572 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T09:18:29.511+03:00  INFO 8572 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T09:18:29.560+03:00  INFO 8572 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:18:29.883+03:00  INFO 8572 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:18:29.883+03:00  INFO 8572 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:18:29.883+03:00  INFO 8572 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736921909881
2025-01-15T09:18:29.887+03:00  INFO 8572 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T09:18:29.913+03:00  INFO 8572 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.947 seconds (process running for 8.811)
2025-01-15T09:18:30.147+03:00  INFO 8572 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T09:18:30.148+03:00  INFO 8572 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:18:30.196+03:00  INFO 8572 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:18:30.196+03:00  INFO 8572 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:18:30.196+03:00  INFO 8572 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736921910196
2025-01-15T09:18:30.197+03:00  INFO 8572 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T09:18:30.661+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:18:30.661+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:18:30.662+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T09:18:30.662+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T09:18:30.670+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T09:18:30.670+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T09:18:30.748+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-dbec0987-4302-42fe-8241-85f74239a30f
2025-01-15T09:18:30.748+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-97940fe5-26b7-4f45-9a82-c3057c3a0e7c
2025-01-15T09:18:30.749+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T09:18:30.749+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T09:18:33.754+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=35, memberId='consumer-my-group-1-dbec0987-4302-42fe-8241-85f74239a30f', protocol='range'}
2025-01-15T09:18:33.764+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 35: {consumer-my-group-1-dbec0987-4302-42fe-8241-85f74239a30f=Assignment(partitions=[test-topic-0])}
2025-01-15T09:18:33.781+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=35, memberId='consumer-my-group-1-dbec0987-4302-42fe-8241-85f74239a30f', protocol='range'}
2025-01-15T09:18:33.781+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T09:18:33.784+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T09:18:33.798+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T09:18:33.800+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T09:19:01.335+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=4, memberId='consumer-dynamic-group-2-97940fe5-26b7-4f45-9a82-c3057c3a0e7c', protocol='range'}
2025-01-15T09:19:01.335+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 4: {consumer-dynamic-group-2-97940fe5-26b7-4f45-9a82-c3057c3a0e7c=Assignment(partitions=[test-topic-0])}
2025-01-15T09:19:01.349+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=4, memberId='consumer-dynamic-group-2-97940fe5-26b7-4f45-9a82-c3057c3a0e7c', protocol='range'}
2025-01-15T09:19:01.349+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T09:19:01.349+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T09:19:01.354+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T09:19:01.359+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=7, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T09:19:03.001+03:00  INFO 8572 --- [kafka] [http-nio-8080-exec-2] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T09:19:03.001+03:00  INFO 8572 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T09:19:03.002+03:00  INFO 8572 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Completed initialization in 1 ms
2025-01-15T09:19:03.070+03:00  INFO 8572 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-01-15T09:19:03.072+03:00  INFO 8572 --- [kafka] [http-nio-8080-exec-2] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:19:03.083+03:00  INFO 8572 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Instantiated an idempotent producer.
2025-01-15T09:19:03.127+03:00  INFO 8572 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:19:03.128+03:00  INFO 8572 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:19:03.128+03:00  INFO 8572 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736921943127
2025-01-15T09:19:03.165+03:00  INFO 8572 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=kafka-producer-1] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:19:03.166+03:00  INFO 8572 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=kafka-producer-1] ProducerId set to 3001 with epoch 0
2025-01-15T09:19:03.262+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T09:19:03.561+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T09:19:03.564+03:00  INFO 8572 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T09:19:03.654+03:00  WARN 8572 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T09:19:03.655+03:00 ERROR 8572 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error processing message from topic: test-topic, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}


java.lang.RuntimeException: Veri tabanı bağlantısı başarısız. Lütfen bağlantı ayarlarını kontrol edin.
	at com.demo.kafka.config.DatabaseConnectionUtil.createEntityManager(DatabaseConnectionUtil.java:35) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.performUpdate(DynamicKafkaConsumer.java:128) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:113) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:92) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:56) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment] due to: Unable to load class [com.microsoft.sqlserver.jdbc.SQLServerDriver]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:276) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:238) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.model.relational.Database.<init>(Database.java:45) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.getDatabase(InFlightMetadataCollectorImpl.java:221) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.<init>(InFlightMetadataCollectorImpl.java:189) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:171) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1431) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1502) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.HibernatePersistenceProvider.createEntityManagerFactory(HibernatePersistenceProvider.java:55) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at jakarta.persistence.Persistence.createEntityManagerFactory(Persistence.java:80) ~[jakarta.persistence-api-3.1.0.jar:3.1.0]
	at com.demo.kafka.config.DatabaseConnectionUtil.createEntityManager(DatabaseConnectionUtil.java:28) ~[classes/:na]
	... 18 common frames omitted
Caused by: org.hibernate.boot.registry.classloading.spi.ClassLoadingException: Unable to load class [com.microsoft.sqlserver.jdbc.SQLServerDriver]
	at org.hibernate.boot.registry.classloading.internal.ClassLoaderServiceImpl.classForName(ClassLoaderServiceImpl.java:126) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.loadDriverIfPossible(DriverManagerConnectionProviderImpl.java:211) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.buildCreator(DriverManagerConnectionProviderImpl.java:112) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.buildPool(DriverManagerConnectionProviderImpl.java:93) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.configure(DriverManagerConnectionProviderImpl.java:82) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:136) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:247) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.ServiceRegistry.requireService(ServiceRegistry.java:68) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.buildJdbcConnectionAccess(JdbcEnvironmentInitiator.java:404) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.getJdbcEnvironmentUsingJdbcMetadata(JdbcEnvironmentInitiator.java:276) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:123) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:77) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:130) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 29 common frames omitted
Caused by: java.lang.ClassNotFoundException: Could not load requested class : com.microsoft.sqlserver.jdbc.SQLServerDriver
	at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:216) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592) ~[na:na]
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
	at java.base/java.lang.Class.forName0(Native Method) ~[na:na]
	at java.base/java.lang.Class.forName(Class.java:529) ~[na:na]
	at java.base/java.lang.Class.forName(Class.java:508) ~[na:na]
	at org.hibernate.boot.registry.classloading.internal.ClassLoaderServiceImpl.classForName(ClassLoaderServiceImpl.java:123) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 43 common frames omitted
Caused by: java.lang.Throwable: null
	at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:209) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 49 common frames omitted
	Suppressed: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
		at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) ~[na:na]
		at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) ~[na:na]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:206) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
		... 49 common frames omitted
	Suppressed: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
		at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) ~[na:na]
		at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) ~[na:na]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at java.base/java.lang.Class.forName0(Native Method) ~[na:na]
		at java.base/java.lang.Class.forName(Class.java:529) ~[na:na]
		at java.base/java.lang.Class.forName(Class.java:508) ~[na:na]
		at org.springframework.boot.devtools.restart.classloader.RestartClassLoader.loadClass(RestartClassLoader.java:121) ~[spring-boot-devtools-3.3.3.jar:3.3.3]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:206) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
		... 49 common frames omitted
	Suppressed: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
		at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) ~[na:na]
		at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) ~[na:na]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:206) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
		... 49 common frames omitted

2025-01-15T09:20:36.177+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T09:20:36.178+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T09:20:36.178+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-dbec0987-4302-42fe-8241-85f74239a30f sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T09:20:36.179+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T09:20:36.179+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T09:20:36.179+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T09:20:36.181+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T09:20:36.181+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T09:20:36.493+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T09:20:36.494+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T09:20:36.494+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T09:20:36.494+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T09:20:36.501+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T09:20:36.502+03:00  INFO 8572 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T09:20:36.521+03:00  INFO 8572 --- [kafka] [SpringApplicationShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-15T09:20:36.527+03:00  INFO 8572 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T09:20:36.527+03:00  INFO 8572 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T09:20:36.527+03:00  INFO 8572 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T09:20:36.527+03:00  INFO 8572 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T09:20:36.528+03:00  INFO 8572 --- [kafka] [SpringApplicationShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for kafka-producer-1 unregistered
2025-01-15T09:20:36.531+03:00  INFO 8572 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T09:20:36.536+03:00  INFO 8572 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T09:20:36.558+03:00  INFO 8572 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T09:20:40.333+03:00  INFO 3632 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 3632 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T09:20:40.336+03:00  INFO 3632 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T09:20:40.405+03:00  INFO 3632 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T09:20:40.405+03:00  INFO 3632 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T09:20:41.473+03:00  INFO 3632 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T09:20:41.558+03:00  INFO 3632 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 74 ms. Found 5 JPA repository interfaces.
2025-01-15T09:20:42.447+03:00  INFO 3632 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T09:20:42.469+03:00  INFO 3632 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T09:20:42.469+03:00  INFO 3632 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T09:20:42.532+03:00  INFO 3632 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T09:20:42.532+03:00  INFO 3632 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2126 ms
2025-01-15T09:20:42.781+03:00  INFO 3632 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T09:20:42.851+03:00  INFO 3632 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T09:20:42.900+03:00  INFO 3632 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T09:20:43.260+03:00  INFO 3632 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T09:20:43.291+03:00  INFO 3632 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T09:20:43.627+03:00  INFO 3632 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@17124d64
2025-01-15T09:20:43.630+03:00  INFO 3632 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T09:20:44.689+03:00  INFO 3632 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T09:20:44.693+03:00  INFO 3632 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T09:20:45.509+03:00  WARN 3632 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T09:20:46.108+03:00  INFO 3632 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T09:20:46.204+03:00  INFO 3632 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T09:20:46.249+03:00  INFO 3632 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T09:20:46.308+03:00  INFO 3632 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:20:46.644+03:00  INFO 3632 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:20:46.644+03:00  INFO 3632 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:20:46.644+03:00  INFO 3632 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736922046642
2025-01-15T09:20:46.649+03:00  INFO 3632 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T09:20:46.675+03:00  INFO 3632 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.007 seconds (process running for 7.744)
2025-01-15T09:20:46.882+03:00  INFO 3632 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T09:20:46.884+03:00  INFO 3632 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:20:46.931+03:00  INFO 3632 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:20:46.931+03:00  INFO 3632 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:20:46.931+03:00  INFO 3632 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736922046931
2025-01-15T09:20:46.932+03:00  INFO 3632 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T09:20:47.266+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:20:47.266+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:20:47.267+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T09:20:47.267+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T09:20:47.271+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T09:20:47.271+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T09:20:47.339+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-84599726-f73b-4eed-b0fa-f88f18925808
2025-01-15T09:20:47.339+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T09:20:47.340+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-8e329a3d-5db3-4ef0-829d-8d7fde4bff38
2025-01-15T09:20:47.341+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T09:20:50.345+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=37, memberId='consumer-my-group-1-84599726-f73b-4eed-b0fa-f88f18925808', protocol='range'}
2025-01-15T09:20:50.354+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 37: {consumer-my-group-1-84599726-f73b-4eed-b0fa-f88f18925808=Assignment(partitions=[test-topic-0])}
2025-01-15T09:20:50.368+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=37, memberId='consumer-my-group-1-84599726-f73b-4eed-b0fa-f88f18925808', protocol='range'}
2025-01-15T09:20:50.369+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T09:20:50.371+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T09:20:50.386+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T09:20:50.388+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T09:21:05.297+03:00  INFO 3632 --- [kafka] [http-nio-8080-exec-2] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T09:21:05.298+03:00  INFO 3632 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T09:21:05.299+03:00  INFO 3632 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Completed initialization in 1 ms
2025-01-15T09:21:05.371+03:00  INFO 3632 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-01-15T09:21:05.372+03:00  INFO 3632 --- [kafka] [http-nio-8080-exec-2] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:21:05.383+03:00  INFO 3632 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Instantiated an idempotent producer.
2025-01-15T09:21:05.438+03:00  INFO 3632 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:21:05.439+03:00  INFO 3632 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:21:05.439+03:00  INFO 3632 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736922065438
2025-01-15T09:21:05.480+03:00  INFO 3632 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=kafka-producer-1] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:21:05.480+03:00  INFO 3632 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=kafka-producer-1] ProducerId set to 3002 with epoch 0
2025-01-15T09:21:19.354+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=5, memberId='consumer-dynamic-group-2-8e329a3d-5db3-4ef0-829d-8d7fde4bff38', protocol='range'}
2025-01-15T09:21:19.354+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 5: {consumer-dynamic-group-2-8e329a3d-5db3-4ef0-829d-8d7fde4bff38=Assignment(partitions=[test-topic-0])}
2025-01-15T09:21:19.368+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=5, memberId='consumer-dynamic-group-2-8e329a3d-5db3-4ef0-829d-8d7fde4bff38', protocol='range'}
2025-01-15T09:21:19.368+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T09:21:19.368+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T09:21:19.375+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T09:21:19.380+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T09:21:19.417+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T09:21:19.602+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T09:21:19.605+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T09:21:19.688+03:00  WARN 3632 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T09:21:19.689+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T09:21:19.689+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T09:21:19.689+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T09:21:19.689+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T09:21:19.692+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T09:21:19.879+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T09:21:19.916+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T09:21:19.918+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T09:21:19.942+03:00  WARN 3632 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T09:21:19.942+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T09:21:19.943+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T09:21:19.943+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T09:21:19.943+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T09:21:19.943+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T09:21:20.094+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T09:21:20.098+03:00  INFO 3632 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T09:21:20.758+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T09:21:20.759+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T09:21:20.761+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-84599726-f73b-4eed-b0fa-f88f18925808 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T09:21:20.762+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T09:21:20.762+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T09:21:20.762+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T09:21:20.765+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T09:21:20.765+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T09:21:21.218+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T09:21:21.218+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T09:21:21.219+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T09:21:21.219+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T09:21:21.226+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T09:21:21.227+03:00  INFO 3632 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T09:21:21.237+03:00  INFO 3632 --- [kafka] [SpringApplicationShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-15T09:21:21.244+03:00  INFO 3632 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T09:21:21.244+03:00  INFO 3632 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T09:21:21.244+03:00  INFO 3632 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T09:21:21.244+03:00  INFO 3632 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T09:21:21.245+03:00  INFO 3632 --- [kafka] [SpringApplicationShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for kafka-producer-1 unregistered
2025-01-15T09:21:21.249+03:00  INFO 3632 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T09:21:21.254+03:00  INFO 3632 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T09:21:21.277+03:00  INFO 3632 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T09:21:34.875+03:00  INFO 1984 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 1984 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T09:21:34.878+03:00  INFO 1984 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T09:21:34.951+03:00  INFO 1984 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T09:21:34.952+03:00  INFO 1984 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T09:21:36.104+03:00  INFO 1984 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T09:21:36.179+03:00  INFO 1984 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 66 ms. Found 5 JPA repository interfaces.
2025-01-15T09:21:37.079+03:00  INFO 1984 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T09:21:37.097+03:00  INFO 1984 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T09:21:37.097+03:00  INFO 1984 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T09:21:37.150+03:00  INFO 1984 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T09:21:37.150+03:00  INFO 1984 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2198 ms
2025-01-15T09:21:37.395+03:00  INFO 1984 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T09:21:37.467+03:00  INFO 1984 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T09:21:37.519+03:00  INFO 1984 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T09:21:38.030+03:00  INFO 1984 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T09:21:38.074+03:00  INFO 1984 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T09:21:38.426+03:00  INFO 1984 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@5b06c8e1
2025-01-15T09:21:38.428+03:00  INFO 1984 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T09:21:39.562+03:00  INFO 1984 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T09:21:39.566+03:00  INFO 1984 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T09:21:40.425+03:00  WARN 1984 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T09:21:41.032+03:00  INFO 1984 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T09:21:41.128+03:00  INFO 1984 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T09:21:41.188+03:00  INFO 1984 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T09:21:41.249+03:00  INFO 1984 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:21:41.594+03:00  INFO 1984 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:21:41.595+03:00  INFO 1984 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:21:41.595+03:00  INFO 1984 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736922101593
2025-01-15T09:21:41.602+03:00  INFO 1984 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T09:21:41.642+03:00  INFO 1984 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.462 seconds (process running for 8.259)
2025-01-15T09:21:41.951+03:00  INFO 1984 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T09:21:41.952+03:00  INFO 1984 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:21:42.019+03:00  INFO 1984 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:21:42.020+03:00  INFO 1984 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:21:42.020+03:00  INFO 1984 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736922102019
2025-01-15T09:21:42.025+03:00  INFO 1984 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T09:21:42.419+03:00  INFO 1984 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:21:42.419+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:21:42.420+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T09:21:42.420+03:00  INFO 1984 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T09:21:42.425+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T09:21:42.425+03:00  INFO 1984 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T09:21:42.487+03:00  INFO 1984 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-aa4c5c9c-7eee-4c5d-a16c-da8c576d72ad
2025-01-15T09:21:42.487+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-5c5ff2ef-7f5e-4f32-9773-4185b24fd712
2025-01-15T09:21:42.488+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T09:21:42.488+03:00  INFO 1984 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T09:21:45.494+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=39, memberId='consumer-my-group-1-5c5ff2ef-7f5e-4f32-9773-4185b24fd712', protocol='range'}
2025-01-15T09:21:45.501+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 39: {consumer-my-group-1-5c5ff2ef-7f5e-4f32-9773-4185b24fd712=Assignment(partitions=[test-topic-0])}
2025-01-15T09:21:45.515+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=39, memberId='consumer-my-group-1-5c5ff2ef-7f5e-4f32-9773-4185b24fd712', protocol='range'}
2025-01-15T09:21:45.515+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T09:21:45.517+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T09:21:45.532+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T09:21:45.534+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T09:22:05.104+03:00  INFO 1984 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=6, memberId='consumer-dynamic-group-2-aa4c5c9c-7eee-4c5d-a16c-da8c576d72ad', protocol='range'}
2025-01-15T09:22:05.104+03:00  INFO 1984 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 6: {consumer-dynamic-group-2-aa4c5c9c-7eee-4c5d-a16c-da8c576d72ad=Assignment(partitions=[test-topic-0])}
2025-01-15T09:22:05.117+03:00  INFO 1984 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=6, memberId='consumer-dynamic-group-2-aa4c5c9c-7eee-4c5d-a16c-da8c576d72ad', protocol='range'}
2025-01-15T09:22:05.117+03:00  INFO 1984 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T09:22:05.117+03:00  INFO 1984 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T09:22:05.122+03:00  INFO 1984 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T09:22:05.128+03:00  INFO 1984 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T09:23:58.598+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T09:23:58.599+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T09:23:58.599+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-5c5ff2ef-7f5e-4f32-9773-4185b24fd712 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T09:23:58.601+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T09:23:58.601+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T09:23:58.601+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T09:23:58.604+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T09:23:58.604+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T09:23:58.776+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T09:23:58.776+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T09:23:58.776+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T09:23:58.776+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T09:23:58.784+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T09:23:58.785+03:00  INFO 1984 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T09:23:58.799+03:00  INFO 1984 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T09:23:58.805+03:00  INFO 1984 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T09:23:58.824+03:00  INFO 1984 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T09:24:01.802+03:00  INFO 11180 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 11180 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T09:24:01.806+03:00  INFO 11180 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T09:24:01.874+03:00  INFO 11180 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T09:24:01.874+03:00  INFO 11180 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T09:24:02.952+03:00  INFO 11180 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T09:24:03.037+03:00  INFO 11180 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 73 ms. Found 5 JPA repository interfaces.
2025-01-15T09:24:03.954+03:00  INFO 11180 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T09:24:03.972+03:00  INFO 11180 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T09:24:03.973+03:00  INFO 11180 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T09:24:04.029+03:00  INFO 11180 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T09:24:04.030+03:00  INFO 11180 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2154 ms
2025-01-15T09:24:04.281+03:00  INFO 11180 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T09:24:04.353+03:00  INFO 11180 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T09:24:04.407+03:00  INFO 11180 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T09:24:04.830+03:00  INFO 11180 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T09:24:04.863+03:00  INFO 11180 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T09:24:05.178+03:00  INFO 11180 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@354fe22
2025-01-15T09:24:05.181+03:00  INFO 11180 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T09:24:06.358+03:00  INFO 11180 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T09:24:06.361+03:00  INFO 11180 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T09:24:07.168+03:00  WARN 11180 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T09:24:07.796+03:00  INFO 11180 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T09:24:07.888+03:00  INFO 11180 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T09:24:07.935+03:00  INFO 11180 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T09:24:08.004+03:00  INFO 11180 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:24:08.339+03:00  INFO 11180 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:24:08.339+03:00  INFO 11180 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:24:08.339+03:00  INFO 11180 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736922248337
2025-01-15T09:24:08.343+03:00  INFO 11180 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T09:24:08.368+03:00  INFO 11180 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.209 seconds (process running for 7.969)
2025-01-15T09:24:08.605+03:00  INFO 11180 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T09:24:08.606+03:00  INFO 11180 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:24:08.649+03:00  INFO 11180 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:24:08.649+03:00  INFO 11180 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:24:08.650+03:00  INFO 11180 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736922248649
2025-01-15T09:24:08.650+03:00  INFO 11180 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T09:24:09.001+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:24:09.001+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:24:09.002+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T09:24:09.002+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T09:24:09.007+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T09:24:09.007+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T09:24:09.066+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-2b2c8557-2564-4498-b29a-2770489c250a
2025-01-15T09:24:09.067+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-9338d01d-5663-4aec-a3b6-4897583f553c
2025-01-15T09:24:09.067+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T09:24:09.067+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T09:24:12.073+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=41, memberId='consumer-my-group-1-2b2c8557-2564-4498-b29a-2770489c250a', protocol='range'}
2025-01-15T09:24:12.082+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 41: {consumer-my-group-1-2b2c8557-2564-4498-b29a-2770489c250a=Assignment(partitions=[test-topic-0])}
2025-01-15T09:24:12.095+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=41, memberId='consumer-my-group-1-2b2c8557-2564-4498-b29a-2770489c250a', protocol='range'}
2025-01-15T09:24:12.096+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T09:24:12.098+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T09:24:12.114+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T09:24:12.116+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T09:24:38.310+03:00  INFO 11180 --- [kafka] [http-nio-8080-exec-3] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T09:24:38.310+03:00  INFO 11180 --- [kafka] [http-nio-8080-exec-3] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T09:24:38.312+03:00  INFO 11180 --- [kafka] [http-nio-8080-exec-3] o.s.web.servlet.DispatcherServlet        : Completed initialization in 2 ms
2025-01-15T09:24:38.406+03:00  INFO 11180 --- [kafka] [http-nio-8080-exec-3] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-01-15T09:24:38.408+03:00  INFO 11180 --- [kafka] [http-nio-8080-exec-3] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T09:24:38.423+03:00  INFO 11180 --- [kafka] [http-nio-8080-exec-3] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Instantiated an idempotent producer.
2025-01-15T09:24:38.486+03:00  INFO 11180 --- [kafka] [http-nio-8080-exec-3] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T09:24:38.486+03:00  INFO 11180 --- [kafka] [http-nio-8080-exec-3] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T09:24:38.487+03:00  INFO 11180 --- [kafka] [http-nio-8080-exec-3] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736922278486
2025-01-15T09:24:38.534+03:00  INFO 11180 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=kafka-producer-1] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T09:24:38.536+03:00  INFO 11180 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=kafka-producer-1] ProducerId set to 3003 with epoch 0
2025-01-15T09:24:41.129+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=7, memberId='consumer-dynamic-group-2-9338d01d-5663-4aec-a3b6-4897583f553c', protocol='range'}
2025-01-15T09:24:41.130+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 7: {consumer-dynamic-group-2-9338d01d-5663-4aec-a3b6-4897583f553c=Assignment(partitions=[test-topic-0])}
2025-01-15T09:24:41.142+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=7, memberId='consumer-dynamic-group-2-9338d01d-5663-4aec-a3b6-4897583f553c', protocol='range'}
2025-01-15T09:24:41.142+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T09:24:41.143+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T09:24:41.146+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T09:24:41.149+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T09:24:41.175+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T09:24:41.419+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T09:24:41.422+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T09:24:41.548+03:00  WARN 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T09:24:41.549+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T09:24:41.549+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T09:24:41.550+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T09:24:41.550+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T09:24:41.553+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T09:24:41.759+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T09:24:41.799+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T09:24:41.803+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T09:24:41.839+03:00  WARN 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T09:24:41.839+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T09:24:41.839+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T09:24:41.840+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T09:24:41.840+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T09:24:41.840+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T09:24:41.979+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T09:24:41.984+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T09:33:09.160+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T09:33:09.222+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Node -1 disconnected.
2025-01-15T09:34:08.543+03:00  INFO 11180 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=kafka-producer-1] Node -1 disconnected.
2025-01-15T09:41:37.288+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T09:43:03.796+03:00  WARN 11180 --- [kafka] [HikariPool-1 housekeeper] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Thread starvation or clock leap detected (housekeeper delta=46s130ms592µs400ns).
2025-01-15T09:43:05.942+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T09:43:05.978+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T09:43:06.236+03:00  WARN 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T09:43:06.238+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T09:43:06.238+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T09:43:06.239+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T09:43:06.239+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T09:43:06.240+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T09:43:07.355+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T09:43:12.466+03:00  INFO 11180 --- [kafka] [kafka-coordinator-heartbeat-thread | dynamic-group] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Node -1 disconnected.
2025-01-15T09:43:12.466+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T09:43:53.025+03:00  INFO 11180 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=kafka-producer-1] Node -1 disconnected.
2025-01-15T09:43:53.026+03:00  WARN 11180 --- [kafka] [HikariPool-1 housekeeper] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Thread starvation or clock leap detected (housekeeper delta=49s229ms673µs).
2025-01-15T09:43:53.047+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T09:43:53.084+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T09:43:53.347+03:00  WARN 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T09:43:53.348+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T09:43:53.349+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T09:43:53.350+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T09:43:53.350+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T09:43:53.351+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T09:43:54.520+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T09:43:54.531+03:00  INFO 11180 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T10:11:53.152+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T10:11:53.152+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T10:11:53.152+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-2b2c8557-2564-4498-b29a-2770489c250a sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T10:11:53.153+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T10:11:53.153+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T10:11:53.153+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T10:11:53.155+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T10:11:53.156+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T10:11:53.227+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T10:11:53.227+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T10:11:53.227+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T10:11:53.227+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T10:11:53.233+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T10:11:53.234+03:00  INFO 11180 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T10:11:53.252+03:00  INFO 11180 --- [kafka] [SpringApplicationShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-15T10:11:53.256+03:00  INFO 11180 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T10:11:53.257+03:00  INFO 11180 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T10:11:53.257+03:00  INFO 11180 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T10:11:53.257+03:00  INFO 11180 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T10:11:53.257+03:00  INFO 11180 --- [kafka] [SpringApplicationShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for kafka-producer-1 unregistered
2025-01-15T10:11:53.261+03:00  INFO 11180 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T10:11:53.267+03:00  INFO 11180 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T10:11:53.289+03:00  INFO 11180 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T10:11:58.021+03:00  INFO 14776 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 14776 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T10:11:58.024+03:00  INFO 14776 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T10:11:58.096+03:00  INFO 14776 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T10:11:58.096+03:00  INFO 14776 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T10:11:59.164+03:00  INFO 14776 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T10:11:59.239+03:00  INFO 14776 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 65 ms. Found 5 JPA repository interfaces.
2025-01-15T10:12:00.165+03:00  INFO 14776 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T10:12:00.183+03:00  INFO 14776 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T10:12:00.183+03:00  INFO 14776 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T10:12:00.233+03:00  INFO 14776 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T10:12:00.233+03:00  INFO 14776 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2137 ms
2025-01-15T10:12:00.481+03:00  INFO 14776 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T10:12:00.553+03:00  INFO 14776 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T10:12:00.610+03:00  INFO 14776 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T10:12:01.008+03:00  INFO 14776 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T10:12:01.043+03:00  INFO 14776 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T10:12:01.384+03:00  INFO 14776 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@6cdc4bdc
2025-01-15T10:12:01.387+03:00  INFO 14776 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T10:12:02.539+03:00  INFO 14776 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T10:12:02.544+03:00  INFO 14776 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T10:12:03.726+03:00  WARN 14776 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T10:12:04.332+03:00  INFO 14776 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T10:12:04.426+03:00  INFO 14776 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T10:12:04.490+03:00  INFO 14776 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T10:12:04.561+03:00  INFO 14776 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T10:12:04.923+03:00  INFO 14776 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T10:12:04.923+03:00  INFO 14776 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T10:12:04.923+03:00  INFO 14776 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736925124921
2025-01-15T10:12:04.930+03:00  INFO 14776 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T10:12:04.956+03:00  INFO 14776 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.571 seconds (process running for 8.313)
2025-01-15T10:12:05.184+03:00  INFO 14776 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T10:12:05.185+03:00  INFO 14776 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T10:12:05.232+03:00  INFO 14776 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T10:12:05.232+03:00  INFO 14776 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T10:12:05.232+03:00  INFO 14776 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736925125232
2025-01-15T10:12:05.233+03:00  INFO 14776 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T10:12:05.579+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T10:12:05.579+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T10:12:05.580+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T10:12:05.581+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T10:12:05.587+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T10:12:05.587+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T10:12:05.642+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-4538cf74-93bc-4e20-b9c0-59bb4c6b3270
2025-01-15T10:12:05.643+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T10:12:05.643+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-58fba370-2005-4fe3-b7d6-e8c535a7aa47
2025-01-15T10:12:05.644+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T10:12:08.648+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=43, memberId='consumer-my-group-1-4538cf74-93bc-4e20-b9c0-59bb4c6b3270', protocol='range'}
2025-01-15T10:12:08.656+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 43: {consumer-my-group-1-4538cf74-93bc-4e20-b9c0-59bb4c6b3270=Assignment(partitions=[test-topic-0])}
2025-01-15T10:12:08.669+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=43, memberId='consumer-my-group-1-4538cf74-93bc-4e20-b9c0-59bb4c6b3270', protocol='range'}
2025-01-15T10:12:08.670+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T10:12:08.672+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T10:12:08.688+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=11, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T10:12:08.690+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T10:12:18.668+03:00  INFO 14776 --- [kafka] [http-nio-8080-exec-2] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T10:12:18.668+03:00  INFO 14776 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T10:12:18.669+03:00  INFO 14776 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Completed initialization in 1 ms
2025-01-15T10:12:18.761+03:00  INFO 14776 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-01-15T10:12:18.763+03:00  INFO 14776 --- [kafka] [http-nio-8080-exec-2] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T10:12:18.779+03:00  INFO 14776 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Instantiated an idempotent producer.
2025-01-15T10:12:18.879+03:00  INFO 14776 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T10:12:18.880+03:00  INFO 14776 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T10:12:18.880+03:00  INFO 14776 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736925138879
2025-01-15T10:12:18.929+03:00  INFO 14776 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=kafka-producer-1] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T10:12:18.930+03:00  INFO 14776 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=kafka-producer-1] ProducerId set to 3004 with epoch 0
2025-01-15T10:12:35.438+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=8, memberId='consumer-dynamic-group-2-58fba370-2005-4fe3-b7d6-e8c535a7aa47', protocol='range'}
2025-01-15T10:12:35.439+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 8: {consumer-dynamic-group-2-58fba370-2005-4fe3-b7d6-e8c535a7aa47=Assignment(partitions=[test-topic-0])}
2025-01-15T10:12:35.452+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=8, memberId='consumer-dynamic-group-2-58fba370-2005-4fe3-b7d6-e8c535a7aa47', protocol='range'}
2025-01-15T10:12:35.452+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T10:12:35.452+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T10:12:35.456+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T10:12:35.459+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=11, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T10:12:35.495+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T10:12:35.710+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T10:12:35.713+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T10:12:35.790+03:00  WARN 14776 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T10:12:35.790+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T10:12:35.791+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T10:12:35.791+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T10:12:35.791+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T10:12:35.794+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T10:12:35.948+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T10:12:35.979+03:00  WARN 14776 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42703
2025-01-15T10:12:35.979+03:00 ERROR 14776 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "name" of relation "test_table" does not exist
  Position: 25
2025-01-15T10:12:35.985+03:00 ERROR 14776 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO test_table (name, id, email) VALUES (?, ?, ?)] [ERROR: column "name" of relation "test_table" does not exist
  Position: 25] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:172) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:124) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:97) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:56) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "name" of relation "test_table" does not exist
  Position: 25
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T10:12:35.995+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T10:12:35.996+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T10:12:36.022+03:00  WARN 14776 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T10:12:36.022+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T10:12:36.023+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T10:12:36.023+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T10:12:36.023+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T10:12:36.023+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T10:12:36.156+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T10:12:36.159+03:00  WARN 14776 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42703
2025-01-15T10:12:36.159+03:00 ERROR 14776 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "name" of relation "test_table" does not exist
  Position: 25
2025-01-15T10:12:36.160+03:00 ERROR 14776 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO test_table (name, id, email) VALUES (?, ?, ?)] [ERROR: column "name" of relation "test_table" does not exist
  Position: 25] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:172) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:124) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:97) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:56) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "name" of relation "test_table" does not exist
  Position: 25
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T10:12:36.162+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T10:17:33.983+03:00  INFO 14776 --- [kafka] [http-nio-8080-exec-2] o.springdoc.api.AbstractOpenApiResource  : Init duration for springdoc-openapi is: 835 ms
2025-01-15T10:17:40.838+03:00  WARN 14776 --- [kafka] [http-nio-8080-exec-3] .m.m.a.ExceptionHandlerExceptionResolver : Resolved [org.springframework.http.converter.HttpMessageNotWritableException: Could not write JSON: Document nesting depth (1001) exceeds the maximum allowed (1000, from `StreamWriteConstraints.getMaxNestingDepth()`)]
2025-01-15T10:21:05.754+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T10:21:06.044+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Node -1 disconnected.
2025-01-15T10:21:18.944+03:00  INFO 14776 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=kafka-producer-1] Node -1 disconnected.
2025-01-15T10:31:05.612+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T10:31:05.615+03:00  INFO 14776 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Node -1 disconnected.
2025-01-15T10:31:18.951+03:00  INFO 14776 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=kafka-producer-1] Node -1 disconnected.
2025-01-15T10:40:52.984+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T10:40:52.985+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T10:40:52.986+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-4538cf74-93bc-4e20-b9c0-59bb4c6b3270 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T10:40:52.987+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T10:40:52.987+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T10:40:52.987+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T10:40:52.989+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T10:40:52.990+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T10:40:53.186+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T10:40:53.186+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T10:40:53.186+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T10:40:53.186+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T10:40:53.193+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T10:40:53.194+03:00  INFO 14776 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T10:40:53.216+03:00  INFO 14776 --- [kafka] [SpringApplicationShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-15T10:40:53.222+03:00  INFO 14776 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T10:40:53.222+03:00  INFO 14776 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T10:40:53.222+03:00  INFO 14776 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T10:40:53.222+03:00  INFO 14776 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T10:40:53.224+03:00  INFO 14776 --- [kafka] [SpringApplicationShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for kafka-producer-1 unregistered
2025-01-15T10:40:53.228+03:00  INFO 14776 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T10:40:53.232+03:00  INFO 14776 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T10:40:53.255+03:00  INFO 14776 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T10:51:32.575+03:00  INFO 11520 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 11520 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T10:51:32.579+03:00  INFO 11520 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T10:51:32.662+03:00  INFO 11520 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T10:51:32.663+03:00  INFO 11520 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T10:51:33.840+03:00  INFO 11520 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T10:51:33.917+03:00  INFO 11520 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 68 ms. Found 5 JPA repository interfaces.
2025-01-15T10:51:34.915+03:00  INFO 11520 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T10:51:34.934+03:00  INFO 11520 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T10:51:34.935+03:00  INFO 11520 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T10:51:34.991+03:00  INFO 11520 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T10:51:34.991+03:00  INFO 11520 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2327 ms
2025-01-15T10:51:35.234+03:00  INFO 11520 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T10:51:35.313+03:00  INFO 11520 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T10:51:35.379+03:00  INFO 11520 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T10:51:35.843+03:00  INFO 11520 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T10:51:35.879+03:00  INFO 11520 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T10:51:36.275+03:00  INFO 11520 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@5b06c8e1
2025-01-15T10:51:36.278+03:00  INFO 11520 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T10:51:37.674+03:00  INFO 11520 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T10:51:37.678+03:00  INFO 11520 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T10:51:38.811+03:00  WARN 11520 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T10:51:39.531+03:00  INFO 11520 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T10:51:39.636+03:00  INFO 11520 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T10:51:39.688+03:00  INFO 11520 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T10:51:39.745+03:00  INFO 11520 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T10:51:40.098+03:00  INFO 11520 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T10:51:40.098+03:00  INFO 11520 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T10:51:40.098+03:00  INFO 11520 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736927500096
2025-01-15T10:51:40.104+03:00  INFO 11520 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T10:51:40.133+03:00  INFO 11520 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 8.235 seconds (process running for 9.017)
2025-01-15T10:51:40.351+03:00  INFO 11520 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T10:51:40.351+03:00  INFO 11520 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T10:51:40.396+03:00  INFO 11520 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T10:51:40.396+03:00  INFO 11520 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T10:51:40.396+03:00  INFO 11520 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736927500396
2025-01-15T10:51:40.397+03:00  INFO 11520 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T10:51:40.754+03:00  INFO 11520 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T10:51:40.754+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T10:51:40.755+03:00  INFO 11520 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T10:51:40.755+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T10:51:40.774+03:00  INFO 11520 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T10:51:40.774+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T10:51:40.835+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-c2d9238d-a628-4f10-84e4-277715999bf0
2025-01-15T10:51:40.836+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T10:51:40.839+03:00  INFO 11520 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-89f8b7cd-acb7-40c3-9e9a-10a5285a1660
2025-01-15T10:51:40.840+03:00  INFO 11520 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T10:51:43.842+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=45, memberId='consumer-my-group-1-c2d9238d-a628-4f10-84e4-277715999bf0', protocol='range'}
2025-01-15T10:51:43.845+03:00  INFO 11520 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=10, memberId='consumer-dynamic-group-2-89f8b7cd-acb7-40c3-9e9a-10a5285a1660', protocol='range'}
2025-01-15T10:51:43.853+03:00  INFO 11520 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 10: {consumer-dynamic-group-2-89f8b7cd-acb7-40c3-9e9a-10a5285a1660=Assignment(partitions=[test-topic-0])}
2025-01-15T10:51:43.853+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 45: {consumer-my-group-1-c2d9238d-a628-4f10-84e4-277715999bf0=Assignment(partitions=[test-topic-0])}
2025-01-15T10:51:43.866+03:00  INFO 11520 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=10, memberId='consumer-dynamic-group-2-89f8b7cd-acb7-40c3-9e9a-10a5285a1660', protocol='range'}
2025-01-15T10:51:43.866+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=45, memberId='consumer-my-group-1-c2d9238d-a628-4f10-84e4-277715999bf0', protocol='range'}
2025-01-15T10:51:43.866+03:00  INFO 11520 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T10:51:43.866+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T10:51:43.869+03:00  INFO 11520 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T10:51:43.869+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T10:51:43.876+03:00  INFO 11520 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T10:51:43.884+03:00  INFO 11520 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=12, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T10:51:43.884+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=12, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T10:51:43.886+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T10:51:59.244+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T10:51:59.245+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T10:51:59.246+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-c2d9238d-a628-4f10-84e4-277715999bf0 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T10:51:59.248+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T10:51:59.248+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T10:51:59.248+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T10:51:59.249+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T10:51:59.249+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T10:51:59.589+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T10:51:59.590+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T10:51:59.590+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T10:51:59.590+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T10:51:59.601+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T10:51:59.602+03:00  INFO 11520 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T10:51:59.625+03:00  INFO 11520 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T10:51:59.629+03:00  INFO 11520 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T10:51:59.650+03:00  INFO 11520 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T10:52:02.620+03:00  INFO 17988 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 17988 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T10:52:02.624+03:00  INFO 17988 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T10:52:02.709+03:00  INFO 17988 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T10:52:02.711+03:00  INFO 17988 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T10:52:03.872+03:00  INFO 17988 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T10:52:03.958+03:00  INFO 17988 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 74 ms. Found 5 JPA repository interfaces.
2025-01-15T10:52:04.963+03:00  INFO 17988 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T10:52:04.986+03:00  INFO 17988 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T10:52:04.987+03:00  INFO 17988 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T10:52:05.051+03:00  INFO 17988 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T10:52:05.052+03:00  INFO 17988 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2340 ms
2025-01-15T10:52:05.324+03:00  INFO 17988 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T10:52:05.403+03:00  INFO 17988 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T10:52:05.469+03:00  INFO 17988 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T10:52:05.897+03:00  INFO 17988 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T10:52:05.930+03:00  INFO 17988 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T10:52:06.343+03:00  INFO 17988 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@354fe22
2025-01-15T10:52:06.346+03:00  INFO 17988 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T10:52:07.842+03:00  INFO 17988 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T10:52:07.847+03:00  INFO 17988 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T10:52:08.873+03:00  WARN 17988 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T10:52:09.593+03:00  INFO 17988 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T10:52:09.719+03:00  INFO 17988 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T10:52:09.766+03:00  INFO 17988 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T10:52:09.829+03:00  INFO 17988 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T10:52:10.211+03:00  INFO 17988 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T10:52:10.212+03:00  INFO 17988 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T10:52:10.212+03:00  INFO 17988 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736927530209
2025-01-15T10:52:10.217+03:00  INFO 17988 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T10:52:10.245+03:00  INFO 17988 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 8.321 seconds (process running for 9.064)
2025-01-15T10:52:10.484+03:00  INFO 17988 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T10:52:10.485+03:00  INFO 17988 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T10:52:10.526+03:00  INFO 17988 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T10:52:10.527+03:00  INFO 17988 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T10:52:10.527+03:00  INFO 17988 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736927530526
2025-01-15T10:52:10.527+03:00  INFO 17988 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T10:52:10.961+03:00  INFO 17988 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T10:52:10.961+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T10:52:10.962+03:00  INFO 17988 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T10:52:10.962+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T10:52:10.967+03:00  INFO 17988 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T10:52:10.967+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T10:52:11.039+03:00  INFO 17988 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-ae90ddf0-4f35-4152-bd6f-4796a6dc75cf
2025-01-15T10:52:11.039+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-df737f5b-91dc-405c-bd5a-19760eeb3f42
2025-01-15T10:52:11.041+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T10:52:11.041+03:00  INFO 17988 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T10:52:14.049+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=47, memberId='consumer-my-group-1-df737f5b-91dc-405c-bd5a-19760eeb3f42', protocol='range'}
2025-01-15T10:52:14.057+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 47: {consumer-my-group-1-df737f5b-91dc-405c-bd5a-19760eeb3f42=Assignment(partitions=[test-topic-0])}
2025-01-15T10:52:14.070+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=47, memberId='consumer-my-group-1-df737f5b-91dc-405c-bd5a-19760eeb3f42', protocol='range'}
2025-01-15T10:52:14.071+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T10:52:14.074+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T10:52:14.092+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=12, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T10:52:14.093+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T10:52:22.886+03:00  INFO 17988 --- [kafka] [http-nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T10:52:22.886+03:00  INFO 17988 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T10:52:22.888+03:00  INFO 17988 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 2 ms
2025-01-15T10:52:23.088+03:00  WARN 17988 --- [kafka] [http-nio-8080-exec-1] .m.m.a.ExceptionHandlerExceptionResolver : Resolved [org.springframework.http.converter.HttpMessageNotWritableException: Could not write JSON: Document nesting depth (1001) exceeds the maximum allowed (1000, from `StreamWriteConstraints.getMaxNestingDepth()`)]
2025-01-15T10:52:43.853+03:00  INFO 17988 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=11, memberId='consumer-dynamic-group-2-ae90ddf0-4f35-4152-bd6f-4796a6dc75cf', protocol='range'}
2025-01-15T10:52:43.853+03:00  INFO 17988 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 11: {consumer-dynamic-group-2-ae90ddf0-4f35-4152-bd6f-4796a6dc75cf=Assignment(partitions=[test-topic-0])}
2025-01-15T10:52:43.867+03:00  INFO 17988 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=11, memberId='consumer-dynamic-group-2-ae90ddf0-4f35-4152-bd6f-4796a6dc75cf', protocol='range'}
2025-01-15T10:52:43.868+03:00  INFO 17988 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T10:52:43.868+03:00  INFO 17988 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T10:52:43.876+03:00  INFO 17988 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T10:52:43.885+03:00  INFO 17988 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=12, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T10:56:11.126+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T10:56:11.127+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T10:56:11.128+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-df737f5b-91dc-405c-bd5a-19760eeb3f42 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T10:56:11.128+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T10:56:11.128+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T10:56:11.128+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T10:56:11.130+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T10:56:11.130+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T10:56:11.594+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T10:56:11.595+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T10:56:11.595+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T10:56:11.595+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T10:56:11.603+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T10:56:11.604+03:00  INFO 17988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T10:56:11.625+03:00  INFO 17988 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T10:56:11.632+03:00  INFO 17988 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T10:56:11.654+03:00  INFO 17988 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T10:56:21.645+03:00  INFO 12556 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 12556 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T10:56:21.649+03:00  INFO 12556 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T10:56:21.727+03:00  INFO 12556 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T10:56:21.728+03:00  INFO 12556 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T10:56:22.827+03:00  INFO 12556 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T10:56:22.943+03:00  INFO 12556 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 100 ms. Found 5 JPA repository interfaces.
2025-01-15T10:56:23.919+03:00  INFO 12556 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T10:56:23.947+03:00  INFO 12556 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T10:56:23.947+03:00  INFO 12556 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T10:56:24.025+03:00  INFO 12556 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T10:56:24.026+03:00  INFO 12556 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2297 ms
2025-01-15T10:56:24.294+03:00  INFO 12556 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T10:56:24.375+03:00  INFO 12556 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T10:56:24.441+03:00  INFO 12556 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T10:56:24.882+03:00  INFO 12556 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T10:56:24.927+03:00  INFO 12556 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T10:56:25.314+03:00  INFO 12556 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@7afb93b4
2025-01-15T10:56:25.316+03:00  INFO 12556 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T10:56:26.502+03:00  INFO 12556 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T10:56:26.505+03:00  INFO 12556 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T10:56:27.379+03:00  WARN 12556 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T10:56:28.046+03:00  INFO 12556 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T10:56:28.142+03:00  INFO 12556 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T10:56:28.189+03:00  INFO 12556 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T10:56:28.248+03:00  INFO 12556 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T10:56:28.610+03:00  INFO 12556 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T10:56:28.611+03:00  INFO 12556 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T10:56:28.611+03:00  INFO 12556 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736927788609
2025-01-15T10:56:28.617+03:00  INFO 12556 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T10:56:28.643+03:00  INFO 12556 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.678 seconds (process running for 8.614)
2025-01-15T10:56:28.876+03:00  INFO 12556 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T10:56:28.878+03:00  INFO 12556 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T10:56:28.937+03:00  INFO 12556 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T10:56:28.937+03:00  INFO 12556 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T10:56:28.937+03:00  INFO 12556 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736927788937
2025-01-15T10:56:28.938+03:00  INFO 12556 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T10:56:29.276+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T10:56:29.276+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T10:56:29.277+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T10:56:29.277+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T10:56:29.282+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T10:56:29.282+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T10:56:29.354+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-b89299b0-57a2-4ad6-af60-136dd2862228
2025-01-15T10:56:29.354+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T10:56:29.354+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-ca39ff97-e735-4045-bcbf-508091487508
2025-01-15T10:56:29.355+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T10:56:32.361+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=49, memberId='consumer-my-group-1-ca39ff97-e735-4045-bcbf-508091487508', protocol='range'}
2025-01-15T10:56:32.370+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 49: {consumer-my-group-1-ca39ff97-e735-4045-bcbf-508091487508=Assignment(partitions=[test-topic-0])}
2025-01-15T10:56:32.384+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=49, memberId='consumer-my-group-1-ca39ff97-e735-4045-bcbf-508091487508', protocol='range'}
2025-01-15T10:56:32.385+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T10:56:32.387+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T10:56:32.402+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=12, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T10:56:32.404+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T10:56:55.910+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=12, memberId='consumer-dynamic-group-2-b89299b0-57a2-4ad6-af60-136dd2862228', protocol='range'}
2025-01-15T10:56:55.911+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 12: {consumer-dynamic-group-2-b89299b0-57a2-4ad6-af60-136dd2862228=Assignment(partitions=[test-topic-0])}
2025-01-15T10:56:55.932+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=12, memberId='consumer-dynamic-group-2-b89299b0-57a2-4ad6-af60-136dd2862228', protocol='range'}
2025-01-15T10:56:55.932+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T10:56:55.932+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T10:56:55.936+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T10:56:55.941+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=12, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T10:57:15.651+03:00  INFO 12556 --- [kafka] [http-nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T10:57:15.651+03:00  INFO 12556 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T10:57:15.653+03:00  INFO 12556 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 2 ms
2025-01-15T10:58:53.854+03:00  INFO 12556 --- [kafka] [http-nio-8080-exec-4] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-01-15T10:58:53.855+03:00  INFO 12556 --- [kafka] [http-nio-8080-exec-4] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T10:58:53.864+03:00  INFO 12556 --- [kafka] [http-nio-8080-exec-4] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Instantiated an idempotent producer.
2025-01-15T10:58:53.905+03:00  INFO 12556 --- [kafka] [http-nio-8080-exec-4] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T10:58:53.905+03:00  INFO 12556 --- [kafka] [http-nio-8080-exec-4] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T10:58:53.905+03:00  INFO 12556 --- [kafka] [http-nio-8080-exec-4] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736927933904
2025-01-15T10:58:53.941+03:00  INFO 12556 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=kafka-producer-1] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T10:58:53.942+03:00  INFO 12556 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=kafka-producer-1] ProducerId set to 3005 with epoch 0
2025-01-15T10:58:54.015+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T10:58:54.246+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T10:58:54.251+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T10:58:54.322+03:00  WARN 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T10:58:54.323+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T10:58:54.323+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T10:58:54.323+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T10:58:54.323+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T10:58:54.326+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T10:58:54.524+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T10:58:54.560+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : INSERT Operation - Inserted Rows: 1
2025-01-15T10:58:54.562+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:00:12.794+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "u", 
    "before": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "after": {
      "id": 1,
      "name": "Metehan Altuner",
      "email": "metehan.altuner@gmail.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:00:12.822+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:00:12.824+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:00:12.858+03:00  WARN 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:00:12.858+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T11:00:12.859+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T11:00:12.859+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T11:00:12.859+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T11:00:12.859+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T11:00:13.001+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:00:13.005+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : UPDATE Operation - Updated Rows: 1
2025-01-15T11:00:13.006+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "u", 
    "before": {
      "id": 1,
      "name": "John Doe",
      "email": "john.doe@example.com"
    },
    "after": {
      "id": 1,
      "name": "Metehan Altuner",
      "email": "metehan.altuner@gmail.com"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:00:58.542+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "d", 
    "before": {
      "id": 1,
      "name": "Metehan Altuner",
      "email": "metehan.altuner@gmail.com"
    },
    "after": null,
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:00:58.557+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:00:58.558+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:00:58.586+03:00  WARN 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:00:58.587+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T11:00:58.587+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T11:00:58.587+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T11:00:58.587+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T11:00:58.587+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T11:00:58.792+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:00:58.797+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : DELETE Operation - Deleted Rows: 1
2025-01-15T11:00:58.799+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "d", 
    "before": {
      "id": 1,
      "name": "Metehan Altuner",
      "email": "metehan.altuner@gmail.com"
    },
    "after": null,
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:05:29.470+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T11:05:29.648+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Node -1 disconnected.
2025-01-15T11:07:53.950+03:00  INFO 12556 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=kafka-producer-1] Node -1 disconnected.
2025-01-15T11:09:34.325+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:09:34.351+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:09:34.354+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:09:34.372+03:00  WARN 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:09:34.372+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T11:09:34.373+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T11:09:34.373+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T11:09:34.373+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T11:09:34.373+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T11:09:34.514+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:09:34.517+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : INSERT Operation - Inserted Rows: 1
2025-01-15T11:09:34.521+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:09:34.523+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:09:34.551+03:00  WARN 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:09:34.551+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T11:09:34.552+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T11:09:34.552+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T11:09:34.552+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T11:09:34.552+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T11:09:34.694+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:09:34.697+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : INSERT Operation - Inserted Rows: 1
2025-01-15T11:09:34.699+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:14:04.649+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:14:04.673+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:14:04.675+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:14:04.689+03:00  WARN 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:14:04.690+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T11:14:04.690+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T11:14:04.690+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T11:14:04.690+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T11:14:04.690+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T11:14:04.811+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:14:04.814+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : INSERT Operation - Inserted Rows: 1
2025-01-15T11:14:04.818+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:14:04.820+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:14:04.833+03:00  WARN 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:14:04.833+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T11:14:04.833+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T11:14:04.833+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T11:14:04.833+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T11:14:04.833+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T11:14:04.942+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:14:04.944+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : INSERT Operation - Inserted Rows: 1
2025-01-15T11:14:04.948+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:14:04.950+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:14:04.969+03:00  WARN 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:14:04.969+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001010: No JDBC driver class specified by hibernate.connection.driver_class
2025-01-15T11:14:04.970+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001011: Loaded JDBC drivers: org.postgresql.Driver
2025-01-15T11:14:04.970+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:sqlserver://10.0.4.35;instanceName=SQL_TEST_DEV;databaseName=kafka_db;encrypt=false;trustServerCertificate=true;
]
2025-01-15T11:14:04.970+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=hakan}
2025-01-15T11:14:04.970+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T11:14:04.970+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T11:14:04.971+03:00  WARN 12556 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata

java.lang.IllegalStateException: Cannot get a connection as the driver manager is not properly initialized
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.getConnection(DriverManagerConnectionProviderImpl.java:260) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:437) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.resource.transaction.backend.jdbc.internal.JdbcIsolationDelegate.delegateWork(JdbcIsolationDelegate.java:61) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.getJdbcEnvironmentUsingJdbcMetadata(JdbcEnvironmentInitiator.java:290) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:123) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:77) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:130) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:238) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.ServiceRegistry.requireService(ServiceRegistry.java:68) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.internal.JdbcServicesImpl.configure(JdbcServicesImpl.java:52) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:136) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:247) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.BasicConnectionCreator.convertSqlException(BasicConnectionCreator.java:133) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionCreator.makeConnection(DriverManagerConnectionCreator.java:39) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.BasicConnectionCreator.createConnection(BasicConnectionCreator.java:61) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl$PooledConnections.addConnections(DriverManagerConnectionProviderImpl.java:500) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl$PooledConnections.<init>(DriverManagerConnectionProviderImpl.java:373) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl$PooledConnections$Builder.build(DriverManagerConnectionProviderImpl.java:551) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.buildPool(DriverManagerConnectionProviderImpl.java:102) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.configure(DriverManagerConnectionProviderImpl.java:82) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:136) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:247) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.ServiceRegistry.requireService(ServiceRegistry.java:68) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.buildJdbcConnectionAccess(JdbcEnvironmentInitiator.java:404) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.getJdbcEnvironmentUsingJdbcMetadata(JdbcEnvironmentInitiator.java:276) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:123) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:77) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:130) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:238) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.model.relational.Database.<init>(Database.java:45) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.getDatabase(InFlightMetadataCollectorImpl.java:221) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.<init>(InFlightMetadataCollectorImpl.java:189) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:171) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1431) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1502) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.HibernatePersistenceProvider.createEntityManagerFactory(HibernatePersistenceProvider.java:55) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at jakarta.persistence.Persistence.createEntityManagerFactory(Persistence.java:80) ~[jakarta.persistence-api-3.1.0.jar:3.1.0]
	at com.demo.kafka.config.DatabaseConnectionUtil.createEntityManager(DatabaseConnectionUtil.java:23) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:152) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:130) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]

2025-01-15T11:14:04.989+03:00 ERROR 12556 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error processing message from topic: test-topic, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}


java.lang.RuntimeException: Veri tabanı bağlantısı başarısız. Lütfen bağlantı ayarlarını kontrol edin.
	at com.demo.kafka.config.DatabaseConnectionUtil.createEntityManager(DatabaseConnectionUtil.java:30) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:152) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:130) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment] due to: Error calling DriverManager.getConnection() [No suitable driver found for jdbc:sqlserver://10.0.4.35;instanceName=SQL_TEST_DEV;databaseName=kafka_db;encrypt=false;trustServerCertificate=true;
]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:276) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:238) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.model.relational.Database.<init>(Database.java:45) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.getDatabase(InFlightMetadataCollectorImpl.java:221) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.<init>(InFlightMetadataCollectorImpl.java:189) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:171) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1431) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1502) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.HibernatePersistenceProvider.createEntityManagerFactory(HibernatePersistenceProvider.java:55) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at jakarta.persistence.Persistence.createEntityManagerFactory(Persistence.java:80) ~[jakarta.persistence-api-3.1.0.jar:3.1.0]
	at com.demo.kafka.config.DatabaseConnectionUtil.createEntityManager(DatabaseConnectionUtil.java:23) ~[classes/:na]
	... 18 common frames omitted
Caused by: org.hibernate.exception.JDBCConnectionException: Error calling DriverManager.getConnection() [No suitable driver found for jdbc:sqlserver://10.0.4.35;instanceName=SQL_TEST_DEV;databaseName=kafka_db;encrypt=false;trustServerCertificate=true;
]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:100) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.BasicConnectionCreator$1.convert(BasicConnectionCreator.java:118) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.BasicConnectionCreator.convertSqlException(BasicConnectionCreator.java:144) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionCreator.makeConnection(DriverManagerConnectionCreator.java:39) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.BasicConnectionCreator.createConnection(BasicConnectionCreator.java:61) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl$PooledConnections.addConnections(DriverManagerConnectionProviderImpl.java:500) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl$PooledConnections.<init>(DriverManagerConnectionProviderImpl.java:373) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl$PooledConnections$Builder.build(DriverManagerConnectionProviderImpl.java:551) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.buildPool(DriverManagerConnectionProviderImpl.java:102) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.configure(DriverManagerConnectionProviderImpl.java:82) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:136) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:247) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.ServiceRegistry.requireService(ServiceRegistry.java:68) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.buildJdbcConnectionAccess(JdbcEnvironmentInitiator.java:404) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.getJdbcEnvironmentUsingJdbcMetadata(JdbcEnvironmentInitiator.java:276) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:123) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:77) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:130) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 29 common frames omitted
Caused by: java.sql.SQLException: No suitable driver found for jdbc:sqlserver://10.0.4.35;instanceName=SQL_TEST_DEV;databaseName=kafka_db;encrypt=false;trustServerCertificate=true;

	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:707) ~[java.sql:na]
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:191) ~[java.sql:na]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionCreator.makeConnection(DriverManagerConnectionCreator.java:36) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 45 common frames omitted

2025-01-15T11:15:29.354+03:00  INFO 12556 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Node -1 disconnected.
2025-01-15T11:15:29.356+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T11:15:45.945+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T11:15:45.946+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T11:15:45.947+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-ca39ff97-e735-4045-bcbf-508091487508 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T11:15:45.948+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T11:15:45.949+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T11:15:45.949+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T11:15:45.951+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T11:15:45.952+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T11:15:45.981+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T11:15:45.981+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T11:15:45.982+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T11:15:45.982+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T11:15:45.988+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T11:15:45.988+03:00  INFO 12556 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T11:15:46.006+03:00  INFO 12556 --- [kafka] [SpringApplicationShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-15T11:15:46.010+03:00  INFO 12556 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T11:15:46.011+03:00  INFO 12556 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T11:15:46.011+03:00  INFO 12556 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T11:15:46.011+03:00  INFO 12556 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T11:15:46.011+03:00  INFO 12556 --- [kafka] [SpringApplicationShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for kafka-producer-1 unregistered
2025-01-15T11:15:46.017+03:00  INFO 12556 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T11:15:46.022+03:00  INFO 12556 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T11:15:46.045+03:00  INFO 12556 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T11:15:49.920+03:00  INFO 18392 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 18392 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T11:15:49.924+03:00  INFO 18392 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T11:15:50.002+03:00  INFO 18392 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T11:15:50.002+03:00  INFO 18392 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T11:15:51.141+03:00  INFO 18392 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T11:15:51.223+03:00  INFO 18392 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 71 ms. Found 5 JPA repository interfaces.
2025-01-15T11:15:52.614+03:00  INFO 18392 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T11:15:52.635+03:00  INFO 18392 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T11:15:52.636+03:00  INFO 18392 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T11:15:52.699+03:00  INFO 18392 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T11:15:52.699+03:00  INFO 18392 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2697 ms
2025-01-15T11:15:52.955+03:00  INFO 18392 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T11:15:53.032+03:00  INFO 18392 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T11:15:53.094+03:00  INFO 18392 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:15:53.507+03:00  INFO 18392 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T11:15:53.539+03:00  INFO 18392 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T11:15:54.038+03:00  INFO 18392 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@41894796
2025-01-15T11:15:54.040+03:00  INFO 18392 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T11:15:55.342+03:00  INFO 18392 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:15:55.345+03:00  INFO 18392 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T11:15:56.322+03:00  WARN 18392 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T11:15:56.947+03:00  INFO 18392 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T11:15:57.046+03:00  INFO 18392 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T11:15:57.101+03:00  INFO 18392 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T11:15:57.163+03:00  INFO 18392 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:15:57.501+03:00  INFO 18392 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:15:57.502+03:00  INFO 18392 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:15:57.502+03:00  INFO 18392 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736928957499
2025-01-15T11:15:57.510+03:00  INFO 18392 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T11:15:57.547+03:00  INFO 18392 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 8.316 seconds (process running for 9.085)
2025-01-15T11:15:57.793+03:00  INFO 18392 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T11:15:57.794+03:00  INFO 18392 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:15:57.855+03:00  INFO 18392 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:15:57.856+03:00  INFO 18392 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:15:57.856+03:00  INFO 18392 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736928957855
2025-01-15T11:15:57.857+03:00  INFO 18392 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T11:15:58.217+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:15:58.217+03:00  INFO 18392 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:15:58.219+03:00  INFO 18392 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T11:15:58.219+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T11:15:58.226+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T11:15:58.226+03:00  INFO 18392 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T11:15:58.286+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-3f77c96d-a07f-4c3e-8999-2c4f4cd29b07
2025-01-15T11:15:58.287+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T11:15:58.287+03:00  INFO 18392 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-9f1d0945-f7b6-4a97-a74d-f5278d34551d
2025-01-15T11:15:58.288+03:00  INFO 18392 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T11:16:00.092+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-3f77c96d-a07f-4c3e-8999-2c4f4cd29b07 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T11:16:00.093+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T11:16:00.094+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T11:16:00.095+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T11:16:00.098+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T11:16:00.099+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T11:16:01.299+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T11:16:01.299+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T11:16:01.300+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T11:16:01.300+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T11:16:01.307+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T11:16:01.308+03:00  INFO 18392 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T11:16:01.318+03:00  INFO 18392 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T11:16:01.324+03:00  INFO 18392 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T11:16:01.344+03:00  INFO 18392 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T11:16:04.954+03:00  INFO 18044 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 18044 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T11:16:04.958+03:00  INFO 18044 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T11:16:05.026+03:00  INFO 18044 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T11:16:05.027+03:00  INFO 18044 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T11:16:06.174+03:00  INFO 18044 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T11:16:06.266+03:00  INFO 18044 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 78 ms. Found 5 JPA repository interfaces.
2025-01-15T11:16:07.274+03:00  INFO 18044 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T11:16:07.294+03:00  INFO 18044 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T11:16:07.294+03:00  INFO 18044 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T11:16:07.352+03:00  INFO 18044 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T11:16:07.352+03:00  INFO 18044 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2325 ms
2025-01-15T11:16:07.608+03:00  INFO 18044 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T11:16:07.688+03:00  INFO 18044 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T11:16:07.750+03:00  INFO 18044 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:16:08.159+03:00  INFO 18044 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T11:16:08.193+03:00  INFO 18044 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T11:16:08.564+03:00  INFO 18044 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@17124d64
2025-01-15T11:16:08.567+03:00  INFO 18044 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T11:16:09.703+03:00  INFO 18044 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:16:09.708+03:00  INFO 18044 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T11:16:10.541+03:00  WARN 18044 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T11:16:11.157+03:00  INFO 18044 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T11:16:11.264+03:00  INFO 18044 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T11:16:11.314+03:00  INFO 18044 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T11:16:11.374+03:00  INFO 18044 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:16:11.706+03:00  INFO 18044 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:16:11.707+03:00  INFO 18044 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:16:11.707+03:00  INFO 18044 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736928971704
2025-01-15T11:16:11.712+03:00  INFO 18044 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T11:16:11.742+03:00  INFO 18044 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.432 seconds (process running for 8.179)
2025-01-15T11:16:11.966+03:00  INFO 18044 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T11:16:11.968+03:00  INFO 18044 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:16:12.030+03:00  INFO 18044 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:16:12.030+03:00  INFO 18044 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:16:12.030+03:00  INFO 18044 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736928972030
2025-01-15T11:16:12.031+03:00  INFO 18044 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T11:16:12.384+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:16:12.384+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:16:12.385+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T11:16:12.385+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T11:16:12.390+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T11:16:12.390+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T11:16:12.446+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-e9cc5f5f-feda-4050-bc03-ec52e18ccbd4
2025-01-15T11:16:12.447+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T11:16:12.451+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-409327ba-75c5-41d9-bc19-da95176407c6
2025-01-15T11:16:12.451+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T11:16:15.453+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=53, memberId='consumer-my-group-1-e9cc5f5f-feda-4050-bc03-ec52e18ccbd4', protocol='range'}
2025-01-15T11:16:15.461+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 53: {consumer-my-group-1-e9cc5f5f-feda-4050-bc03-ec52e18ccbd4=Assignment(partitions=[test-topic-0])}
2025-01-15T11:16:15.473+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=53, memberId='consumer-my-group-1-e9cc5f5f-feda-4050-bc03-ec52e18ccbd4', protocol='range'}
2025-01-15T11:16:15.474+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T11:16:15.477+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T11:16:15.491+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=17, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T11:16:15.493+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T11:16:29.110+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=13, memberId='consumer-dynamic-group-2-409327ba-75c5-41d9-bc19-da95176407c6', protocol='range'}
2025-01-15T11:16:29.110+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 13: {consumer-dynamic-group-2-409327ba-75c5-41d9-bc19-da95176407c6=Assignment(partitions=[test-topic-0]), consumer-dynamic-group-2-9f1d0945-f7b6-4a97-a74d-f5278d34551d=Assignment(partitions=[])}
2025-01-15T11:16:29.119+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=13, memberId='consumer-dynamic-group-2-409327ba-75c5-41d9-bc19-da95176407c6', protocol='range'}
2025-01-15T11:16:29.119+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T11:16:29.119+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T11:16:29.122+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T11:16:29.126+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=17, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T11:17:14.120+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: group is already rebalancing
2025-01-15T11:17:14.121+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Revoke previously assigned partitions test-topic-0
2025-01-15T11:17:14.122+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions revoked: [test-topic-0]
2025-01-15T11:17:14.122+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T11:17:14.126+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=14, memberId='consumer-dynamic-group-2-409327ba-75c5-41d9-bc19-da95176407c6', protocol='range'}
2025-01-15T11:17:14.126+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 14: {consumer-dynamic-group-2-409327ba-75c5-41d9-bc19-da95176407c6=Assignment(partitions=[test-topic-0])}
2025-01-15T11:17:14.137+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=14, memberId='consumer-dynamic-group-2-409327ba-75c5-41d9-bc19-da95176407c6', protocol='range'}
2025-01-15T11:17:14.137+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T11:17:14.137+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T11:17:14.141+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T11:17:14.145+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=17, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T11:17:58.904+03:00  INFO 18044 --- [kafka] [http-nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T11:17:58.905+03:00  INFO 18044 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T11:17:58.906+03:00  INFO 18044 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 1 ms
2025-01-15T11:17:58.976+03:00  INFO 18044 --- [kafka] [http-nio-8080-exec-1] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-01-15T11:17:58.977+03:00  INFO 18044 --- [kafka] [http-nio-8080-exec-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:17:58.989+03:00  INFO 18044 --- [kafka] [http-nio-8080-exec-1] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Instantiated an idempotent producer.
2025-01-15T11:17:59.032+03:00  INFO 18044 --- [kafka] [http-nio-8080-exec-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:17:59.033+03:00  INFO 18044 --- [kafka] [http-nio-8080-exec-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:17:59.033+03:00  INFO 18044 --- [kafka] [http-nio-8080-exec-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736929079032
2025-01-15T11:17:59.061+03:00  INFO 18044 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=kafka-producer-1] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:17:59.062+03:00  INFO 18044 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=kafka-producer-1] ProducerId set to 3006 with epoch 0
2025-01-15T11:17:59.143+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:17:59.460+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:17:59.467+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:17:59.557+03:00  WARN 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:17:59.558+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T11:17:59.558+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T11:17:59.558+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T11:17:59.558+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T11:17:59.563+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T11:17:59.749+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:17:59.783+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : INSERT Operation - Inserted Rows: 1
2025-01-15T11:17:59.788+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:17:59.789+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:17:59.811+03:00  WARN 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:17:59.811+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T11:17:59.811+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T11:17:59.812+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T11:17:59.812+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T11:17:59.812+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T11:17:59.944+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:17:59.948+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : INSERT Operation - Inserted Rows: 1
2025-01-15T11:17:59.954+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:17:59.957+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:18:00.001+03:00  WARN 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:18:00.006+03:00 ERROR 18044 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error processing message from topic: test-topic, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}


java.lang.RuntimeException: Veri tabanı bağlantısı başarısız. Lütfen bağlantı ayarlarını kontrol edin.
	at com.demo.kafka.config.DatabaseConnectionUtil.createEntityManager(DatabaseConnectionUtil.java:35) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:152) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:130) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment] due to: Unable to load class [com.microsoft.sqlserver.jdbc.SQLServerDriver]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:276) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:238) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.model.relational.Database.<init>(Database.java:45) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.getDatabase(InFlightMetadataCollectorImpl.java:221) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.<init>(InFlightMetadataCollectorImpl.java:189) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:171) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1431) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1502) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.HibernatePersistenceProvider.createEntityManagerFactory(HibernatePersistenceProvider.java:55) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at jakarta.persistence.Persistence.createEntityManagerFactory(Persistence.java:80) ~[jakarta.persistence-api-3.1.0.jar:3.1.0]
	at com.demo.kafka.config.DatabaseConnectionUtil.createEntityManager(DatabaseConnectionUtil.java:28) ~[classes/:na]
	... 18 common frames omitted
Caused by: org.hibernate.boot.registry.classloading.spi.ClassLoadingException: Unable to load class [com.microsoft.sqlserver.jdbc.SQLServerDriver]
	at org.hibernate.boot.registry.classloading.internal.ClassLoaderServiceImpl.classForName(ClassLoaderServiceImpl.java:126) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.loadDriverIfPossible(DriverManagerConnectionProviderImpl.java:211) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.buildCreator(DriverManagerConnectionProviderImpl.java:112) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.buildPool(DriverManagerConnectionProviderImpl.java:93) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.configure(DriverManagerConnectionProviderImpl.java:82) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:136) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:247) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.ServiceRegistry.requireService(ServiceRegistry.java:68) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.buildJdbcConnectionAccess(JdbcEnvironmentInitiator.java:404) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.getJdbcEnvironmentUsingJdbcMetadata(JdbcEnvironmentInitiator.java:276) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:123) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:77) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:130) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 29 common frames omitted
Caused by: java.lang.ClassNotFoundException: Could not load requested class : com.microsoft.sqlserver.jdbc.SQLServerDriver
	at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:216) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592) ~[na:na]
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
	at java.base/java.lang.Class.forName0(Native Method) ~[na:na]
	at java.base/java.lang.Class.forName(Class.java:529) ~[na:na]
	at java.base/java.lang.Class.forName(Class.java:508) ~[na:na]
	at org.hibernate.boot.registry.classloading.internal.ClassLoaderServiceImpl.classForName(ClassLoaderServiceImpl.java:123) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 43 common frames omitted
Caused by: java.lang.Throwable: null
	at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:209) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 49 common frames omitted
	Suppressed: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
		at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) ~[na:na]
		at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) ~[na:na]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:206) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
		... 49 common frames omitted
	Suppressed: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
		at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) ~[na:na]
		at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) ~[na:na]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at java.base/java.lang.Class.forName0(Native Method) ~[na:na]
		at java.base/java.lang.Class.forName(Class.java:529) ~[na:na]
		at java.base/java.lang.Class.forName(Class.java:508) ~[na:na]
		at org.springframework.boot.devtools.restart.classloader.RestartClassLoader.loadClass(RestartClassLoader.java:121) ~[spring-boot-devtools-3.3.3.jar:3.3.3]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:206) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
		... 49 common frames omitted
	Suppressed: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
		at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) ~[na:na]
		at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) ~[na:na]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:206) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
		... 49 common frames omitted

2025-01-15T11:18:26.107+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:18:37.352+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:18:37.387+03:00  INFO 18044 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:18:37.649+03:00  WARN 18044 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:18:37.654+03:00 ERROR 18044 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error processing message from topic: test-topic, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}


java.lang.RuntimeException: Veri tabanı bağlantısı başarısız. Lütfen bağlantı ayarlarını kontrol edin.
	at com.demo.kafka.config.DatabaseConnectionUtil.createEntityManager(DatabaseConnectionUtil.java:35) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:152) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:130) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment] due to: Unable to load class [com.microsoft.sqlserver.jdbc.SQLServerDriver]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:276) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:238) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.model.relational.Database.<init>(Database.java:45) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.getDatabase(InFlightMetadataCollectorImpl.java:221) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.<init>(InFlightMetadataCollectorImpl.java:189) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:171) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1431) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1502) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.HibernatePersistenceProvider.createEntityManagerFactory(HibernatePersistenceProvider.java:55) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at jakarta.persistence.Persistence.createEntityManagerFactory(Persistence.java:80) ~[jakarta.persistence-api-3.1.0.jar:3.1.0]
	at com.demo.kafka.config.DatabaseConnectionUtil.createEntityManager(DatabaseConnectionUtil.java:28) ~[classes/:na]
	... 18 common frames omitted
Caused by: org.hibernate.boot.registry.classloading.spi.ClassLoadingException: Unable to load class [com.microsoft.sqlserver.jdbc.SQLServerDriver]
	at org.hibernate.boot.registry.classloading.internal.ClassLoaderServiceImpl.classForName(ClassLoaderServiceImpl.java:126) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.loadDriverIfPossible(DriverManagerConnectionProviderImpl.java:211) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.buildCreator(DriverManagerConnectionProviderImpl.java:112) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.buildPool(DriverManagerConnectionProviderImpl.java:93) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.configure(DriverManagerConnectionProviderImpl.java:82) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:136) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:247) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.ServiceRegistry.requireService(ServiceRegistry.java:68) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.buildJdbcConnectionAccess(JdbcEnvironmentInitiator.java:404) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.getJdbcEnvironmentUsingJdbcMetadata(JdbcEnvironmentInitiator.java:276) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:123) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:77) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:130) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 29 common frames omitted
Caused by: java.lang.ClassNotFoundException: Could not load requested class : com.microsoft.sqlserver.jdbc.SQLServerDriver
	at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:216) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592) ~[na:na]
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
	at java.base/java.lang.Class.forName0(Native Method) ~[na:na]
	at java.base/java.lang.Class.forName(Class.java:529) ~[na:na]
	at java.base/java.lang.Class.forName(Class.java:508) ~[na:na]
	at org.hibernate.boot.registry.classloading.internal.ClassLoaderServiceImpl.classForName(ClassLoaderServiceImpl.java:123) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 43 common frames omitted
Caused by: java.lang.Throwable: null
	at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:209) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 49 common frames omitted
	Suppressed: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
		at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) ~[na:na]
		at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) ~[na:na]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:206) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
		... 49 common frames omitted
	Suppressed: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
		at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) ~[na:na]
		at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) ~[na:na]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at java.base/java.lang.Class.forName0(Native Method) ~[na:na]
		at java.base/java.lang.Class.forName(Class.java:529) ~[na:na]
		at java.base/java.lang.Class.forName(Class.java:508) ~[na:na]
		at org.springframework.boot.devtools.restart.classloader.RestartClassLoader.loadClass(RestartClassLoader.java:121) ~[spring-boot-devtools-3.3.3.jar:3.3.3]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:206) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
		... 49 common frames omitted
	Suppressed: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
		at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) ~[na:na]
		at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) ~[na:na]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:206) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
		... 49 common frames omitted

2025-01-15T11:18:56.144+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T11:18:56.145+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T11:18:56.145+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-e9cc5f5f-feda-4050-bc03-ec52e18ccbd4 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T11:18:56.146+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T11:18:56.146+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T11:18:56.146+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T11:18:56.148+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T11:18:56.148+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T11:18:56.503+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T11:18:56.503+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T11:18:56.503+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T11:18:56.503+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T11:18:56.510+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T11:18:56.510+03:00  INFO 18044 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T11:18:56.521+03:00  INFO 18044 --- [kafka] [SpringApplicationShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-15T11:18:56.525+03:00  INFO 18044 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T11:18:56.525+03:00  INFO 18044 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T11:18:56.525+03:00  INFO 18044 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T11:18:56.525+03:00  INFO 18044 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T11:18:56.526+03:00  INFO 18044 --- [kafka] [SpringApplicationShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for kafka-producer-1 unregistered
2025-01-15T11:18:56.529+03:00  INFO 18044 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T11:18:56.532+03:00  INFO 18044 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T11:18:56.550+03:00  INFO 18044 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T11:19:00.181+03:00  INFO 18416 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 18416 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T11:19:00.184+03:00  INFO 18416 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T11:19:00.258+03:00  INFO 18416 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T11:19:00.258+03:00  INFO 18416 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T11:19:01.396+03:00  INFO 18416 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T11:19:01.473+03:00  INFO 18416 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 66 ms. Found 5 JPA repository interfaces.
2025-01-15T11:19:02.409+03:00  INFO 18416 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T11:19:02.429+03:00  INFO 18416 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T11:19:02.429+03:00  INFO 18416 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T11:19:02.484+03:00  INFO 18416 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T11:19:02.484+03:00  INFO 18416 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2225 ms
2025-01-15T11:19:02.747+03:00  INFO 18416 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T11:19:02.831+03:00  INFO 18416 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T11:19:02.889+03:00  INFO 18416 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:19:03.296+03:00  INFO 18416 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T11:19:03.330+03:00  INFO 18416 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T11:19:03.674+03:00  INFO 18416 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@5b06c8e1
2025-01-15T11:19:03.677+03:00  INFO 18416 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T11:19:04.791+03:00  INFO 18416 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:19:04.795+03:00  INFO 18416 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T11:19:05.628+03:00  WARN 18416 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T11:19:06.307+03:00  INFO 18416 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T11:19:06.404+03:00  INFO 18416 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T11:19:06.453+03:00  INFO 18416 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T11:19:06.527+03:00  INFO 18416 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:19:06.868+03:00  INFO 18416 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:19:06.868+03:00  INFO 18416 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:19:06.868+03:00  INFO 18416 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736929146867
2025-01-15T11:19:06.872+03:00  INFO 18416 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T11:19:06.900+03:00  INFO 18416 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.37 seconds (process running for 8.099)
2025-01-15T11:19:07.138+03:00  INFO 18416 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T11:19:07.139+03:00  INFO 18416 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:19:07.190+03:00  INFO 18416 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:19:07.191+03:00  INFO 18416 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:19:07.191+03:00  INFO 18416 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736929147190
2025-01-15T11:19:07.192+03:00  INFO 18416 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T11:19:07.543+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:19:07.543+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:19:07.544+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T11:19:07.544+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T11:19:07.548+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T11:19:07.548+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T11:19:07.611+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-9a6a7d4b-8c35-4a8f-8031-d17c085677fb
2025-01-15T11:19:07.611+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-9679fb81-5a9a-4e48-81ad-33f63264a57a
2025-01-15T11:19:07.612+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T11:19:07.612+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T11:19:10.617+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=55, memberId='consumer-my-group-1-9a6a7d4b-8c35-4a8f-8031-d17c085677fb', protocol='range'}
2025-01-15T11:19:10.626+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 55: {consumer-my-group-1-9a6a7d4b-8c35-4a8f-8031-d17c085677fb=Assignment(partitions=[test-topic-0])}
2025-01-15T11:19:10.640+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=55, memberId='consumer-my-group-1-9a6a7d4b-8c35-4a8f-8031-d17c085677fb', protocol='range'}
2025-01-15T11:19:10.641+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T11:19:10.643+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T11:19:10.658+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=19, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T11:19:10.660+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T11:19:12.179+03:00  INFO 18416 --- [kafka] [http-nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T11:19:12.179+03:00  INFO 18416 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T11:19:12.181+03:00  INFO 18416 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 1 ms
2025-01-15T11:19:12.249+03:00  INFO 18416 --- [kafka] [http-nio-8080-exec-1] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-01-15T11:19:12.250+03:00  INFO 18416 --- [kafka] [http-nio-8080-exec-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:19:12.260+03:00  INFO 18416 --- [kafka] [http-nio-8080-exec-1] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Instantiated an idempotent producer.
2025-01-15T11:19:12.300+03:00  INFO 18416 --- [kafka] [http-nio-8080-exec-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:19:12.300+03:00  INFO 18416 --- [kafka] [http-nio-8080-exec-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:19:12.300+03:00  INFO 18416 --- [kafka] [http-nio-8080-exec-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736929152300
2025-01-15T11:19:12.336+03:00  INFO 18416 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=kafka-producer-1] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:19:12.338+03:00  INFO 18416 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=kafka-producer-1] ProducerId set to 3007 with epoch 0
2025-01-15T11:19:40.339+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=15, memberId='consumer-dynamic-group-2-9679fb81-5a9a-4e48-81ad-33f63264a57a', protocol='range'}
2025-01-15T11:19:40.340+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 15: {consumer-dynamic-group-2-9679fb81-5a9a-4e48-81ad-33f63264a57a=Assignment(partitions=[test-topic-0])}
2025-01-15T11:19:40.354+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=15, memberId='consumer-dynamic-group-2-9679fb81-5a9a-4e48-81ad-33f63264a57a', protocol='range'}
2025-01-15T11:19:40.354+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T11:19:40.355+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T11:19:40.359+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T11:19:40.368+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=19, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T11:19:40.404+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:19:40.606+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:19:40.609+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:19:40.692+03:00  WARN 18416 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:19:40.693+03:00 ERROR 18416 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error processing message from topic: test-topic, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}


java.lang.RuntimeException: Veri tabanı bağlantısı başarısız. Lütfen bağlantı ayarlarını kontrol edin.
	at com.demo.kafka.config.DatabaseConnectionUtil.createEntityManager(DatabaseConnectionUtil.java:37) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:152) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:130) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment] due to: Unable to load class [com.microsoft.sqlserver.jdbc.SQLServerDriver]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:276) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:238) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.model.relational.Database.<init>(Database.java:45) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.getDatabase(InFlightMetadataCollectorImpl.java:221) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.<init>(InFlightMetadataCollectorImpl.java:189) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:171) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:1431) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1502) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.jpa.HibernatePersistenceProvider.createEntityManagerFactory(HibernatePersistenceProvider.java:55) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at jakarta.persistence.Persistence.createEntityManagerFactory(Persistence.java:80) ~[jakarta.persistence-api-3.1.0.jar:3.1.0]
	at com.demo.kafka.config.DatabaseConnectionUtil.createEntityManager(DatabaseConnectionUtil.java:30) ~[classes/:na]
	... 18 common frames omitted
Caused by: org.hibernate.boot.registry.classloading.spi.ClassLoadingException: Unable to load class [com.microsoft.sqlserver.jdbc.SQLServerDriver]
	at org.hibernate.boot.registry.classloading.internal.ClassLoaderServiceImpl.classForName(ClassLoaderServiceImpl.java:126) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.loadDriverIfPossible(DriverManagerConnectionProviderImpl.java:211) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.buildCreator(DriverManagerConnectionProviderImpl.java:112) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.buildPool(DriverManagerConnectionProviderImpl.java:93) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl.configure(DriverManagerConnectionProviderImpl.java:82) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:136) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:247) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.ServiceRegistry.requireService(ServiceRegistry.java:68) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.buildJdbcConnectionAccess(JdbcEnvironmentInitiator.java:404) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.getJdbcEnvironmentUsingJdbcMetadata(JdbcEnvironmentInitiator.java:276) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:123) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:77) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:130) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 29 common frames omitted
Caused by: java.lang.ClassNotFoundException: Could not load requested class : com.microsoft.sqlserver.jdbc.SQLServerDriver
	at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:216) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592) ~[na:na]
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
	at java.base/java.lang.Class.forName0(Native Method) ~[na:na]
	at java.base/java.lang.Class.forName(Class.java:529) ~[na:na]
	at java.base/java.lang.Class.forName(Class.java:508) ~[na:na]
	at org.hibernate.boot.registry.classloading.internal.ClassLoaderServiceImpl.classForName(ClassLoaderServiceImpl.java:123) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 43 common frames omitted
Caused by: java.lang.Throwable: null
	at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:209) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 49 common frames omitted
	Suppressed: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
		at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) ~[na:na]
		at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) ~[na:na]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:206) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
		... 49 common frames omitted
	Suppressed: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
		at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) ~[na:na]
		at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) ~[na:na]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at java.base/java.lang.Class.forName0(Native Method) ~[na:na]
		at java.base/java.lang.Class.forName(Class.java:529) ~[na:na]
		at java.base/java.lang.Class.forName(Class.java:508) ~[na:na]
		at org.springframework.boot.devtools.restart.classloader.RestartClassLoader.loadClass(RestartClassLoader.java:121) ~[spring-boot-devtools-3.3.3.jar:3.3.3]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:206) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
		... 49 common frames omitted
	Suppressed: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
		at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) ~[na:na]
		at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) ~[na:na]
		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) ~[na:na]
		at org.hibernate.boot.registry.classloading.internal.AggregatedClassLoader.findClass(AggregatedClassLoader.java:206) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
		... 49 common frames omitted

2025-01-15T11:28:07.728+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T11:28:08.054+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Node -1 disconnected.
2025-01-15T11:28:12.350+03:00  INFO 18416 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=kafka-producer-1] Node -1 disconnected.
2025-01-15T11:38:07.604+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T11:38:07.876+03:00  INFO 18416 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Node -1 disconnected.
2025-01-15T11:38:12.362+03:00  INFO 18416 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=kafka-producer-1] Node -1 disconnected.
2025-01-15T11:42:06.762+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T11:42:06.768+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T11:42:06.769+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-9a6a7d4b-8c35-4a8f-8031-d17c085677fb sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T11:42:06.770+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T11:42:06.771+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T11:42:06.771+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T11:42:06.774+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T11:42:06.775+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T11:42:07.235+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T11:42:07.235+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T11:42:07.236+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T11:42:07.236+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T11:42:07.242+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T11:42:07.242+03:00  INFO 18416 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T11:42:07.262+03:00  INFO 18416 --- [kafka] [SpringApplicationShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-15T11:42:07.269+03:00  INFO 18416 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T11:42:07.269+03:00  INFO 18416 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T11:42:07.269+03:00  INFO 18416 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T11:42:07.269+03:00  INFO 18416 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T11:42:07.270+03:00  INFO 18416 --- [kafka] [SpringApplicationShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for kafka-producer-1 unregistered
2025-01-15T11:42:07.274+03:00  INFO 18416 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T11:42:07.278+03:00  INFO 18416 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T11:42:07.303+03:00  INFO 18416 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T11:42:10.962+03:00  INFO 4508 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 4508 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T11:42:10.966+03:00  INFO 4508 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T11:42:11.055+03:00  INFO 4508 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T11:42:11.056+03:00  INFO 4508 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T11:42:12.209+03:00  INFO 4508 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T11:42:12.290+03:00  INFO 4508 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 72 ms. Found 5 JPA repository interfaces.
2025-01-15T11:42:13.230+03:00  INFO 4508 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T11:42:13.251+03:00  INFO 4508 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T11:42:13.252+03:00  INFO 4508 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T11:42:13.307+03:00  INFO 4508 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T11:42:13.308+03:00  INFO 4508 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2251 ms
2025-01-15T11:42:13.578+03:00  INFO 4508 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T11:42:13.653+03:00  INFO 4508 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T11:42:13.722+03:00  INFO 4508 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:42:14.157+03:00  INFO 4508 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T11:42:14.198+03:00  INFO 4508 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T11:42:14.651+03:00  INFO 4508 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@7afb93b4
2025-01-15T11:42:14.655+03:00  INFO 4508 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T11:42:15.901+03:00  INFO 4508 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:42:15.904+03:00  INFO 4508 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T11:42:16.771+03:00  WARN 4508 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T11:42:17.464+03:00  INFO 4508 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T11:42:17.571+03:00  INFO 4508 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T11:42:17.618+03:00  INFO 4508 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T11:42:17.680+03:00  INFO 4508 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:42:18.007+03:00  INFO 4508 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:42:18.008+03:00  INFO 4508 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:42:18.008+03:00  INFO 4508 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736930538006
2025-01-15T11:42:18.013+03:00  INFO 4508 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T11:42:18.045+03:00  INFO 4508 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.705 seconds (process running for 8.431)
2025-01-15T11:42:18.293+03:00  INFO 4508 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T11:42:18.294+03:00  INFO 4508 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:42:18.345+03:00  INFO 4508 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:42:18.345+03:00  INFO 4508 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:42:18.345+03:00  INFO 4508 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736930538344
2025-01-15T11:42:18.346+03:00  INFO 4508 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T11:42:18.694+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:42:18.694+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:42:18.695+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T11:42:18.695+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T11:42:18.701+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T11:42:18.701+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T11:42:18.765+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-155c60c4-7fca-4743-8b4f-00e34bba13c3
2025-01-15T11:42:18.766+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T11:42:18.767+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-b893f7a6-2169-4dfc-8ec8-96b742fddd4b
2025-01-15T11:42:18.768+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T11:42:21.772+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=57, memberId='consumer-my-group-1-155c60c4-7fca-4743-8b4f-00e34bba13c3', protocol='range'}
2025-01-15T11:42:21.781+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 57: {consumer-my-group-1-155c60c4-7fca-4743-8b4f-00e34bba13c3=Assignment(partitions=[test-topic-0])}
2025-01-15T11:42:21.797+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=57, memberId='consumer-my-group-1-155c60c4-7fca-4743-8b4f-00e34bba13c3', protocol='range'}
2025-01-15T11:42:21.797+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T11:42:21.800+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T11:42:21.815+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=20, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T11:42:21.816+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T11:42:27.356+03:00  INFO 4508 --- [kafka] [http-nio-8080-exec-2] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T11:42:27.356+03:00  INFO 4508 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T11:42:27.358+03:00  INFO 4508 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Completed initialization in 2 ms
2025-01-15T11:42:27.452+03:00  INFO 4508 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-01-15T11:42:27.455+03:00  INFO 4508 --- [kafka] [http-nio-8080-exec-2] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:42:27.474+03:00  INFO 4508 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Instantiated an idempotent producer.
2025-01-15T11:42:27.525+03:00  INFO 4508 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:42:27.525+03:00  INFO 4508 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:42:27.525+03:00  INFO 4508 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736930547524
2025-01-15T11:42:27.557+03:00  INFO 4508 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=kafka-producer-1] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:42:27.559+03:00  INFO 4508 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=kafka-producer-1] ProducerId set to 3008 with epoch 0
2025-01-15T11:42:49.564+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=16, memberId='consumer-dynamic-group-2-b893f7a6-2169-4dfc-8ec8-96b742fddd4b', protocol='range'}
2025-01-15T11:42:49.565+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 16: {consumer-dynamic-group-2-b893f7a6-2169-4dfc-8ec8-96b742fddd4b=Assignment(partitions=[test-topic-0])}
2025-01-15T11:42:49.572+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=16, memberId='consumer-dynamic-group-2-b893f7a6-2169-4dfc-8ec8-96b742fddd4b', protocol='range'}
2025-01-15T11:42:49.573+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T11:42:49.573+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T11:42:49.575+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T11:42:49.578+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=20, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T11:42:49.608+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:42:49.787+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:42:49.792+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:42:49.884+03:00  WARN 4508 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:42:49.885+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T11:42:49.885+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T11:42:49.885+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T11:42:49.885+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T11:42:49.888+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T11:42:50.034+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:42:50.061+03:00  WARN 4508 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T11:42:50.061+03:00 ERROR 4508 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
2025-01-15T11:42:50.066+03:00 ERROR 4508 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: other_test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO other_test_table (surname, name, phone_number, id, email) VALUES (?, ?, ?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:167) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:130) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T11:42:50.075+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:42:50.077+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:42:50.107+03:00  WARN 4508 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:42:50.107+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T11:42:50.107+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T11:42:50.107+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T11:42:50.108+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T11:42:50.108+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T11:42:50.275+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:42:50.278+03:00  WARN 4508 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T11:42:50.279+03:00 ERROR 4508 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
2025-01-15T11:42:50.280+03:00 ERROR 4508 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO test_table (full_name, id, email) VALUES (?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:167) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:130) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T11:42:50.282+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:51:18.713+03:00  INFO 4508 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Node -1 disconnected.
2025-01-15T11:51:18.904+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T11:51:27.566+03:00  INFO 4508 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=kafka-producer-1] Node -1 disconnected.
2025-01-15T11:55:41.058+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T11:55:41.059+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T11:55:41.060+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-155c60c4-7fca-4743-8b4f-00e34bba13c3 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T11:55:41.061+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T11:55:41.061+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T11:55:41.061+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T11:55:41.063+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T11:55:41.063+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T11:55:41.558+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T11:55:41.558+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T11:55:41.558+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T11:55:41.558+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T11:55:41.565+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T11:55:41.566+03:00  INFO 4508 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T11:55:41.577+03:00  INFO 4508 --- [kafka] [SpringApplicationShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-15T11:55:41.581+03:00  INFO 4508 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T11:55:41.582+03:00  INFO 4508 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T11:55:41.582+03:00  INFO 4508 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T11:55:41.582+03:00  INFO 4508 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T11:55:41.583+03:00  INFO 4508 --- [kafka] [SpringApplicationShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for kafka-producer-1 unregistered
2025-01-15T11:55:41.587+03:00  INFO 4508 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T11:55:41.591+03:00  INFO 4508 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T11:55:41.609+03:00  INFO 4508 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T11:55:46.912+03:00  INFO 15808 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 15808 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T11:55:46.915+03:00  INFO 15808 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T11:55:46.993+03:00  INFO 15808 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T11:55:46.994+03:00  INFO 15808 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T11:55:48.188+03:00  INFO 15808 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T11:55:48.297+03:00  INFO 15808 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 97 ms. Found 5 JPA repository interfaces.
2025-01-15T11:55:49.284+03:00  INFO 15808 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T11:55:49.305+03:00  INFO 15808 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T11:55:49.305+03:00  INFO 15808 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T11:55:49.362+03:00  INFO 15808 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T11:55:49.363+03:00  INFO 15808 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2367 ms
2025-01-15T11:55:49.625+03:00  INFO 15808 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T11:55:49.736+03:00  INFO 15808 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T11:55:49.787+03:00  INFO 15808 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:55:50.182+03:00  INFO 15808 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T11:55:50.214+03:00  INFO 15808 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T11:55:50.556+03:00  INFO 15808 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@a4b211b
2025-01-15T11:55:50.558+03:00  INFO 15808 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T11:55:51.653+03:00  INFO 15808 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:55:51.656+03:00  INFO 15808 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T11:55:52.456+03:00  WARN 15808 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T11:55:53.062+03:00  INFO 15808 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T11:55:53.156+03:00  INFO 15808 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T11:55:53.200+03:00  INFO 15808 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T11:55:53.246+03:00  INFO 15808 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:55:53.565+03:00  INFO 15808 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:55:53.566+03:00  INFO 15808 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:55:53.566+03:00  INFO 15808 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736931353564
2025-01-15T11:55:53.572+03:00  INFO 15808 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T11:55:53.598+03:00  INFO 15808 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.341 seconds (process running for 8.129)
2025-01-15T11:55:53.832+03:00  INFO 15808 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T11:55:53.833+03:00  INFO 15808 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:55:53.888+03:00  INFO 15808 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:55:53.888+03:00  INFO 15808 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:55:53.888+03:00  INFO 15808 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736931353888
2025-01-15T11:55:53.889+03:00  INFO 15808 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T11:55:54.285+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:55:54.285+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:55:54.286+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T11:55:54.286+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T11:55:54.292+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T11:55:54.292+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T11:55:54.353+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-39d47220-3a14-48e0-96d3-a6a7b81368be
2025-01-15T11:55:54.354+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T11:55:54.357+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-264f12b9-7ada-4eff-b360-35c5947ee82d
2025-01-15T11:55:54.357+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T11:55:57.359+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=59, memberId='consumer-my-group-1-39d47220-3a14-48e0-96d3-a6a7b81368be', protocol='range'}
2025-01-15T11:55:57.368+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 59: {consumer-my-group-1-39d47220-3a14-48e0-96d3-a6a7b81368be=Assignment(partitions=[test-topic-0])}
2025-01-15T11:55:57.379+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=59, memberId='consumer-my-group-1-39d47220-3a14-48e0-96d3-a6a7b81368be', protocol='range'}
2025-01-15T11:55:57.380+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T11:55:57.382+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T11:55:57.395+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=21, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T11:55:57.396+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T11:56:02.727+03:00  INFO 15808 --- [kafka] [http-nio-8080-exec-2] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T11:56:02.727+03:00  INFO 15808 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T11:56:02.729+03:00  INFO 15808 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Completed initialization in 2 ms
2025-01-15T11:56:02.821+03:00  INFO 15808 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-01-15T11:56:02.823+03:00  INFO 15808 --- [kafka] [http-nio-8080-exec-2] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T11:56:02.837+03:00  INFO 15808 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Instantiated an idempotent producer.
2025-01-15T11:56:02.886+03:00  INFO 15808 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T11:56:02.887+03:00  INFO 15808 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T11:56:02.887+03:00  INFO 15808 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736931362886
2025-01-15T11:56:02.919+03:00  INFO 15808 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=kafka-producer-1] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T11:56:02.921+03:00  INFO 15808 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=kafka-producer-1] ProducerId set to 3009 with epoch 0
2025-01-15T11:56:25.734+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=17, memberId='consumer-dynamic-group-2-264f12b9-7ada-4eff-b360-35c5947ee82d', protocol='range'}
2025-01-15T11:56:25.735+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 17: {consumer-dynamic-group-2-264f12b9-7ada-4eff-b360-35c5947ee82d=Assignment(partitions=[test-topic-0])}
2025-01-15T11:56:25.745+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=17, memberId='consumer-dynamic-group-2-264f12b9-7ada-4eff-b360-35c5947ee82d', protocol='range'}
2025-01-15T11:56:25.746+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T11:56:25.746+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T11:56:25.749+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T11:56:25.752+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=21, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T11:56:25.785+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T11:56:25.997+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:56:26.000+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:56:26.111+03:00  WARN 15808 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:56:26.113+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T11:56:26.113+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T11:56:26.113+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T11:56:26.113+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T11:56:26.118+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T11:56:26.284+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:56:26.317+03:00  WARN 15808 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T11:56:26.317+03:00 ERROR 15808 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
2025-01-15T11:56:26.324+03:00 ERROR 15808 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: other_test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO other_test_table (surname, name, phone_number, id, email) VALUES (?, ?, ?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:167) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:130) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T11:56:26.335+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T11:56:26.337+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T11:56:26.363+03:00  WARN 15808 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T11:56:26.363+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T11:56:26.363+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T11:56:26.365+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T11:56:26.365+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T11:56:26.365+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T11:56:26.520+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T11:56:26.523+03:00  WARN 15808 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T11:56:26.523+03:00 ERROR 15808 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
2025-01-15T11:56:26.524+03:00 ERROR 15808 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO test_table (full_name, id, email) VALUES (?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:167) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:130) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T11:56:26.527+03:00  INFO 15808 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T12:01:54.227+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T12:01:54.228+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T12:01:54.228+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-39d47220-3a14-48e0-96d3-a6a7b81368be sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T12:01:54.229+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T12:01:54.229+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T12:01:54.229+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T12:01:54.230+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T12:01:54.230+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T12:01:54.674+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T12:01:54.674+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T12:01:54.675+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T12:01:54.675+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T12:01:54.682+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T12:01:54.682+03:00  INFO 15808 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T12:01:54.691+03:00  INFO 15808 --- [kafka] [SpringApplicationShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-15T12:01:54.696+03:00  INFO 15808 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T12:01:54.696+03:00  INFO 15808 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T12:01:54.696+03:00  INFO 15808 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T12:01:54.696+03:00  INFO 15808 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T12:01:54.697+03:00  INFO 15808 --- [kafka] [SpringApplicationShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for kafka-producer-1 unregistered
2025-01-15T12:01:54.700+03:00  INFO 15808 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T12:01:54.703+03:00  INFO 15808 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T12:01:54.720+03:00  INFO 15808 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T12:02:00.066+03:00  INFO 3360 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 3360 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T12:02:00.069+03:00  INFO 3360 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T12:02:00.137+03:00  INFO 3360 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T12:02:00.138+03:00  INFO 3360 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T12:02:01.269+03:00  INFO 3360 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T12:02:01.345+03:00  INFO 3360 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 67 ms. Found 5 JPA repository interfaces.
2025-01-15T12:02:02.263+03:00  INFO 3360 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T12:02:02.282+03:00  INFO 3360 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T12:02:02.283+03:00  INFO 3360 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T12:02:02.337+03:00  INFO 3360 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T12:02:02.338+03:00  INFO 3360 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2199 ms
2025-01-15T12:02:02.592+03:00  INFO 3360 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T12:02:02.665+03:00  INFO 3360 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T12:02:02.724+03:00  INFO 3360 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T12:02:03.121+03:00  INFO 3360 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T12:02:03.157+03:00  INFO 3360 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T12:02:03.530+03:00  INFO 3360 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@749c19dd
2025-01-15T12:02:03.532+03:00  INFO 3360 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T12:02:04.619+03:00  INFO 3360 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T12:02:04.623+03:00  INFO 3360 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T12:02:05.451+03:00  WARN 3360 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T12:02:06.063+03:00  INFO 3360 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T12:02:06.161+03:00  INFO 3360 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T12:02:06.212+03:00  INFO 3360 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T12:02:06.277+03:00  INFO 3360 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T12:02:06.611+03:00  INFO 3360 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T12:02:06.611+03:00  INFO 3360 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T12:02:06.611+03:00  INFO 3360 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736931726609
2025-01-15T12:02:06.616+03:00  INFO 3360 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T12:02:06.642+03:00  INFO 3360 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.164 seconds (process running for 7.887)
2025-01-15T12:02:06.887+03:00  INFO 3360 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T12:02:06.888+03:00  INFO 3360 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T12:02:06.927+03:00  INFO 3360 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T12:02:06.928+03:00  INFO 3360 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T12:02:06.928+03:00  INFO 3360 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736931726927
2025-01-15T12:02:06.929+03:00  INFO 3360 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T12:02:07.276+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T12:02:07.276+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T12:02:07.278+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T12:02:07.278+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T12:02:07.282+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T12:02:07.282+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T12:02:07.340+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-7f589fb9-6a0b-4485-ad5a-4f14ce7980ad
2025-01-15T12:02:07.341+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T12:02:07.342+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-60289035-d5db-415a-b584-b94b030dafea
2025-01-15T12:02:07.343+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T12:02:10.348+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=61, memberId='consumer-my-group-1-7f589fb9-6a0b-4485-ad5a-4f14ce7980ad', protocol='range'}
2025-01-15T12:02:10.357+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 61: {consumer-my-group-1-7f589fb9-6a0b-4485-ad5a-4f14ce7980ad=Assignment(partitions=[test-topic-0])}
2025-01-15T12:02:10.371+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=61, memberId='consumer-my-group-1-7f589fb9-6a0b-4485-ad5a-4f14ce7980ad', protocol='range'}
2025-01-15T12:02:10.372+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T12:02:10.374+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T12:02:10.388+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=22, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T12:02:10.390+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T12:02:17.907+03:00  INFO 3360 --- [kafka] [http-nio-8080-exec-2] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T12:02:17.908+03:00  INFO 3360 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T12:02:17.911+03:00  INFO 3360 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Completed initialization in 3 ms
2025-01-15T12:02:17.976+03:00  INFO 3360 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-01-15T12:02:17.977+03:00  INFO 3360 --- [kafka] [http-nio-8080-exec-2] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T12:02:17.986+03:00  INFO 3360 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Instantiated an idempotent producer.
2025-01-15T12:02:18.032+03:00  INFO 3360 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T12:02:18.032+03:00  INFO 3360 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T12:02:18.032+03:00  INFO 3360 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736931738032
2025-01-15T12:02:18.068+03:00  INFO 3360 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=kafka-producer-1] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T12:02:18.069+03:00  INFO 3360 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=kafka-producer-1] ProducerId set to 3010 with epoch 0
2025-01-15T12:02:37.804+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=18, memberId='consumer-dynamic-group-2-60289035-d5db-415a-b584-b94b030dafea', protocol='range'}
2025-01-15T12:02:37.805+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 18: {consumer-dynamic-group-2-60289035-d5db-415a-b584-b94b030dafea=Assignment(partitions=[test-topic-0])}
2025-01-15T12:02:37.817+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=18, memberId='consumer-dynamic-group-2-60289035-d5db-415a-b584-b94b030dafea', protocol='range'}
2025-01-15T12:02:37.818+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T12:02:37.818+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T12:02:37.822+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T12:02:37.826+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=22, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T12:02:37.864+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T12:02:38.062+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T12:02:38.065+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T12:02:38.137+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T12:02:38.138+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T12:02:38.138+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T12:02:38.138+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T12:02:38.139+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T12:02:38.141+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T12:02:38.272+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T12:02:38.300+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T12:02:38.300+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
2025-01-15T12:02:38.306+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: other_test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO other_test_table (surname, name, phone_number, id, email) VALUES (?, ?, ?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:179) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:137) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T12:02:38.316+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T12:02:38.318+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T12:02:38.339+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T12:02:38.339+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T12:02:38.339+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T12:02:38.339+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T12:02:38.340+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T12:02:38.340+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T12:02:38.486+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T12:02:38.488+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T12:02:38.489+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
2025-01-15T12:02:38.490+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO test_table (full_name, id, email) VALUES (?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:179) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:137) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T12:02:38.494+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T12:11:07.457+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T12:11:07.673+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Node -1 disconnected.
2025-01-15T12:11:18.075+03:00  INFO 3360 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=kafka-producer-1] Node -1 disconnected.
2025-01-15T12:21:07.304+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Node -1 disconnected.
2025-01-15T12:21:07.564+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T12:21:18.085+03:00  INFO 3360 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=kafka-producer-1] Node -1 disconnected.
2025-01-15T13:38:52.774+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T13:38:52.806+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T13:38:52.808+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T13:38:52.829+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T13:38:52.829+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T13:38:52.829+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T13:38:52.830+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T13:38:52.830+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T13:38:52.830+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T13:38:52.962+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T13:38:52.965+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T13:38:52.965+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
2025-01-15T13:38:52.966+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO test_table (full_name, id, email) VALUES (?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:179) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:137) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T13:38:52.970+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T13:38:52.972+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T13:38:53.000+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T13:38:53.001+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T13:38:53.001+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T13:38:53.002+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T13:38:53.002+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T13:38:53.002+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T13:38:53.182+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T13:38:53.185+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T13:38:53.186+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
2025-01-15T13:38:53.187+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: other_test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO other_test_table (surname, name, phone_number, id, email) VALUES (?, ?, ?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:179) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:137) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T13:38:53.192+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T13:39:00.849+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T13:40:08.417+03:00  INFO 3360 --- [kafka] [kafka-coordinator-heartbeat-thread | my-group] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) is unavailable or invalid due to cause: session timed out without receiving a heartbeat response. isDisconnected: false. Rediscovery will be attempted.
2025-01-15T13:40:08.416+03:00  INFO 3360 --- [kafka] [kafka-coordinator-heartbeat-thread | dynamic-group] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null) is unavailable or invalid due to cause: session timed out without receiving a heartbeat response. isDisconnected: false. Rediscovery will be attempted.
2025-01-15T13:40:08.418+03:00  INFO 3360 --- [kafka] [kafka-coordinator-heartbeat-thread | my-group] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Requesting disconnect from last known coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T13:40:08.418+03:00  INFO 3360 --- [kafka] [kafka-coordinator-heartbeat-thread | dynamic-group] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Requesting disconnect from last known coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T13:40:08.417+03:00  WARN 3360 --- [kafka] [HikariPool-1 housekeeper] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Thread starvation or clock leap detected (housekeeper delta=1m34s525ms856µs400ns).
2025-01-15T13:40:08.419+03:00  INFO 3360 --- [kafka] [kafka-coordinator-heartbeat-thread | dynamic-group] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Client requested disconnect from node 2147483646
2025-01-15T13:40:08.420+03:00  INFO 3360 --- [kafka] [kafka-coordinator-heartbeat-thread | my-group] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Client requested disconnect from node 2147483645
2025-01-15T13:40:13.662+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T13:40:14.958+03:00  INFO 3360 --- [kafka] [kafka-coordinator-heartbeat-thread | dynamic-group] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T13:40:16.302+03:00  INFO 3360 --- [kafka] [kafka-coordinator-heartbeat-thread | dynamic-group] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T13:40:17.166+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Attempt to heartbeat with Generation{generationId=61, memberId='consumer-my-group-1-7f589fb9-6a0b-4485-ad5a-4f14ce7980ad', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2025-01-15T13:40:17.169+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response
2025-01-15T13:40:17.170+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response
2025-01-15T13:40:17.358+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Giving away all assigned partitions as lost since generation/memberID has been reset,indicating that consumer is in old state or no longer part of the group
2025-01-15T13:40:17.359+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Lost previously assigned partitions test-topic-0
2025-01-15T13:40:17.361+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions lost: [test-topic-0]
2025-01-15T13:40:17.617+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T13:40:17.985+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T13:40:18.718+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-5cb12d36-966e-4d02-89f3-ad9457971bcc
2025-01-15T13:40:18.720+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T13:40:18.925+03:00  INFO 3360 --- [kafka] [kafka-coordinator-heartbeat-thread | dynamic-group] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Attempt to heartbeat with Generation{generationId=18, memberId='consumer-dynamic-group-2-60289035-d5db-415a-b584-b94b030dafea', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
2025-01-15T13:40:18.926+03:00  INFO 3360 --- [kafka] [kafka-coordinator-heartbeat-thread | dynamic-group] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Resetting generation and member id due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response
2025-01-15T13:40:18.926+03:00  INFO 3360 --- [kafka] [kafka-coordinator-heartbeat-thread | dynamic-group] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: encountered UNKNOWN_MEMBER_ID from HEARTBEAT response
2025-01-15T13:40:21.982+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=63, memberId='consumer-my-group-1-5cb12d36-966e-4d02-89f3-ad9457971bcc', protocol='range'}
2025-01-15T13:40:21.984+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 63: {consumer-my-group-1-5cb12d36-966e-4d02-89f3-ad9457971bcc=Assignment(partitions=[test-topic-0])}
2025-01-15T13:40:27.598+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=63, memberId='consumer-my-group-1-5cb12d36-966e-4d02-89f3-ad9457971bcc', protocol='range'}
2025-01-15T13:40:27.600+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T13:40:27.600+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T13:40:46.722+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=25, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T13:40:46.723+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T13:40:46.738+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T13:40:46.771+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T13:40:47.032+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T13:40:47.034+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T13:40:47.034+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T13:40:47.035+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T13:40:47.035+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T13:40:47.036+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T13:40:48.178+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T13:40:48.187+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T13:40:48.188+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
2025-01-15T13:40:48.190+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: other_test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO other_test_table (surname, name, phone_number, id, email) VALUES (?, ?, ?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:179) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:137) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T13:41:38.528+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T13:41:38.561+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T13:41:38.821+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T13:41:38.822+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T13:41:38.823+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T13:41:38.824+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T13:41:38.824+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T13:41:38.824+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T13:41:39.940+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T13:41:39.948+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T13:41:39.949+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
2025-01-15T13:41:39.952+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO test_table (full_name, id, email) VALUES (?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:179) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:137) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T13:41:39.965+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T13:41:39.966+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Failing OffsetCommit request since the consumer is not part of an active group
2025-01-15T13:41:39.966+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Attempt to heartbeat with stale Generation{generationId=18, memberId='consumer-dynamic-group-2-60289035-d5db-415a-b584-b94b030dafea', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, ignoring the error
2025-01-15T13:41:39.967+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Giving away all assigned partitions as lost since generation/memberID has been reset,indicating that consumer is in old state or no longer part of the group
2025-01-15T13:41:39.967+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Lost previously assigned partitions test-topic-0
2025-01-15T13:41:39.967+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions lost: [test-topic-0]
2025-01-15T13:41:39.967+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions revoked: [test-topic-0]
2025-01-15T13:41:39.967+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T13:41:39.972+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-93c58f26-6fb5-484a-9988-29432cea42cb
2025-01-15T13:41:39.973+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T13:41:42.978+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=20, memberId='consumer-dynamic-group-2-93c58f26-6fb5-484a-9988-29432cea42cb', protocol='range'}
2025-01-15T13:41:42.978+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 20: {consumer-dynamic-group-2-93c58f26-6fb5-484a-9988-29432cea42cb=Assignment(partitions=[test-topic-0])}
2025-01-15T13:41:42.988+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=20, memberId='consumer-dynamic-group-2-93c58f26-6fb5-484a-9988-29432cea42cb', protocol='range'}
2025-01-15T13:41:42.988+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T13:41:42.988+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T13:41:42.991+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T13:41:42.995+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=24, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T13:41:43.002+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T13:41:45.959+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T13:41:45.961+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T13:41:45.977+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T13:41:45.978+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T13:41:45.978+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T13:41:45.978+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T13:41:45.978+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T13:41:45.978+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T13:41:46.150+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T13:41:46.153+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T13:41:46.153+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
2025-01-15T13:41:46.154+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO test_table (full_name, id, email) VALUES (?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:179) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:137) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T13:41:46.164+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T13:41:46.168+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T13:41:46.214+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T13:41:46.215+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T13:41:46.216+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T13:41:46.216+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T13:41:46.216+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T13:41:46.216+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T13:41:46.362+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T13:41:46.365+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T13:41:46.365+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
2025-01-15T13:41:46.366+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: other_test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO other_test_table (surname, name, phone_number, id, email) VALUES (?, ?, ?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:179) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:137) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T13:41:46.369+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T13:41:46.369+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T13:41:46.391+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T13:41:46.393+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T13:41:46.410+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T13:41:46.410+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T13:41:46.410+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T13:41:46.410+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T13:41:46.410+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T13:41:46.410+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T13:41:46.541+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T13:41:46.544+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T13:41:46.544+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
2025-01-15T13:41:46.545+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO test_table (full_name, id, email) VALUES (?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:179) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:137) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 59
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T13:41:46.551+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T13:41:46.553+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T13:41:46.566+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T13:41:46.566+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T13:41:46.566+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T13:41:46.566+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T13:41:46.566+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T13:41:46.566+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T13:41:46.679+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T13:41:46.683+03:00  WARN 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42804
2025-01-15T13:41:46.683+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
2025-01-15T13:41:46.683+03:00 ERROR 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: other_test_table

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [INSERT INTO other_test_table (surname, name, phone_number, id, email) VALUES (?, ?, ?, ?, ?)] [ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:179) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:137) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column "id" is of type integer but expression is of type character varying
  İpucu: You will need to rewrite or cast the expression.
  Position: 91
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T13:41:46.686+03:00  INFO 3360 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T13:58:19.196+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T13:58:19.197+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T13:58:19.197+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-5cb12d36-966e-4d02-89f3-ad9457971bcc sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T13:58:19.198+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T13:58:19.198+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T13:58:19.198+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T13:58:19.200+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T13:58:19.201+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T13:58:19.593+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T13:58:19.593+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T13:58:19.593+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T13:58:19.593+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T13:58:19.597+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T13:58:19.598+03:00  INFO 3360 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T13:58:19.607+03:00  INFO 3360 --- [kafka] [SpringApplicationShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-15T13:58:19.611+03:00  INFO 3360 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T13:58:19.611+03:00  INFO 3360 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T13:58:19.611+03:00  INFO 3360 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T13:58:19.611+03:00  INFO 3360 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T13:58:19.611+03:00  INFO 3360 --- [kafka] [SpringApplicationShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for kafka-producer-1 unregistered
2025-01-15T13:58:19.615+03:00  INFO 3360 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T13:58:19.618+03:00  INFO 3360 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T13:58:19.637+03:00  INFO 3360 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T13:58:24.635+03:00  INFO 22456 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 22456 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T13:58:24.639+03:00  INFO 22456 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T13:58:24.715+03:00  INFO 22456 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T13:58:24.715+03:00  INFO 22456 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T13:58:25.859+03:00  INFO 22456 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T13:58:25.940+03:00  INFO 22456 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 72 ms. Found 5 JPA repository interfaces.
2025-01-15T13:58:26.919+03:00  INFO 22456 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T13:58:26.940+03:00  INFO 22456 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T13:58:26.941+03:00  INFO 22456 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T13:58:27.002+03:00  INFO 22456 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T13:58:27.003+03:00  INFO 22456 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2287 ms
2025-01-15T13:58:27.264+03:00  INFO 22456 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T13:58:27.350+03:00  INFO 22456 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T13:58:27.405+03:00  INFO 22456 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T13:58:27.792+03:00  INFO 22456 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T13:58:27.823+03:00  INFO 22456 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T13:58:28.193+03:00  INFO 22456 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@17124d64
2025-01-15T13:58:28.195+03:00  INFO 22456 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T13:58:29.332+03:00  INFO 22456 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T13:58:29.335+03:00  INFO 22456 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T13:58:30.204+03:00  WARN 22456 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T13:58:30.838+03:00  INFO 22456 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T13:58:30.937+03:00  INFO 22456 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T13:58:30.984+03:00  INFO 22456 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T13:58:31.041+03:00  INFO 22456 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T13:58:31.363+03:00  INFO 22456 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T13:58:31.363+03:00  INFO 22456 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T13:58:31.363+03:00  INFO 22456 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736938711361
2025-01-15T13:58:31.369+03:00  INFO 22456 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T13:58:31.397+03:00  INFO 22456 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.399 seconds (process running for 8.131)
2025-01-15T13:58:31.645+03:00  INFO 22456 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T13:58:31.646+03:00  INFO 22456 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T13:58:31.686+03:00  INFO 22456 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T13:58:31.686+03:00  INFO 22456 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T13:58:31.686+03:00  INFO 22456 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736938711686
2025-01-15T13:58:31.687+03:00  INFO 22456 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T13:58:32.027+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T13:58:32.027+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T13:58:32.028+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T13:58:32.028+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T13:58:32.034+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T13:58:32.034+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T13:58:32.099+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-25270f88-5d64-4e97-bd3e-3ea21dca364f
2025-01-15T13:58:32.099+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T13:58:32.099+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-2b560e43-295d-4f9b-b2c3-1e0047a50c3f
2025-01-15T13:58:32.100+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T13:58:35.105+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=65, memberId='consumer-my-group-1-25270f88-5d64-4e97-bd3e-3ea21dca364f', protocol='range'}
2025-01-15T13:58:35.113+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 65: {consumer-my-group-1-25270f88-5d64-4e97-bd3e-3ea21dca364f=Assignment(partitions=[test-topic-0])}
2025-01-15T13:58:35.126+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=65, memberId='consumer-my-group-1-25270f88-5d64-4e97-bd3e-3ea21dca364f', protocol='range'}
2025-01-15T13:58:35.127+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T13:58:35.129+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T13:58:35.145+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=26, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T13:58:35.147+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T13:59:04.274+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=21, memberId='consumer-dynamic-group-2-2b560e43-295d-4f9b-b2c3-1e0047a50c3f', protocol='range'}
2025-01-15T13:59:04.275+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 21: {consumer-dynamic-group-2-2b560e43-295d-4f9b-b2c3-1e0047a50c3f=Assignment(partitions=[test-topic-0])}
2025-01-15T13:59:04.284+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=21, memberId='consumer-dynamic-group-2-2b560e43-295d-4f9b-b2c3-1e0047a50c3f', protocol='range'}
2025-01-15T13:59:04.284+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T13:59:04.284+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T13:59:04.288+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T13:59:04.290+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=26, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T13:59:08.018+03:00  INFO 22456 --- [kafka] [http-nio-8080-exec-2] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T13:59:08.018+03:00  INFO 22456 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T13:59:08.019+03:00  INFO 22456 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Completed initialization in 1 ms
2025-01-15T13:59:08.099+03:00  INFO 22456 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-01-15T13:59:08.101+03:00  INFO 22456 --- [kafka] [http-nio-8080-exec-2] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T13:59:08.131+03:00  INFO 22456 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Instantiated an idempotent producer.
2025-01-15T13:59:08.190+03:00  INFO 22456 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T13:59:08.191+03:00  INFO 22456 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T13:59:08.191+03:00  INFO 22456 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736938748190
2025-01-15T13:59:08.226+03:00  INFO 22456 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=kafka-producer-1] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T13:59:08.227+03:00  INFO 22456 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=kafka-producer-1] ProducerId set to 3011 with epoch 0
2025-01-15T13:59:08.331+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T13:59:08.588+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T13:59:08.591+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T13:59:08.685+03:00  WARN 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T13:59:08.686+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T13:59:08.686+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T13:59:08.686+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T13:59:08.687+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T13:59:08.690+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T13:59:08.856+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T13:59:08.879+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : INSERT Operation - Inserted Rows: 1
2025-01-15T13:59:08.883+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T13:59:08.885+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T13:59:08.907+03:00  WARN 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T13:59:08.908+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T13:59:08.908+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T13:59:08.908+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T13:59:08.908+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T13:59:08.908+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T13:59:09.054+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T13:59:09.057+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : INSERT Operation - Inserted Rows: 1
2025-01-15T13:59:09.058+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T13:59:25.957+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T13:59:25.980+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T13:59:25.982+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T13:59:26.002+03:00  WARN 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T13:59:26.002+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T13:59:26.002+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T13:59:26.003+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T13:59:26.003+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T13:59:26.003+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T13:59:26.144+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T13:59:26.153+03:00  WARN 22456 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 23505
2025-01-15T13:59:26.154+03:00 ERROR 22456 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: duplicate key value violates unique constraint "other_test_table_pk"
  Ayrıntı: Key (id)=(1) already exists.
2025-01-15T13:59:26.160+03:00 ERROR 22456 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: other_test_table

org.hibernate.exception.ConstraintViolationException: JDBC exception executing SQL [INSERT INTO other_test_table (surname, name, phone_number, id, email) VALUES (?, ?, ?, ?, ?)] [ERROR: duplicate key value violates unique constraint "other_test_table_pk"
  Ayrıntı: Key (id)=(1) already exists.] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:97) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:167) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:130) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "other_test_table_pk"
  Ayrıntı: Key (id)=(1) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T13:59:26.171+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T13:59:26.173+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T13:59:26.193+03:00  WARN 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T13:59:26.194+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T13:59:26.194+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T13:59:26.194+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T13:59:26.194+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T13:59:26.194+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T13:59:26.306+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T13:59:26.309+03:00  WARN 22456 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 23505
2025-01-15T13:59:26.309+03:00 ERROR 22456 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: duplicate key value violates unique constraint "test_table_pk"
  Ayrıntı: Key (id)=(1) already exists.
2025-01-15T13:59:26.310+03:00 ERROR 22456 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error occurred during INSERT operation on table: test_table

org.hibernate.exception.ConstraintViolationException: JDBC exception executing SQL [INSERT INTO test_table (full_name, id, email) VALUES (?, ?, ?)] [ERROR: duplicate key value violates unique constraint "test_table_pk"
  Ayrıntı: Key (id)=(1) already exists.] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:97) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:104) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:892) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:651) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at com.demo.kafka.service.DynamicKafkaConsumer.performInsert(DynamicKafkaConsumer.java:167) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.handleInsert(DynamicKafkaConsumer.java:130) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:102) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "test_table_pk"
  Ayrıntı: Key (id)=(1) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:155) ~[postgresql-42.7.4.jar:42.7.4]
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:90) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 21 common frames omitted

2025-01-15T13:59:26.312+03:00  INFO 22456 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T14:06:18.442+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T14:06:18.442+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T14:06:18.443+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-25270f88-5d64-4e97-bd3e-3ea21dca364f sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T14:06:18.444+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T14:06:18.444+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T14:06:18.444+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T14:06:18.446+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T14:06:18.447+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T14:06:18.890+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T14:06:18.890+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T14:06:18.890+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T14:06:18.891+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T14:06:18.899+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T14:06:18.899+03:00  INFO 22456 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T14:06:18.920+03:00  INFO 22456 --- [kafka] [SpringApplicationShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-15T14:06:18.926+03:00  INFO 22456 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T14:06:18.926+03:00  INFO 22456 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T14:06:18.927+03:00  INFO 22456 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T14:06:18.927+03:00  INFO 22456 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T14:06:18.927+03:00  INFO 22456 --- [kafka] [SpringApplicationShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for kafka-producer-1 unregistered
2025-01-15T14:06:18.930+03:00  INFO 22456 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T14:06:18.934+03:00  INFO 22456 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T14:06:18.954+03:00  INFO 22456 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T14:12:57.747+03:00  INFO 20964 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 20964 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T14:12:57.753+03:00  INFO 20964 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T14:12:57.854+03:00  INFO 20964 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T14:12:57.855+03:00  INFO 20964 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T14:12:59.134+03:00  INFO 20964 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T14:12:59.218+03:00  INFO 20964 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 73 ms. Found 5 JPA repository interfaces.
2025-01-15T14:13:00.284+03:00  INFO 20964 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T14:13:00.309+03:00  INFO 20964 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T14:13:00.310+03:00  INFO 20964 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T14:13:00.378+03:00  INFO 20964 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T14:13:00.378+03:00  INFO 20964 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2523 ms
2025-01-15T14:13:00.751+03:00  INFO 20964 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T14:13:00.851+03:00  INFO 20964 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T14:13:00.931+03:00  INFO 20964 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T14:13:01.403+03:00  INFO 20964 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T14:13:01.455+03:00  INFO 20964 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T14:13:01.845+03:00  INFO 20964 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@a4b211b
2025-01-15T14:13:01.848+03:00  INFO 20964 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T14:13:03.146+03:00  INFO 20964 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T14:13:03.149+03:00  INFO 20964 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T14:13:04.045+03:00  WARN 20964 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T14:13:04.777+03:00  INFO 20964 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T14:13:04.889+03:00  INFO 20964 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T14:13:04.946+03:00  INFO 20964 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T14:13:04.999+03:00  INFO 20964 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T14:13:05.359+03:00  INFO 20964 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T14:13:05.360+03:00  INFO 20964 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T14:13:05.360+03:00  INFO 20964 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736939585357
2025-01-15T14:13:05.365+03:00  INFO 20964 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T14:13:05.396+03:00  INFO 20964 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 8.648 seconds (process running for 9.54)
2025-01-15T14:13:05.703+03:00  INFO 20964 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T14:13:05.704+03:00  INFO 20964 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T14:13:05.752+03:00  INFO 20964 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T14:13:05.752+03:00  INFO 20964 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T14:13:05.752+03:00  INFO 20964 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736939585752
2025-01-15T14:13:05.753+03:00  INFO 20964 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T14:13:06.111+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T14:13:06.111+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T14:13:06.112+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T14:13:06.112+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T14:13:06.118+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T14:13:06.118+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T14:13:06.173+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-cdfb3bb8-eacb-46b9-8410-e7df6275e8ed
2025-01-15T14:13:06.173+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-e4164778-f480-4d38-8cd0-8ca6fa2d4e19
2025-01-15T14:13:06.174+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T14:13:06.174+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T14:13:09.179+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=67, memberId='consumer-my-group-1-e4164778-f480-4d38-8cd0-8ca6fa2d4e19', protocol='range'}
2025-01-15T14:13:09.179+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=23, memberId='consumer-dynamic-group-2-cdfb3bb8-eacb-46b9-8410-e7df6275e8ed', protocol='range'}
2025-01-15T14:13:09.190+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 67: {consumer-my-group-1-e4164778-f480-4d38-8cd0-8ca6fa2d4e19=Assignment(partitions=[test-topic-0])}
2025-01-15T14:13:09.190+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 23: {consumer-dynamic-group-2-cdfb3bb8-eacb-46b9-8410-e7df6275e8ed=Assignment(partitions=[test-topic-0])}
2025-01-15T14:13:09.203+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=23, memberId='consumer-dynamic-group-2-cdfb3bb8-eacb-46b9-8410-e7df6275e8ed', protocol='range'}
2025-01-15T14:13:09.203+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=67, memberId='consumer-my-group-1-e4164778-f480-4d38-8cd0-8ca6fa2d4e19', protocol='range'}
2025-01-15T14:13:09.204+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T14:13:09.204+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T14:13:09.207+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T14:13:09.207+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T14:13:09.217+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T14:13:09.228+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=28, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T14:13:09.228+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=28, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T14:13:09.231+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T14:13:18.655+03:00  INFO 20964 --- [kafka] [http-nio-8080-exec-2] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T14:13:18.656+03:00  INFO 20964 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T14:13:18.657+03:00  INFO 20964 --- [kafka] [http-nio-8080-exec-2] o.s.web.servlet.DispatcherServlet        : Completed initialization in 1 ms
2025-01-15T14:13:18.724+03:00  INFO 20964 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-01-15T14:13:18.726+03:00  INFO 20964 --- [kafka] [http-nio-8080-exec-2] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T14:13:18.736+03:00  INFO 20964 --- [kafka] [http-nio-8080-exec-2] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Instantiated an idempotent producer.
2025-01-15T14:13:18.778+03:00  INFO 20964 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T14:13:18.778+03:00  INFO 20964 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T14:13:18.778+03:00  INFO 20964 --- [kafka] [http-nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736939598778
2025-01-15T14:13:18.808+03:00  INFO 20964 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=kafka-producer-1] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T14:13:18.809+03:00  INFO 20964 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=kafka-producer-1] ProducerId set to 3012 with epoch 0
2025-01-15T14:13:18.889+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T14:13:19.058+03:00  WARN 20964 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Error: 0, SQLState: 42703
2025-01-15T14:13:19.059+03:00 ERROR 20964 --- [kafka] [noBeanNameSet-0-C-1] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: column c1_0.is_primary_key does not exist
  İpucu: Perhaps you meant to reference the column "c1_0.isprimarykey".
  Position: 16
2025-01-15T14:13:19.063+03:00  INFO 20964 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.internal.DefaultLoadEventListener  : HHH000327: Error performing load command

org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [select c1_0.id,c1_0.is_primary_key,c1_0.name,c1_0.table_id,t1_0.id,t1_0.database_id,d1_0.id,d1_0.connection_url,d1_0.name,d1_0.password,d1_0.username,t1_0.name from columns c1_0 join tables t1_0 on t1_0.id=c1_0.table_id left join databases d1_0 on d1_0.id=t1_0.database_id where c1_0.id=?] [ERROR: column c1_0.is_primary_key does not exist
  İpucu: Perhaps you meant to reference the column "c1_0.isprimarykey".
  Position: 16] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.DeferredResultSetAccess.executeQuery(DeferredResultSetAccess.java:264) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.DeferredResultSetAccess.getResultSet(DeferredResultSetAccess.java:167) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.JdbcValuesResultSetImpl.advanceNext(JdbcValuesResultSetImpl.java:265) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.JdbcValuesResultSetImpl.processNext(JdbcValuesResultSetImpl.java:145) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.AbstractJdbcValues.next(AbstractJdbcValues.java:19) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.internal.RowProcessingStateStandardImpl.next(RowProcessingStateStandardImpl.java:67) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.spi.ListResultsConsumer.consume(ListResultsConsumer.java:182) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.spi.ListResultsConsumer.consume(ListResultsConsumer.java:33) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.JdbcSelectExecutorStandardImpl.doExecuteQuery(JdbcSelectExecutorStandardImpl.java:211) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.JdbcSelectExecutorStandardImpl.executeQuery(JdbcSelectExecutorStandardImpl.java:83) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.spi.JdbcSelectExecutor.list(JdbcSelectExecutor.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.spi.JdbcSelectExecutor.list(JdbcSelectExecutor.java:65) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.loader.ast.internal.SingleIdLoadPlan.load(SingleIdLoadPlan.java:145) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.loader.ast.internal.SingleIdLoadPlan.load(SingleIdLoadPlan.java:117) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.loader.ast.internal.SingleIdEntityLoaderStandardImpl.load(SingleIdEntityLoaderStandardImpl.java:75) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.persister.entity.AbstractEntityPersister.doLoad(AbstractEntityPersister.java:3726) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.persister.entity.AbstractEntityPersister.load(AbstractEntityPersister.java:3715) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.loadFromDatasource(DefaultLoadEventListener.java:604) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.loadFromCacheOrDatasource(DefaultLoadEventListener.java:590) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.load(DefaultLoadEventListener.java:560) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.doLoad(DefaultLoadEventListener.java:544) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.load(DefaultLoadEventListener.java:207) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.loadWithRegularProxy(DefaultLoadEventListener.java:290) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.proxyOrLoad(DefaultLoadEventListener.java:242) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.doOnLoad(DefaultLoadEventListener.java:111) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.onLoad(DefaultLoadEventListener.java:68) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.service.internal.EventListenerGroupImpl.fireEventOnEachListener(EventListenerGroupImpl.java:138) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.internal.SessionImpl.fireLoadNoChecks(SessionImpl.java:1225) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.internal.SessionImpl.internalLoad(SessionImpl.java:1071) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.graph.entity.internal.EntitySelectFetchInitializer.initializeInstance(EntitySelectFetchInitializer.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.internal.InitializersList.initializeInstance(InitializersList.java:73) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.internal.StandardRowReader.coordinateInitializers(StandardRowReader.java:113) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.internal.StandardRowReader.readRow(StandardRowReader.java:87) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.spi.ListResultsConsumer.consume(ListResultsConsumer.java:205) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.spi.ListResultsConsumer.consume(ListResultsConsumer.java:33) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.JdbcSelectExecutorStandardImpl.doExecuteQuery(JdbcSelectExecutorStandardImpl.java:211) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.JdbcSelectExecutorStandardImpl.executeQuery(JdbcSelectExecutorStandardImpl.java:83) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.spi.JdbcSelectExecutor.list(JdbcSelectExecutor.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.spi.JdbcSelectExecutor.list(JdbcSelectExecutor.java:65) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.ConcreteSqmSelectQueryPlan.lambda$new$2(ConcreteSqmSelectQueryPlan.java:139) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.ConcreteSqmSelectQueryPlan.withCacheableSqmInterpretation(ConcreteSqmSelectQueryPlan.java:382) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.ConcreteSqmSelectQueryPlan.performList(ConcreteSqmSelectQueryPlan.java:302) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.QuerySqmImpl.doList(QuerySqmImpl.java:526) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractSelectionQuery.list(AbstractSelectionQuery.java:423) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.Query.getResultList(Query.java:120) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[na:na]
	at org.springframework.orm.jpa.SharedEntityManagerCreator$DeferredQueryInvocationHandler.invoke(SharedEntityManagerCreator.java:418) ~[spring-orm-6.1.12.jar:6.1.12]
	at jdk.proxy4/jdk.proxy4.$Proxy156.getResultList(Unknown Source) ~[na:na]
	at org.springframework.data.jpa.repository.query.JpaQueryExecution$CollectionExecution.doExecute(JpaQueryExecution.java:129) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at org.springframework.data.jpa.repository.query.JpaQueryExecution.execute(JpaQueryExecution.java:92) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at org.springframework.data.jpa.repository.query.AbstractJpaQuery.doExecute(AbstractJpaQuery.java:152) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at org.springframework.data.jpa.repository.query.AbstractJpaQuery.execute(AbstractJpaQuery.java:140) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:170) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:158) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:169) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:148) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:70) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:379) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:119) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:138) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.data.jpa.repository.support.CrudMethodMetadataPostProcessor$CrudMethodMetadataPopulatingMethodInterceptor.invoke(CrudMethodMetadataPostProcessor.java:136) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:223) ~[spring-aop-6.1.12.jar:6.1.12]
	at jdk.proxy4/jdk.proxy4.$Proxy135.findByTopicName(Unknown Source) ~[na:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:84) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.postgresql.util.PSQLException: ERROR: column c1_0.is_primary_key does not exist
  İpucu: Perhaps you meant to reference the column "c1_0.isprimarykey".
  Position: 16
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:137) ~[postgresql-42.7.4.jar:42.7.4]
	at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeQuery(ProxyPreparedStatement.java:52) ~[HikariCP-5.1.0.jar:na]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeQuery(HikariProxyPreparedStatement.java) ~[HikariCP-5.1.0.jar:na]
	at org.hibernate.sql.results.jdbc.internal.DeferredResultSetAccess.executeQuery(DeferredResultSetAccess.java:246) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 86 common frames omitted

2025-01-15T14:13:19.084+03:00 ERROR 20964 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Error processing message from topic: test-topic, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}


org.springframework.dao.InvalidDataAccessResourceUsageException: JDBC exception executing SQL [select c1_0.id,c1_0.is_primary_key,c1_0.name,c1_0.table_id,t1_0.id,t1_0.database_id,d1_0.id,d1_0.connection_url,d1_0.name,d1_0.password,d1_0.username,t1_0.name from columns c1_0 join tables t1_0 on t1_0.id=c1_0.table_id left join databases d1_0 on d1_0.id=t1_0.database_id where c1_0.id=?] [ERROR: column c1_0.is_primary_key does not exist
  İpucu: Perhaps you meant to reference the column "c1_0.isprimarykey".
  Position: 16] [n/a]; SQL [n/a]
	at org.springframework.orm.jpa.vendor.HibernateJpaDialect.convertHibernateAccessException(HibernateJpaDialect.java:277) ~[spring-orm-6.1.12.jar:6.1.12]
	at org.springframework.orm.jpa.vendor.HibernateJpaDialect.translateExceptionIfPossible(HibernateJpaDialect.java:241) ~[spring-orm-6.1.12.jar:6.1.12]
	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.translateExceptionIfPossible(AbstractEntityManagerFactoryBean.java:550) ~[spring-orm-6.1.12.jar:6.1.12]
	at org.springframework.dao.support.ChainedPersistenceExceptionTranslator.translateExceptionIfPossible(ChainedPersistenceExceptionTranslator.java:61) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.dao.support.DataAccessUtils.translateIfNecessary(DataAccessUtils.java:335) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:160) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.data.jpa.repository.support.CrudMethodMetadataPostProcessor$CrudMethodMetadataPopulatingMethodInterceptor.invoke(CrudMethodMetadataPostProcessor.java:136) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:223) ~[spring-aop-6.1.12.jar:6.1.12]
	at jdk.proxy4/jdk.proxy4.$Proxy135.findByTopicName(Unknown Source) ~[na:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.processMessage(DynamicKafkaConsumer.java:84) ~[classes/:na]
	at com.demo.kafka.service.DynamicKafkaConsumer.lambda$subscribeToTopics$0(DynamicKafkaConsumer.java:57) ~[classes/:na]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2810) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701) ~[spring-kafka-3.2.3.jar:3.2.3]
	at io.micrometer.observation.Observation.observe(Observation.java:565) ~[micrometer-observation-1.13.3.jar:1.13.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426) ~[spring-kafka-3.2.3.jar:3.2.3]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296) ~[spring-kafka-3.2.3.jar:3.2.3]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1804) ~[na:na]
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) ~[na:na]
	at java.base/java.lang.Thread.run(Thread.java:1570) ~[na:na]
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [select c1_0.id,c1_0.is_primary_key,c1_0.name,c1_0.table_id,t1_0.id,t1_0.database_id,d1_0.id,d1_0.connection_url,d1_0.name,d1_0.password,d1_0.username,t1_0.name from columns c1_0 join tables t1_0 on t1_0.id=c1_0.table_id left join databases d1_0 on d1_0.id=t1_0.database_id where c1_0.id=?] [ERROR: column c1_0.is_primary_key does not exist
  İpucu: Perhaps you meant to reference the column "c1_0.isprimarykey".
  Position: 16] [n/a]
	at org.hibernate.exception.internal.SQLStateConversionDelegate.convert(SQLStateConversionDelegate.java:91) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.DeferredResultSetAccess.executeQuery(DeferredResultSetAccess.java:264) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.DeferredResultSetAccess.getResultSet(DeferredResultSetAccess.java:167) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.JdbcValuesResultSetImpl.advanceNext(JdbcValuesResultSetImpl.java:265) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.JdbcValuesResultSetImpl.processNext(JdbcValuesResultSetImpl.java:145) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.jdbc.internal.AbstractJdbcValues.next(AbstractJdbcValues.java:19) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.internal.RowProcessingStateStandardImpl.next(RowProcessingStateStandardImpl.java:67) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.spi.ListResultsConsumer.consume(ListResultsConsumer.java:182) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.spi.ListResultsConsumer.consume(ListResultsConsumer.java:33) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.JdbcSelectExecutorStandardImpl.doExecuteQuery(JdbcSelectExecutorStandardImpl.java:211) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.JdbcSelectExecutorStandardImpl.executeQuery(JdbcSelectExecutorStandardImpl.java:83) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.spi.JdbcSelectExecutor.list(JdbcSelectExecutor.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.spi.JdbcSelectExecutor.list(JdbcSelectExecutor.java:65) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.loader.ast.internal.SingleIdLoadPlan.load(SingleIdLoadPlan.java:145) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.loader.ast.internal.SingleIdLoadPlan.load(SingleIdLoadPlan.java:117) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.loader.ast.internal.SingleIdEntityLoaderStandardImpl.load(SingleIdEntityLoaderStandardImpl.java:75) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.persister.entity.AbstractEntityPersister.doLoad(AbstractEntityPersister.java:3726) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.persister.entity.AbstractEntityPersister.load(AbstractEntityPersister.java:3715) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.loadFromDatasource(DefaultLoadEventListener.java:604) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.loadFromCacheOrDatasource(DefaultLoadEventListener.java:590) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.load(DefaultLoadEventListener.java:560) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.doLoad(DefaultLoadEventListener.java:544) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.load(DefaultLoadEventListener.java:207) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.loadWithRegularProxy(DefaultLoadEventListener.java:290) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.proxyOrLoad(DefaultLoadEventListener.java:242) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.doOnLoad(DefaultLoadEventListener.java:111) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.internal.DefaultLoadEventListener.onLoad(DefaultLoadEventListener.java:68) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.event.service.internal.EventListenerGroupImpl.fireEventOnEachListener(EventListenerGroupImpl.java:138) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.internal.SessionImpl.fireLoadNoChecks(SessionImpl.java:1225) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.internal.SessionImpl.internalLoad(SessionImpl.java:1071) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.graph.entity.internal.EntitySelectFetchInitializer.initializeInstance(EntitySelectFetchInitializer.java:215) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.internal.InitializersList.initializeInstance(InitializersList.java:73) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.internal.StandardRowReader.coordinateInitializers(StandardRowReader.java:113) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.internal.StandardRowReader.readRow(StandardRowReader.java:87) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.spi.ListResultsConsumer.consume(ListResultsConsumer.java:205) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.results.spi.ListResultsConsumer.consume(ListResultsConsumer.java:33) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.JdbcSelectExecutorStandardImpl.doExecuteQuery(JdbcSelectExecutorStandardImpl.java:211) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.internal.JdbcSelectExecutorStandardImpl.executeQuery(JdbcSelectExecutorStandardImpl.java:83) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.spi.JdbcSelectExecutor.list(JdbcSelectExecutor.java:76) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.sql.exec.spi.JdbcSelectExecutor.list(JdbcSelectExecutor.java:65) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.ConcreteSqmSelectQueryPlan.lambda$new$2(ConcreteSqmSelectQueryPlan.java:139) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.ConcreteSqmSelectQueryPlan.withCacheableSqmInterpretation(ConcreteSqmSelectQueryPlan.java:382) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.ConcreteSqmSelectQueryPlan.performList(ConcreteSqmSelectQueryPlan.java:302) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.sqm.internal.QuerySqmImpl.doList(QuerySqmImpl.java:526) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.spi.AbstractSelectionQuery.list(AbstractSelectionQuery.java:423) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at org.hibernate.query.Query.getResultList(Query.java:120) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:580) ~[na:na]
	at org.springframework.orm.jpa.SharedEntityManagerCreator$DeferredQueryInvocationHandler.invoke(SharedEntityManagerCreator.java:418) ~[spring-orm-6.1.12.jar:6.1.12]
	at jdk.proxy4/jdk.proxy4.$Proxy156.getResultList(Unknown Source) ~[na:na]
	at org.springframework.data.jpa.repository.query.JpaQueryExecution$CollectionExecution.doExecute(JpaQueryExecution.java:129) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at org.springframework.data.jpa.repository.query.JpaQueryExecution.execute(JpaQueryExecution.java:92) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at org.springframework.data.jpa.repository.query.AbstractJpaQuery.doExecute(AbstractJpaQuery.java:152) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at org.springframework.data.jpa.repository.query.AbstractJpaQuery.execute(AbstractJpaQuery.java:140) ~[spring-data-jpa-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:170) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:158) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:169) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:148) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:70) ~[spring-data-commons-3.3.3.jar:3.3.3]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:379) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:119) ~[spring-tx-6.1.12.jar:6.1.12]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.1.12.jar:6.1.12]
	at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:138) ~[spring-tx-6.1.12.jar:6.1.12]
	... 23 common frames omitted
Caused by: org.postgresql.util.PSQLException: ERROR: column c1_0.is_primary_key does not exist
  İpucu: Perhaps you meant to reference the column "c1_0.isprimarykey".
  Position: 16
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194) ~[postgresql-42.7.4.jar:42.7.4]
	at org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:137) ~[postgresql-42.7.4.jar:42.7.4]
	at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeQuery(ProxyPreparedStatement.java:52) ~[HikariCP-5.1.0.jar:na]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeQuery(HikariProxyPreparedStatement.java) ~[HikariCP-5.1.0.jar:na]
	at org.hibernate.sql.results.jdbc.internal.DeferredResultSetAccess.executeQuery(DeferredResultSetAccess.java:246) ~[hibernate-core-6.5.2.Final.jar:6.5.2.Final]
	... 86 common frames omitted

2025-01-15T14:15:21.380+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T14:15:21.381+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T14:15:21.381+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-e4164778-f480-4d38-8cd0-8ca6fa2d4e19 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T14:15:21.382+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T14:15:21.382+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T14:15:21.382+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T14:15:21.384+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T14:15:21.385+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T14:15:21.468+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T14:15:21.468+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T14:15:21.469+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T14:15:21.469+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T14:15:21.481+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T14:15:21.482+03:00  INFO 20964 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T14:15:21.537+03:00  INFO 20964 --- [kafka] [SpringApplicationShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-15T14:15:21.545+03:00  INFO 20964 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T14:15:21.545+03:00  INFO 20964 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T14:15:21.545+03:00  INFO 20964 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T14:15:21.545+03:00  INFO 20964 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T14:15:21.546+03:00  INFO 20964 --- [kafka] [SpringApplicationShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for kafka-producer-1 unregistered
2025-01-15T14:15:21.551+03:00  INFO 20964 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T14:15:21.557+03:00  INFO 20964 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T14:15:21.582+03:00  INFO 20964 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T14:15:26.238+03:00  INFO 11012 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 11012 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T14:15:26.242+03:00  INFO 11012 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T14:15:26.319+03:00  INFO 11012 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T14:15:26.320+03:00  INFO 11012 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T14:15:27.475+03:00  INFO 11012 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T14:15:27.565+03:00  INFO 11012 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 77 ms. Found 5 JPA repository interfaces.
2025-01-15T14:15:28.543+03:00  INFO 11012 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T14:15:28.567+03:00  INFO 11012 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T14:15:28.567+03:00  INFO 11012 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T14:15:28.627+03:00  INFO 11012 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T14:15:28.627+03:00  INFO 11012 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2307 ms
2025-01-15T14:15:28.885+03:00  INFO 11012 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T14:15:28.960+03:00  INFO 11012 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T14:15:29.025+03:00  INFO 11012 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T14:15:29.425+03:00  INFO 11012 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T14:15:29.461+03:00  INFO 11012 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T14:15:29.822+03:00  INFO 11012 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@17124d64
2025-01-15T14:15:29.825+03:00  INFO 11012 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T14:15:30.937+03:00  INFO 11012 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T14:15:30.940+03:00  INFO 11012 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T14:15:31.762+03:00  WARN 11012 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T14:15:32.378+03:00  INFO 11012 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T14:15:32.477+03:00  INFO 11012 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T14:15:32.528+03:00  INFO 11012 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T14:15:32.586+03:00  INFO 11012 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T14:15:32.936+03:00  INFO 11012 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T14:15:32.937+03:00  INFO 11012 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T14:15:32.937+03:00  INFO 11012 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736939732934
2025-01-15T14:15:32.943+03:00  INFO 11012 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T14:15:32.969+03:00  INFO 11012 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.391 seconds (process running for 8.108)
2025-01-15T14:15:33.201+03:00  INFO 11012 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T14:15:33.202+03:00  INFO 11012 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T14:15:33.248+03:00  INFO 11012 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T14:15:33.248+03:00  INFO 11012 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T14:15:33.248+03:00  INFO 11012 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736939733247
2025-01-15T14:15:33.249+03:00  INFO 11012 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T14:15:33.599+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T14:15:33.599+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T14:15:33.600+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T14:15:33.600+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T14:15:33.605+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T14:15:33.605+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T14:15:33.665+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-a3d1893d-ce59-4522-b464-0be6833c9349
2025-01-15T14:15:33.666+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T14:15:33.670+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-4f6c10fd-51e1-4c3e-ac79-61b294849312
2025-01-15T14:15:33.671+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T14:15:36.678+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=69, memberId='consumer-my-group-1-4f6c10fd-51e1-4c3e-ac79-61b294849312', protocol='range'}
2025-01-15T14:15:36.686+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 69: {consumer-my-group-1-4f6c10fd-51e1-4c3e-ac79-61b294849312=Assignment(partitions=[test-topic-0])}
2025-01-15T14:15:36.699+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=69, memberId='consumer-my-group-1-4f6c10fd-51e1-4c3e-ac79-61b294849312', protocol='range'}
2025-01-15T14:15:36.700+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T14:15:36.702+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T14:15:36.716+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=29, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T14:15:36.717+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T14:15:39.664+03:00  INFO 11012 --- [kafka] [http-nio-8080-exec-3] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T14:15:39.664+03:00  INFO 11012 --- [kafka] [http-nio-8080-exec-3] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T14:15:39.666+03:00  INFO 11012 --- [kafka] [http-nio-8080-exec-3] o.s.web.servlet.DispatcherServlet        : Completed initialization in 1 ms
2025-01-15T14:15:39.755+03:00  INFO 11012 --- [kafka] [http-nio-8080-exec-3] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-01-15T14:15:39.757+03:00  INFO 11012 --- [kafka] [http-nio-8080-exec-3] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T14:15:39.770+03:00  INFO 11012 --- [kafka] [http-nio-8080-exec-3] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Instantiated an idempotent producer.
2025-01-15T14:15:39.814+03:00  INFO 11012 --- [kafka] [http-nio-8080-exec-3] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T14:15:39.815+03:00  INFO 11012 --- [kafka] [http-nio-8080-exec-3] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T14:15:39.815+03:00  INFO 11012 --- [kafka] [http-nio-8080-exec-3] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736939739814
2025-01-15T14:15:39.848+03:00  INFO 11012 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=kafka-producer-1] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T14:15:39.849+03:00  INFO 11012 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=kafka-producer-1] ProducerId set to 3013 with epoch 0
2025-01-15T14:16:06.252+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=24, memberId='consumer-dynamic-group-2-a3d1893d-ce59-4522-b464-0be6833c9349', protocol='range'}
2025-01-15T14:16:06.252+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 24: {consumer-dynamic-group-2-a3d1893d-ce59-4522-b464-0be6833c9349=Assignment(partitions=[test-topic-0])}
2025-01-15T14:16:06.265+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=24, memberId='consumer-dynamic-group-2-a3d1893d-ce59-4522-b464-0be6833c9349', protocol='range'}
2025-01-15T14:16:06.265+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T14:16:06.265+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T14:16:06.270+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T14:16:06.274+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=29, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T14:16:06.303+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Processing message from topic: test-topic, key: null, value: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T14:16:06.498+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T14:16:06.502+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T14:16:06.584+03:00  WARN 11012 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T14:16:06.585+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T14:16:06.585+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T14:16:06.585+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T14:16:06.585+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T14:16:06.588+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T14:16:06.736+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T14:16:06.783+03:00  WARN 11012 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Record with primary key already exists in table test_table
2025-01-15T14:16:06.786+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: dynamic-persistence-unit]
2025-01-15T14:16:06.788+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T14:16:06.813+03:00  WARN 11012 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001002: Using built-in connection pool (not intended for production use)
2025-01-15T14:16:06.814+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001005: Loaded JDBC driver class: org.postgresql.Driver
2025-01-15T14:16:06.814+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001012: Connecting with JDBC URL [jdbc:postgresql://10.0.4.54:5433/kafka-db]
2025-01-15T14:16:06.814+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001001: Connection properties: {password=****, user=45868582848}
2025-01-15T14:16:06.814+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001003: Autocommit mode: false
2025-01-15T14:16:06.814+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] org.hibernate.orm.connections.pooling    : HHH10001115: Connection pool size: 20 (min=1)
2025-01-15T14:16:06.970+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T14:16:06.975+03:00  WARN 11012 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Record with primary key already exists in table other_test_table
2025-01-15T14:16:06.975+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] c.d.kafka.service.DynamicKafkaConsumer   : Message processed successfully: {
  "payload": {
    "op": "c", 
    "before": null,
    "after": {
      "id": 1,
      "name": "Metehan",
      "surname": "Altuner",
      "email": "metehan.altuner@gmail.com",
      "phone_number": "5413494038"
    },
    "source": {
      "version": "2.2.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1673626800000,
      "snapshot": "false",
      "db": "exampledb",
      "sequence": null,
      "table": "users",
      "server_id": 223344,
      "gtid": null,
      "file": "binlog.000003",
      "pos": 156,
      "row": 0,
      "thread": null,
      "query": null
    },
    "ts_ms": 1673626800500,
    "transaction": null
  }
}

2025-01-15T14:24:33.758+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-my-group-1, groupId=my-group] Node -1 disconnected.
2025-01-15T14:24:34.018+03:00  INFO 11012 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Node -1 disconnected.
2025-01-15T14:24:39.857+03:00  INFO 11012 --- [kafka] [kafka-producer-network-thread | kafka-producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=kafka-producer-1] Node -1 disconnected.
2025-01-15T14:30:35.211+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T14:30:35.212+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T14:30:35.213+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-4f6c10fd-51e1-4c3e-ac79-61b294849312 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T14:30:35.213+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T14:30:35.213+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T14:30:35.214+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T14:30:35.216+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T14:30:35.216+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T14:30:35.661+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T14:30:35.661+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T14:30:35.661+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T14:30:35.661+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T14:30:35.667+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T14:30:35.668+03:00  INFO 11012 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T14:30:35.685+03:00  INFO 11012 --- [kafka] [SpringApplicationShutdownHook] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=kafka-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-01-15T14:30:35.691+03:00  INFO 11012 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T14:30:35.691+03:00  INFO 11012 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T14:30:35.691+03:00  INFO 11012 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T14:30:35.691+03:00  INFO 11012 --- [kafka] [SpringApplicationShutdownHook] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T14:30:35.692+03:00  INFO 11012 --- [kafka] [SpringApplicationShutdownHook] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for kafka-producer-1 unregistered
2025-01-15T14:30:35.696+03:00  INFO 11012 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T14:30:35.702+03:00  INFO 11012 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T14:30:35.723+03:00  INFO 11012 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T14:56:02.190+03:00  INFO 17772 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 17772 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T14:56:02.193+03:00  INFO 17772 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T14:56:02.277+03:00  INFO 17772 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T14:56:02.278+03:00  INFO 17772 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T14:56:03.524+03:00  INFO 17772 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T14:56:03.615+03:00  INFO 17772 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 80 ms. Found 5 JPA repository interfaces.
2025-01-15T14:56:04.782+03:00  INFO 17772 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T14:56:04.811+03:00  INFO 17772 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T14:56:04.811+03:00  INFO 17772 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T14:56:04.883+03:00  INFO 17772 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T14:56:04.883+03:00  INFO 17772 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2605 ms
2025-01-15T14:56:05.152+03:00  INFO 17772 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T14:56:05.247+03:00  INFO 17772 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T14:56:05.328+03:00  INFO 17772 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T14:56:05.832+03:00  INFO 17772 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T14:56:05.883+03:00  INFO 17772 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T14:56:06.290+03:00  INFO 17772 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@70406a61
2025-01-15T14:56:06.293+03:00  INFO 17772 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T14:56:07.596+03:00  INFO 17772 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T14:56:07.601+03:00  INFO 17772 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T14:56:08.473+03:00  WARN 17772 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T14:56:09.076+03:00  INFO 17772 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T14:56:09.172+03:00  INFO 17772 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T14:56:09.243+03:00  INFO 17772 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T14:56:09.311+03:00  INFO 17772 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T14:56:09.656+03:00  INFO 17772 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T14:56:09.656+03:00  INFO 17772 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T14:56:09.656+03:00  INFO 17772 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736942169653
2025-01-15T14:56:09.662+03:00  INFO 17772 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T14:56:09.690+03:00  INFO 17772 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 8.302 seconds (process running for 9.082)
2025-01-15T14:56:09.918+03:00  INFO 17772 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T14:56:09.919+03:00  INFO 17772 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T14:56:09.969+03:00  INFO 17772 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T14:56:09.969+03:00  INFO 17772 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T14:56:09.969+03:00  INFO 17772 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736942169969
2025-01-15T14:56:09.970+03:00  INFO 17772 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T14:56:10.341+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T14:56:10.341+03:00  INFO 17772 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T14:56:10.342+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T14:56:10.342+03:00  INFO 17772 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T14:56:10.347+03:00  INFO 17772 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T14:56:10.347+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T14:56:10.414+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-6ed6d905-3f12-4c6b-ab30-8f50bdaf585a
2025-01-15T14:56:10.416+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T14:56:10.416+03:00  INFO 17772 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-01888c47-da6c-423f-8b70-db576d9ab361
2025-01-15T14:56:10.416+03:00  INFO 17772 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T14:56:13.422+03:00  INFO 17772 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=26, memberId='consumer-dynamic-group-2-01888c47-da6c-423f-8b70-db576d9ab361', protocol='range'}
2025-01-15T14:56:13.422+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=71, memberId='consumer-my-group-1-6ed6d905-3f12-4c6b-ab30-8f50bdaf585a', protocol='range'}
2025-01-15T14:56:13.436+03:00  INFO 17772 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 26: {consumer-dynamic-group-2-01888c47-da6c-423f-8b70-db576d9ab361=Assignment(partitions=[test-topic-0])}
2025-01-15T14:56:13.436+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 71: {consumer-my-group-1-6ed6d905-3f12-4c6b-ab30-8f50bdaf585a=Assignment(partitions=[test-topic-0])}
2025-01-15T14:56:13.451+03:00  INFO 17772 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=26, memberId='consumer-dynamic-group-2-01888c47-da6c-423f-8b70-db576d9ab361', protocol='range'}
2025-01-15T14:56:13.451+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=71, memberId='consumer-my-group-1-6ed6d905-3f12-4c6b-ab30-8f50bdaf585a', protocol='range'}
2025-01-15T14:56:13.451+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T14:56:13.451+03:00  INFO 17772 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T14:56:13.454+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T14:56:13.454+03:00  INFO 17772 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T14:56:13.467+03:00  INFO 17772 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T14:56:13.478+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T14:56:13.478+03:00  INFO 17772 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T14:56:13.481+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T14:56:24.890+03:00  INFO 17772 --- [kafka] [http-nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T14:56:24.892+03:00  INFO 17772 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T14:56:24.895+03:00  INFO 17772 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 3 ms
2025-01-15T14:56:26.318+03:00  INFO 17772 --- [kafka] [http-nio-8080-exec-3] o.springdoc.api.AbstractOpenApiResource  : Init duration for springdoc-openapi is: 654 ms
2025-01-15T15:01:43.519+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T15:01:43.520+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T15:01:43.520+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-6ed6d905-3f12-4c6b-ab30-8f50bdaf585a sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T15:01:43.521+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:01:43.521+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:01:43.521+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T15:01:43.523+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:01:43.523+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:01:43.745+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T15:01:43.746+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T15:01:43.746+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T15:01:43.746+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T15:01:43.756+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T15:01:43.757+03:00  INFO 17772 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T15:01:43.779+03:00  INFO 17772 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:01:43.785+03:00  INFO 17772 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T15:01:43.805+03:00  INFO 17772 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T15:01:47.987+03:00  INFO 16016 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 16016 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T15:01:47.991+03:00  INFO 16016 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T15:01:48.077+03:00  INFO 16016 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T15:01:48.078+03:00  INFO 16016 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T15:01:49.240+03:00  INFO 16016 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T15:01:49.325+03:00  INFO 16016 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 76 ms. Found 5 JPA repository interfaces.
2025-01-15T15:01:50.271+03:00  INFO 16016 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T15:01:50.291+03:00  INFO 16016 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T15:01:50.291+03:00  INFO 16016 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T15:01:50.353+03:00  INFO 16016 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T15:01:50.354+03:00  INFO 16016 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2275 ms
2025-01-15T15:01:50.631+03:00  INFO 16016 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T15:01:50.708+03:00  INFO 16016 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T15:01:50.776+03:00  INFO 16016 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T15:01:51.199+03:00  INFO 16016 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T15:01:51.235+03:00  INFO 16016 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T15:01:51.600+03:00  INFO 16016 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@6c8e978d
2025-01-15T15:01:51.603+03:00  INFO 16016 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T15:01:52.846+03:00  INFO 16016 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T15:01:52.849+03:00  INFO 16016 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:01:53.709+03:00  WARN 16016 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T15:01:54.360+03:00  INFO 16016 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T15:01:54.473+03:00  INFO 16016 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T15:01:54.530+03:00  INFO 16016 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:01:54.613+03:00  INFO 16016 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:01:54.939+03:00  INFO 16016 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:01:54.940+03:00  INFO 16016 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:01:54.940+03:00  INFO 16016 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736942514938
2025-01-15T15:01:54.946+03:00  INFO 16016 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T15:01:54.977+03:00  INFO 16016 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.909 seconds (process running for 8.689)
2025-01-15T15:01:55.242+03:00  INFO 16016 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:01:55.243+03:00  INFO 16016 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:01:55.287+03:00  INFO 16016 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:01:55.288+03:00  INFO 16016 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:01:55.288+03:00  INFO 16016 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736942515287
2025-01-15T15:01:55.289+03:00  INFO 16016 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T15:01:55.705+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:01:55.705+03:00  INFO 16016 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:01:55.706+03:00  INFO 16016 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T15:01:55.706+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T15:01:55.713+03:00  INFO 16016 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:01:55.713+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:01:55.786+03:00  INFO 16016 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-9debeb0c-c6eb-49bf-9e4d-a999b56e6e91
2025-01-15T15:01:55.787+03:00  INFO 16016 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:01:55.791+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-8f956470-0b85-4ae4-9f7e-00195f87b9ff
2025-01-15T15:01:55.791+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:01:58.801+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=73, memberId='consumer-my-group-1-8f956470-0b85-4ae4-9f7e-00195f87b9ff', protocol='range'}
2025-01-15T15:01:58.809+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 73: {consumer-my-group-1-8f956470-0b85-4ae4-9f7e-00195f87b9ff=Assignment(partitions=[test-topic-0])}
2025-01-15T15:01:58.823+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=73, memberId='consumer-my-group-1-8f956470-0b85-4ae4-9f7e-00195f87b9ff', protocol='range'}
2025-01-15T15:01:58.823+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T15:01:58.825+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T15:01:58.840+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T15:01:58.841+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T15:02:05.389+03:00  INFO 16016 --- [kafka] [http-nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T15:02:05.389+03:00  INFO 16016 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T15:02:05.391+03:00  INFO 16016 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 2 ms
2025-01-15T15:02:07.053+03:00  INFO 16016 --- [kafka] [http-nio-8080-exec-3] o.springdoc.api.AbstractOpenApiResource  : Init duration for springdoc-openapi is: 689 ms
2025-01-15T15:02:28.497+03:00  INFO 16016 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=27, memberId='consumer-dynamic-group-2-9debeb0c-c6eb-49bf-9e4d-a999b56e6e91', protocol='range'}
2025-01-15T15:02:28.497+03:00  INFO 16016 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 27: {consumer-dynamic-group-2-9debeb0c-c6eb-49bf-9e4d-a999b56e6e91=Assignment(partitions=[test-topic-0])}
2025-01-15T15:02:28.510+03:00  INFO 16016 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=27, memberId='consumer-dynamic-group-2-9debeb0c-c6eb-49bf-9e4d-a999b56e6e91', protocol='range'}
2025-01-15T15:02:28.511+03:00  INFO 16016 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T15:02:28.511+03:00  INFO 16016 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T15:02:28.515+03:00  INFO 16016 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T15:02:28.520+03:00  INFO 16016 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T15:06:43.790+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T15:06:43.791+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T15:06:43.791+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-8f956470-0b85-4ae4-9f7e-00195f87b9ff sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T15:06:43.792+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:06:43.792+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:06:43.792+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T15:06:43.794+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:06:43.794+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:06:43.942+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T15:06:43.942+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T15:06:43.943+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T15:06:43.943+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T15:06:43.951+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T15:06:43.951+03:00  INFO 16016 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T15:06:43.974+03:00  INFO 16016 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:06:43.978+03:00  INFO 16016 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T15:06:43.998+03:00  INFO 16016 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T15:06:48.315+03:00  INFO 21624 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 21624 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T15:06:48.319+03:00  INFO 21624 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T15:06:48.400+03:00  INFO 21624 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T15:06:48.401+03:00  INFO 21624 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T15:06:49.773+03:00  INFO 21624 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T15:06:49.874+03:00  INFO 21624 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 88 ms. Found 5 JPA repository interfaces.
2025-01-15T15:06:50.981+03:00  INFO 21624 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T15:06:51.004+03:00  INFO 21624 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T15:06:51.004+03:00  INFO 21624 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T15:06:51.082+03:00  INFO 21624 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T15:06:51.082+03:00  INFO 21624 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2681 ms
2025-01-15T15:06:51.379+03:00  INFO 21624 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T15:06:51.479+03:00  INFO 21624 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T15:06:51.552+03:00  INFO 21624 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T15:06:51.979+03:00  INFO 21624 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T15:06:52.012+03:00  INFO 21624 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T15:06:52.370+03:00  INFO 21624 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@6c8e978d
2025-01-15T15:06:52.373+03:00  INFO 21624 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T15:06:53.577+03:00  INFO 21624 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T15:06:53.581+03:00  INFO 21624 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:06:54.414+03:00  WARN 21624 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T15:06:55.043+03:00  INFO 21624 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T15:06:55.142+03:00  INFO 21624 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T15:06:55.193+03:00  INFO 21624 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:06:55.251+03:00  INFO 21624 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:06:55.578+03:00  INFO 21624 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:06:55.578+03:00  INFO 21624 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:06:55.578+03:00  INFO 21624 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736942815576
2025-01-15T15:06:55.583+03:00  INFO 21624 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T15:06:55.609+03:00  INFO 21624 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 8.135 seconds (process running for 8.965)
2025-01-15T15:06:55.837+03:00  INFO 21624 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:06:55.837+03:00  INFO 21624 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:06:55.889+03:00  INFO 21624 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:06:55.889+03:00  INFO 21624 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:06:55.889+03:00  INFO 21624 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736942815889
2025-01-15T15:06:55.890+03:00  INFO 21624 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T15:06:56.240+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:06:56.240+03:00  INFO 21624 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:06:56.241+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T15:06:56.241+03:00  INFO 21624 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T15:06:56.247+03:00  INFO 21624 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:06:56.247+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:06:56.306+03:00  INFO 21624 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-3eef87c2-ffd7-418b-bcac-7dc76203998d
2025-01-15T15:06:56.306+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-5aced3ae-2979-4cf2-a6d9-52074e4c5c0d
2025-01-15T15:06:56.307+03:00  INFO 21624 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:06:56.307+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:06:59.311+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=75, memberId='consumer-my-group-1-5aced3ae-2979-4cf2-a6d9-52074e4c5c0d', protocol='range'}
2025-01-15T15:06:59.321+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 75: {consumer-my-group-1-5aced3ae-2979-4cf2-a6d9-52074e4c5c0d=Assignment(partitions=[test-topic-0])}
2025-01-15T15:06:59.335+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=75, memberId='consumer-my-group-1-5aced3ae-2979-4cf2-a6d9-52074e4c5c0d', protocol='range'}
2025-01-15T15:06:59.335+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T15:06:59.337+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T15:06:59.352+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T15:06:59.354+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T15:07:02.883+03:00  INFO 21624 --- [kafka] [http-nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T15:07:02.883+03:00  INFO 21624 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T15:07:02.885+03:00  INFO 21624 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 2 ms
2025-01-15T15:07:04.280+03:00  INFO 21624 --- [kafka] [http-nio-8080-exec-4] o.springdoc.api.AbstractOpenApiResource  : Init duration for springdoc-openapi is: 824 ms
2025-01-15T15:07:28.555+03:00  INFO 21624 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=28, memberId='consumer-dynamic-group-2-3eef87c2-ffd7-418b-bcac-7dc76203998d', protocol='range'}
2025-01-15T15:07:28.555+03:00  INFO 21624 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 28: {consumer-dynamic-group-2-3eef87c2-ffd7-418b-bcac-7dc76203998d=Assignment(partitions=[test-topic-0])}
2025-01-15T15:07:28.567+03:00  INFO 21624 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=28, memberId='consumer-dynamic-group-2-3eef87c2-ffd7-418b-bcac-7dc76203998d', protocol='range'}
2025-01-15T15:07:28.567+03:00  INFO 21624 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T15:07:28.568+03:00  INFO 21624 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T15:07:28.571+03:00  INFO 21624 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T15:07:28.575+03:00  INFO 21624 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T15:08:31.889+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T15:08:31.890+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T15:08:31.891+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-5aced3ae-2979-4cf2-a6d9-52074e4c5c0d sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T15:08:31.892+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:08:31.893+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:08:31.893+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T15:08:31.895+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:08:31.895+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:08:32.232+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T15:08:32.232+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T15:08:32.232+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T15:08:32.232+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T15:08:32.240+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T15:08:32.242+03:00  INFO 21624 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T15:08:32.255+03:00  INFO 21624 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:08:32.261+03:00  INFO 21624 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T15:08:32.277+03:00  INFO 21624 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T15:08:36.026+03:00  INFO 21424 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 21424 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T15:08:36.029+03:00  INFO 21424 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T15:08:36.103+03:00  INFO 21424 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T15:08:36.103+03:00  INFO 21424 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T15:08:37.210+03:00  INFO 21424 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T15:08:37.306+03:00  INFO 21424 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 84 ms. Found 5 JPA repository interfaces.
2025-01-15T15:08:38.225+03:00  INFO 21424 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T15:08:38.243+03:00  INFO 21424 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T15:08:38.244+03:00  INFO 21424 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T15:08:38.305+03:00  INFO 21424 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T15:08:38.305+03:00  INFO 21424 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2201 ms
2025-01-15T15:08:38.566+03:00  INFO 21424 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T15:08:38.643+03:00  INFO 21424 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T15:08:38.706+03:00  INFO 21424 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T15:08:39.134+03:00  INFO 21424 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T15:08:39.173+03:00  INFO 21424 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T15:08:39.556+03:00  INFO 21424 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@37fb9a92
2025-01-15T15:08:39.559+03:00  INFO 21424 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T15:08:40.733+03:00  INFO 21424 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T15:08:40.736+03:00  INFO 21424 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:08:41.595+03:00  WARN 21424 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T15:08:42.279+03:00  INFO 21424 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T15:08:42.381+03:00  INFO 21424 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T15:08:42.428+03:00  INFO 21424 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:08:42.478+03:00  INFO 21424 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:08:42.958+03:00  INFO 21424 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:08:42.958+03:00  INFO 21424 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:08:42.958+03:00  INFO 21424 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736942922957
2025-01-15T15:08:42.963+03:00  INFO 21424 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T15:08:42.989+03:00  INFO 21424 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.559 seconds (process running for 8.323)
2025-01-15T15:08:43.221+03:00  INFO 21424 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:08:43.222+03:00  INFO 21424 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:08:43.273+03:00  INFO 21424 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:08:43.273+03:00  INFO 21424 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:08:43.273+03:00  INFO 21424 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736942923273
2025-01-15T15:08:43.274+03:00  INFO 21424 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T15:08:43.683+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:08:43.683+03:00  INFO 21424 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:08:43.685+03:00  INFO 21424 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T15:08:43.685+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T15:08:43.689+03:00  INFO 21424 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:08:43.689+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:08:43.749+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-0f67bd09-759c-4f30-a8a7-39e93deaea00
2025-01-15T15:08:43.750+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:08:43.753+03:00  INFO 21424 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-aa984de0-914e-4d17-873f-609233ecdb5a
2025-01-15T15:08:43.754+03:00  INFO 21424 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:08:46.756+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=77, memberId='consumer-my-group-1-0f67bd09-759c-4f30-a8a7-39e93deaea00', protocol='range'}
2025-01-15T15:08:46.766+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 77: {consumer-my-group-1-0f67bd09-759c-4f30-a8a7-39e93deaea00=Assignment(partitions=[test-topic-0])}
2025-01-15T15:08:46.781+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=77, memberId='consumer-my-group-1-0f67bd09-759c-4f30-a8a7-39e93deaea00', protocol='range'}
2025-01-15T15:08:46.781+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T15:08:46.785+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T15:08:46.803+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T15:08:46.805+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T15:08:48.779+03:00  INFO 21424 --- [kafka] [http-nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T15:08:48.779+03:00  INFO 21424 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T15:08:48.781+03:00  INFO 21424 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 1 ms
2025-01-15T15:08:50.718+03:00  INFO 21424 --- [kafka] [http-nio-8080-exec-4] o.springdoc.api.AbstractOpenApiResource  : Init duration for springdoc-openapi is: 973 ms
2025-01-15T15:09:16.577+03:00  INFO 21424 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=29, memberId='consumer-dynamic-group-2-aa984de0-914e-4d17-873f-609233ecdb5a', protocol='range'}
2025-01-15T15:09:16.578+03:00  INFO 21424 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 29: {consumer-dynamic-group-2-aa984de0-914e-4d17-873f-609233ecdb5a=Assignment(partitions=[test-topic-0])}
2025-01-15T15:09:16.589+03:00  INFO 21424 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=29, memberId='consumer-dynamic-group-2-aa984de0-914e-4d17-873f-609233ecdb5a', protocol='range'}
2025-01-15T15:09:16.590+03:00  INFO 21424 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T15:09:16.590+03:00  INFO 21424 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T15:09:16.595+03:00  INFO 21424 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T15:09:16.600+03:00  INFO 21424 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T15:09:25.560+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T15:09:25.561+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T15:09:25.561+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-0f67bd09-759c-4f30-a8a7-39e93deaea00 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T15:09:25.562+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:09:25.562+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:09:25.562+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T15:09:25.564+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:09:25.565+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:09:25.730+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T15:09:25.730+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T15:09:25.730+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T15:09:25.730+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T15:09:25.738+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T15:09:25.739+03:00  INFO 21424 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T15:09:25.765+03:00  INFO 21424 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:09:25.769+03:00  INFO 21424 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T15:09:25.794+03:00  INFO 21424 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T15:09:29.720+03:00  INFO 21492 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 21492 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T15:09:29.724+03:00  INFO 21492 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T15:09:29.815+03:00  INFO 21492 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T15:09:29.816+03:00  INFO 21492 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T15:09:30.943+03:00  INFO 21492 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T15:09:31.036+03:00  INFO 21492 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 82 ms. Found 5 JPA repository interfaces.
2025-01-15T15:09:32.032+03:00  INFO 21492 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T15:09:32.052+03:00  INFO 21492 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T15:09:32.053+03:00  INFO 21492 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T15:09:32.117+03:00  INFO 21492 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T15:09:32.117+03:00  INFO 21492 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2301 ms
2025-01-15T15:09:32.396+03:00  INFO 21492 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T15:09:32.468+03:00  INFO 21492 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T15:09:32.525+03:00  INFO 21492 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T15:09:32.936+03:00  INFO 21492 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T15:09:32.967+03:00  INFO 21492 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T15:09:33.316+03:00  INFO 21492 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@3584e97e
2025-01-15T15:09:33.319+03:00  INFO 21492 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T15:09:34.508+03:00  INFO 21492 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T15:09:34.512+03:00  INFO 21492 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:09:35.349+03:00  WARN 21492 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T15:09:36.042+03:00  INFO 21492 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T15:09:36.160+03:00  INFO 21492 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T15:09:36.220+03:00  INFO 21492 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:09:36.321+03:00  INFO 21492 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:09:36.684+03:00  INFO 21492 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:09:36.684+03:00  INFO 21492 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:09:36.684+03:00  INFO 21492 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736942976683
2025-01-15T15:09:36.689+03:00  INFO 21492 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T15:09:36.719+03:00  INFO 21492 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.591 seconds (process running for 8.303)
2025-01-15T15:09:36.968+03:00  INFO 21492 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:09:36.968+03:00  INFO 21492 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:09:37.015+03:00  INFO 21492 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:09:37.015+03:00  INFO 21492 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:09:37.015+03:00  INFO 21492 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736942977015
2025-01-15T15:09:37.015+03:00  INFO 21492 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T15:09:37.373+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:09:37.373+03:00  INFO 21492 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:09:37.374+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T15:09:37.374+03:00  INFO 21492 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T15:09:37.378+03:00  INFO 21492 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:09:37.378+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:09:37.438+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-12aec240-9eb3-4ad6-8709-96186e74f9fb
2025-01-15T15:09:37.438+03:00  INFO 21492 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-8038c4ed-c0c9-4f2f-be25-b78b722f68c2
2025-01-15T15:09:37.439+03:00  INFO 21492 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:09:37.439+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:09:37.676+03:00  INFO 21492 --- [kafka] [http-nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T15:09:37.676+03:00  INFO 21492 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T15:09:37.678+03:00  INFO 21492 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 2 ms
2025-01-15T15:09:38.924+03:00  INFO 21492 --- [kafka] [http-nio-8080-exec-4] o.springdoc.api.AbstractOpenApiResource  : Init duration for springdoc-openapi is: 750 ms
2025-01-15T15:09:40.445+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=79, memberId='consumer-my-group-1-12aec240-9eb3-4ad6-8709-96186e74f9fb', protocol='range'}
2025-01-15T15:09:40.453+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 79: {consumer-my-group-1-12aec240-9eb3-4ad6-8709-96186e74f9fb=Assignment(partitions=[test-topic-0])}
2025-01-15T15:09:40.465+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=79, memberId='consumer-my-group-1-12aec240-9eb3-4ad6-8709-96186e74f9fb', protocol='range'}
2025-01-15T15:09:40.466+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T15:09:40.468+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T15:09:40.481+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T15:09:40.482+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T15:10:10.585+03:00  INFO 21492 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=30, memberId='consumer-dynamic-group-2-8038c4ed-c0c9-4f2f-be25-b78b722f68c2', protocol='range'}
2025-01-15T15:10:10.585+03:00  INFO 21492 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 30: {consumer-dynamic-group-2-8038c4ed-c0c9-4f2f-be25-b78b722f68c2=Assignment(partitions=[test-topic-0])}
2025-01-15T15:10:10.597+03:00  INFO 21492 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=30, memberId='consumer-dynamic-group-2-8038c4ed-c0c9-4f2f-be25-b78b722f68c2', protocol='range'}
2025-01-15T15:10:10.597+03:00  INFO 21492 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T15:10:10.597+03:00  INFO 21492 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T15:10:10.603+03:00  INFO 21492 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T15:10:10.608+03:00  INFO 21492 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T15:10:26.515+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T15:10:26.516+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T15:10:26.517+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-12aec240-9eb3-4ad6-8709-96186e74f9fb sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T15:10:26.518+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:10:26.518+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:10:26.518+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T15:10:26.520+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:10:26.520+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:10:26.989+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T15:10:26.990+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T15:10:26.990+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T15:10:26.990+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T15:10:26.997+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T15:10:26.998+03:00  INFO 21492 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T15:10:27.019+03:00  INFO 21492 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:10:27.025+03:00  INFO 21492 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T15:10:27.046+03:00  INFO 21492 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T15:10:30.684+03:00  INFO 16988 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 16988 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T15:10:30.688+03:00  INFO 16988 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T15:10:30.776+03:00  INFO 16988 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T15:10:30.777+03:00  INFO 16988 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T15:10:31.851+03:00  INFO 16988 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T15:10:31.947+03:00  INFO 16988 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 83 ms. Found 5 JPA repository interfaces.
2025-01-15T15:10:32.936+03:00  INFO 16988 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T15:10:32.955+03:00  INFO 16988 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T15:10:32.956+03:00  INFO 16988 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T15:10:33.013+03:00  INFO 16988 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T15:10:33.013+03:00  INFO 16988 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2235 ms
2025-01-15T15:10:33.281+03:00  INFO 16988 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T15:10:33.358+03:00  INFO 16988 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T15:10:33.414+03:00  INFO 16988 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T15:10:33.815+03:00  INFO 16988 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T15:10:33.846+03:00  INFO 16988 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T15:10:34.177+03:00  INFO 16988 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@37fb9a92
2025-01-15T15:10:34.180+03:00  INFO 16988 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T15:10:35.352+03:00  INFO 16988 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T15:10:35.356+03:00  INFO 16988 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:10:36.216+03:00  WARN 16988 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T15:10:36.848+03:00  INFO 16988 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T15:10:36.971+03:00  INFO 16988 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T15:10:37.032+03:00  INFO 16988 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:10:37.119+03:00  INFO 16988 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:10:37.506+03:00  INFO 16988 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:10:37.506+03:00  INFO 16988 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:10:37.506+03:00  INFO 16988 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736943037504
2025-01-15T15:10:37.513+03:00  INFO 16988 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T15:10:37.542+03:00  INFO 16988 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.536 seconds (process running for 8.255)
2025-01-15T15:10:37.794+03:00  INFO 16988 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:10:37.795+03:00  INFO 16988 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:10:37.844+03:00  INFO 16988 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:10:37.844+03:00  INFO 16988 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:10:37.844+03:00  INFO 16988 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736943037844
2025-01-15T15:10:37.845+03:00  INFO 16988 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T15:10:38.191+03:00  INFO 16988 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:10:38.191+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:10:38.193+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T15:10:38.193+03:00  INFO 16988 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T15:10:38.197+03:00  INFO 16988 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:10:38.197+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:10:38.247+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-0ddbe69a-040c-4b9b-855d-679cc1842fae
2025-01-15T15:10:38.248+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:10:38.249+03:00  INFO 16988 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-9a833e77-bc8a-44d0-b80e-127b12da1e2c
2025-01-15T15:10:38.250+03:00  INFO 16988 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:10:41.254+03:00  INFO 16988 --- [kafka] [http-nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T15:10:41.254+03:00  INFO 16988 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T15:10:41.256+03:00  INFO 16988 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 2 ms
2025-01-15T15:10:41.258+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=81, memberId='consumer-my-group-1-0ddbe69a-040c-4b9b-855d-679cc1842fae', protocol='range'}
2025-01-15T15:10:41.271+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 81: {consumer-my-group-1-0ddbe69a-040c-4b9b-855d-679cc1842fae=Assignment(partitions=[test-topic-0])}
2025-01-15T15:10:41.281+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=81, memberId='consumer-my-group-1-0ddbe69a-040c-4b9b-855d-679cc1842fae', protocol='range'}
2025-01-15T15:10:41.282+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T15:10:41.285+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T15:10:41.300+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T15:10:41.303+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T15:10:42.692+03:00  INFO 16988 --- [kafka] [http-nio-8080-exec-5] o.springdoc.api.AbstractOpenApiResource  : Init duration for springdoc-openapi is: 553 ms
2025-01-15T15:11:00.485+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T15:11:00.486+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T15:11:00.487+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-0ddbe69a-040c-4b9b-855d-679cc1842fae sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T15:11:00.488+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:11:00.488+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:11:00.488+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T15:11:00.489+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:11:00.490+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:11:00.568+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T15:11:00.568+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T15:11:00.569+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T15:11:00.569+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T15:11:00.576+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T15:11:00.576+03:00  INFO 16988 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T15:11:00.596+03:00  INFO 16988 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:11:00.602+03:00  INFO 16988 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T15:11:00.622+03:00  INFO 16988 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T15:11:04.287+03:00  INFO 20276 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 20276 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T15:11:04.291+03:00  INFO 20276 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T15:11:04.365+03:00  INFO 20276 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T15:11:04.365+03:00  INFO 20276 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T15:11:05.435+03:00  INFO 20276 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T15:11:05.526+03:00  INFO 20276 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 79 ms. Found 5 JPA repository interfaces.
2025-01-15T15:11:06.419+03:00  INFO 20276 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T15:11:06.439+03:00  INFO 20276 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T15:11:06.440+03:00  INFO 20276 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T15:11:06.503+03:00  INFO 20276 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T15:11:06.504+03:00  INFO 20276 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2138 ms
2025-01-15T15:11:06.736+03:00  INFO 20276 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T15:11:06.811+03:00  INFO 20276 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T15:11:06.868+03:00  INFO 20276 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T15:11:07.277+03:00  INFO 20276 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T15:11:07.313+03:00  INFO 20276 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T15:11:07.644+03:00  INFO 20276 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@af82e3a
2025-01-15T15:11:07.646+03:00  INFO 20276 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T15:11:08.840+03:00  INFO 20276 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T15:11:08.843+03:00  INFO 20276 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:11:09.689+03:00  WARN 20276 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T15:11:10.341+03:00  INFO 20276 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T15:11:10.445+03:00  INFO 20276 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T15:11:10.506+03:00  INFO 20276 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:11:10.580+03:00  INFO 20276 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:11:10.951+03:00  INFO 20276 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:11:10.952+03:00  INFO 20276 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:11:10.952+03:00  INFO 20276 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736943070950
2025-01-15T15:11:10.957+03:00  INFO 20276 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T15:11:10.988+03:00  INFO 20276 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.339 seconds (process running for 8.094)
2025-01-15T15:11:11.221+03:00  INFO 20276 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:11:11.222+03:00  INFO 20276 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:11:11.291+03:00  INFO 20276 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:11:11.292+03:00  INFO 20276 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:11:11.292+03:00  INFO 20276 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736943071291
2025-01-15T15:11:11.293+03:00  INFO 20276 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T15:11:11.648+03:00  INFO 20276 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:11:11.648+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:11:11.649+03:00  INFO 20276 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T15:11:11.649+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T15:11:11.654+03:00  INFO 20276 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:11:11.654+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:11:11.717+03:00  INFO 20276 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-f8dbd41e-fd6e-4561-b468-c4551691f33f
2025-01-15T15:11:11.717+03:00  INFO 20276 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:11:11.722+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-b2fbc74f-8f94-4ef2-a7a9-b0421c0b3b2c
2025-01-15T15:11:11.723+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:11:14.731+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=83, memberId='consumer-my-group-1-b2fbc74f-8f94-4ef2-a7a9-b0421c0b3b2c', protocol='range'}
2025-01-15T15:11:14.739+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 83: {consumer-my-group-1-b2fbc74f-8f94-4ef2-a7a9-b0421c0b3b2c=Assignment(partitions=[test-topic-0])}
2025-01-15T15:11:14.752+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=83, memberId='consumer-my-group-1-b2fbc74f-8f94-4ef2-a7a9-b0421c0b3b2c', protocol='range'}
2025-01-15T15:11:14.752+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T15:11:14.755+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T15:11:14.769+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T15:11:14.771+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T15:11:23.637+03:00  INFO 20276 --- [kafka] [http-nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T15:11:23.637+03:00  INFO 20276 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T15:11:23.640+03:00  INFO 20276 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 3 ms
2025-01-15T15:11:25.028+03:00  INFO 20276 --- [kafka] [http-nio-8080-exec-4] o.springdoc.api.AbstractOpenApiResource  : Init duration for springdoc-openapi is: 712 ms
2025-01-15T15:11:55.594+03:00  INFO 20276 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=32, memberId='consumer-dynamic-group-2-f8dbd41e-fd6e-4561-b468-c4551691f33f', protocol='range'}
2025-01-15T15:11:55.595+03:00  INFO 20276 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 32: {consumer-dynamic-group-2-f8dbd41e-fd6e-4561-b468-c4551691f33f=Assignment(partitions=[test-topic-0])}
2025-01-15T15:11:55.603+03:00  INFO 20276 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=32, memberId='consumer-dynamic-group-2-f8dbd41e-fd6e-4561-b468-c4551691f33f', protocol='range'}
2025-01-15T15:11:55.604+03:00  INFO 20276 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T15:11:55.604+03:00  INFO 20276 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T15:11:55.606+03:00  INFO 20276 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T15:11:55.610+03:00  INFO 20276 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T15:16:25.694+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T15:16:25.695+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T15:16:25.696+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-b2fbc74f-8f94-4ef2-a7a9-b0421c0b3b2c sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T15:16:25.697+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:16:25.697+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:16:25.697+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T15:16:25.699+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:16:25.699+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:16:26.045+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T15:16:26.046+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T15:16:26.046+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T15:16:26.046+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T15:16:26.053+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T15:16:26.053+03:00  INFO 20276 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T15:16:26.076+03:00  INFO 20276 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:16:26.081+03:00  INFO 20276 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T15:16:26.104+03:00  INFO 20276 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T15:16:30.049+03:00  INFO 16088 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Starting KafkaApplication using Java 22.0.1 with PID 16088 (C:\Users\45868582848\IdeaProjects\kafka\target\classes started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T15:16:30.051+03:00  INFO 16088 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : No active profile set, falling back to 1 default profile: "default"
2025-01-15T15:16:30.143+03:00  INFO 16088 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
2025-01-15T15:16:30.144+03:00  INFO 16088 --- [kafka] [restartedMain] .e.DevToolsPropertyDefaultsPostProcessor : For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
2025-01-15T15:16:31.233+03:00  INFO 16088 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T15:16:31.314+03:00  INFO 16088 --- [kafka] [restartedMain] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 72 ms. Found 5 JPA repository interfaces.
2025-01-15T15:16:32.426+03:00  INFO 16088 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8080 (http)
2025-01-15T15:16:32.456+03:00  INFO 16088 --- [kafka] [restartedMain] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-01-15T15:16:32.457+03:00  INFO 16088 --- [kafka] [restartedMain] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.28]
2025-01-15T15:16:32.560+03:00  INFO 16088 --- [kafka] [restartedMain] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2025-01-15T15:16:32.560+03:00  INFO 16088 --- [kafka] [restartedMain] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2416 ms
2025-01-15T15:16:32.856+03:00  INFO 16088 --- [kafka] [restartedMain] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T15:16:32.930+03:00  INFO 16088 --- [kafka] [restartedMain] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T15:16:33.005+03:00  INFO 16088 --- [kafka] [restartedMain] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T15:16:33.503+03:00  INFO 16088 --- [kafka] [restartedMain] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T15:16:33.536+03:00  INFO 16088 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T15:16:33.898+03:00  INFO 16088 --- [kafka] [restartedMain] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@558aeb27
2025-01-15T15:16:33.901+03:00  INFO 16088 --- [kafka] [restartedMain] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T15:16:35.047+03:00  INFO 16088 --- [kafka] [restartedMain] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T15:16:35.051+03:00  INFO 16088 --- [kafka] [restartedMain] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:16:35.937+03:00  WARN 16088 --- [kafka] [restartedMain] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T15:16:36.578+03:00  INFO 16088 --- [kafka] [restartedMain] o.s.b.d.a.OptionalLiveReloadServer       : LiveReload server is running on port 35729
2025-01-15T15:16:36.679+03:00  INFO 16088 --- [kafka] [restartedMain] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8080 (http) with context path '/'
2025-01-15T15:16:36.729+03:00  INFO 16088 --- [kafka] [restartedMain] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:16:36.796+03:00  INFO 16088 --- [kafka] [restartedMain] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:16:37.187+03:00  INFO 16088 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:16:37.187+03:00  INFO 16088 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:16:37.187+03:00  INFO 16088 --- [kafka] [restartedMain] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736943397185
2025-01-15T15:16:37.194+03:00  INFO 16088 --- [kafka] [restartedMain] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T15:16:37.231+03:00  INFO 16088 --- [kafka] [restartedMain] com.demo.kafka.KafkaApplication          : Started KafkaApplication in 7.826 seconds (process running for 8.533)
2025-01-15T15:16:37.498+03:00  INFO 16088 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T15:16:37.499+03:00  INFO 16088 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T15:16:37.535+03:00  INFO 16088 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T15:16:37.535+03:00  INFO 16088 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T15:16:37.535+03:00  INFO 16088 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736943397535
2025-01-15T15:16:37.536+03:00  INFO 16088 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T15:16:37.878+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:16:37.878+03:00  INFO 16088 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T15:16:37.879+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T15:16:37.879+03:00  INFO 16088 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T15:16:37.884+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:16:37.884+03:00  INFO 16088 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:16:37.949+03:00  INFO 16088 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-75d4ee93-b6df-4d30-a416-df45d9c60fdd
2025-01-15T15:16:37.949+03:00  INFO 16088 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T15:16:37.950+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-284437ab-0241-45db-bcf7-b32db4d9cc92
2025-01-15T15:16:37.951+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T15:16:40.955+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully joined group with generation Generation{generationId=85, memberId='consumer-my-group-1-284437ab-0241-45db-bcf7-b32db4d9cc92', protocol='range'}
2025-01-15T15:16:40.964+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Finished assignment for group at generation 85: {consumer-my-group-1-284437ab-0241-45db-bcf7-b32db4d9cc92=Assignment(partitions=[test-topic-0])}
2025-01-15T15:16:40.977+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Successfully synced group in generation Generation{generationId=85, memberId='consumer-my-group-1-284437ab-0241-45db-bcf7-b32db4d9cc92', protocol='range'}
2025-01-15T15:16:40.978+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T15:16:40.980+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Adding newly assigned partitions: test-topic-0
2025-01-15T15:16:40.997+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T15:16:40.998+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions assigned: [test-topic-0]
2025-01-15T15:16:51.461+03:00  INFO 16088 --- [kafka] [http-nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-01-15T15:16:51.461+03:00  INFO 16088 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2025-01-15T15:16:51.462+03:00  INFO 16088 --- [kafka] [http-nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 1 ms
2025-01-15T15:16:53.011+03:00  INFO 16088 --- [kafka] [http-nio-8080-exec-3] o.springdoc.api.AbstractOpenApiResource  : Init duration for springdoc-openapi is: 743 ms
2025-01-15T15:17:10.646+03:00  INFO 16088 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=33, memberId='consumer-dynamic-group-2-75d4ee93-b6df-4d30-a416-df45d9c60fdd', protocol='range'}
2025-01-15T15:17:10.646+03:00  INFO 16088 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 33: {consumer-dynamic-group-2-75d4ee93-b6df-4d30-a416-df45d9c60fdd=Assignment(partitions=[test-topic-0])}
2025-01-15T15:17:10.655+03:00  INFO 16088 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=33, memberId='consumer-dynamic-group-2-75d4ee93-b6df-4d30-a416-df45d9c60fdd', protocol='range'}
2025-01-15T15:17:10.656+03:00  INFO 16088 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T15:17:10.656+03:00  INFO 16088 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T15:17:10.659+03:00  INFO 16088 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T15:17:10.662+03:00  INFO 16088 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2 rack: null)], epoch=15}}
2025-01-15T15:18:08.714+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-my-group-1, groupId=my-group] Revoke previously assigned partitions test-topic-0
2025-01-15T15:18:08.715+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: partitions revoked: [test-topic-0]
2025-01-15T15:18:08.716+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-284437ab-0241-45db-bcf7-b32db4d9cc92 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T15:18:08.717+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:18:08.717+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:18:08.717+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T15:18:08.723+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T15:18:08.724+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T15:18:08.924+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T15:18:08.924+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T15:18:08.924+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T15:18:08.924+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T15:18:08.934+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T15:18:08.936+03:00  INFO 16088 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T15:18:08.962+03:00  INFO 16088 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T15:18:08.967+03:00  INFO 16088 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T15:18:08.986+03:00  INFO 16088 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
2025-01-15T16:16:09.713+03:00  INFO 22228 --- [kafka] [main] com.demo.kafka.KafkaApplicationTests     : Starting KafkaApplicationTests using Java 22.0.1 with PID 22228 (started by 45868582848 in C:\Users\45868582848\IdeaProjects\kafka)
2025-01-15T16:16:09.715+03:00  INFO 22228 --- [kafka] [main] com.demo.kafka.KafkaApplicationTests     : No active profile set, falling back to 1 default profile: "default"
2025-01-15T16:16:10.681+03:00  INFO 22228 --- [kafka] [main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2025-01-15T16:16:10.750+03:00  INFO 22228 --- [kafka] [main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 60 ms. Found 5 JPA repository interfaces.
2025-01-15T16:16:11.424+03:00  INFO 22228 --- [kafka] [main] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2025-01-15T16:16:11.495+03:00  INFO 22228 --- [kafka] [main] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.5.2.Final
2025-01-15T16:16:11.534+03:00  INFO 22228 --- [kafka] [main] o.h.c.internal.RegionFactoryInitiator    : HHH000026: Second-level cache disabled
2025-01-15T16:16:11.918+03:00  INFO 22228 --- [kafka] [main] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2025-01-15T16:16:11.955+03:00  INFO 22228 --- [kafka] [main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-01-15T16:16:12.296+03:00  INFO 22228 --- [kafka] [main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@4ec0229c
2025-01-15T16:16:12.297+03:00  INFO 22228 --- [kafka] [main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-01-15T16:16:13.680+03:00  INFO 22228 --- [kafka] [main] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-01-15T16:16:13.683+03:00  INFO 22228 --- [kafka] [main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T16:16:14.571+03:00  WARN 22228 --- [kafka] [main] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-01-15T16:16:15.375+03:00  INFO 22228 --- [kafka] [main] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T16:16:15.437+03:00  INFO 22228 --- [kafka] [main] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T16:16:15.773+03:00  INFO 22228 --- [kafka] [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T16:16:15.774+03:00  INFO 22228 --- [kafka] [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T16:16:15.774+03:00  INFO 22228 --- [kafka] [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736946975772
2025-01-15T16:16:15.778+03:00  INFO 22228 --- [kafka] [main] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Subscribed to topic(s): test-topic
2025-01-15T16:16:15.804+03:00  INFO 22228 --- [kafka] [main] com.demo.kafka.KafkaApplicationTests     : Started KafkaApplicationTests in 6.527 seconds (process running for 7.69)
2025-01-15T16:16:16.059+03:00  INFO 22228 --- [kafka] [scheduling-1] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [https://tcdd-kafka-kafka-listener1-bootstrap-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dynamic-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dynamic-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = C:\Users\45868582848\Desktop\ssl\user.p12
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = C:\Users\45868582848\Desktop\ssl\ca.p12
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-01-15T16:16:16.061+03:00  INFO 22228 --- [kafka] [scheduling-1] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-01-15T16:16:16.132+03:00  INFO 22228 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.7.1
2025-01-15T16:16:16.132+03:00  INFO 22228 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: e2494e6ffb89f828
2025-01-15T16:16:16.133+03:00  INFO 22228 --- [kafka] [scheduling-1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1736946976132
2025-01-15T16:16:16.133+03:00  INFO 22228 --- [kafka] [scheduling-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Subscribed to topic(s): test-topic
2025-01-15T16:16:16.608+03:00  INFO 22228 --- [kafka] [noBeanNameSet-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T16:16:16.608+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-my-group-1, groupId=my-group] Cluster ID: 63qcyjAGSXekLTVKL9AVOw
2025-01-15T16:16:16.609+03:00  INFO 22228 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Discovered group coordinator tcdd-kafka-kafka-listener1-1-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483646 rack: null)
2025-01-15T16:16:16.613+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Discovered group coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null)
2025-01-15T16:16:16.622+03:00  INFO 22228 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T16:16:16.624+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T16:16:16.698+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: need to re-join with the given member-id: consumer-my-group-1-18146d33-a6c9-41b0-a23e-a0ffd2ec9c15
2025-01-15T16:16:16.699+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] (Re-)joining group
2025-01-15T16:16:16.699+03:00  INFO 22228 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Request joining group due to: need to re-join with the given member-id: consumer-dynamic-group-2-9430bc8c-ba7e-401c-bc8d-0a6b7c2cf74f
2025-01-15T16:16:16.699+03:00  INFO 22228 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] (Re-)joining group
2025-01-15T16:16:16.783+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Member consumer-my-group-1-18146d33-a6c9-41b0-a23e-a0ffd2ec9c15 sending LeaveGroup request to coordinator tcdd-kafka-kafka-listener1-2-tcdd-amq-streams-kafka.apps.ocp.tcdd.gov.tr:443 (id: 2147483645 rack: null) due to the consumer unsubscribed from all topics
2025-01-15T16:16:16.785+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T16:16:16.785+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T16:16:16.785+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Unsubscribed all topics or patterns and assigned partitions
2025-01-15T16:16:16.790+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-01-15T16:16:16.790+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-my-group-1, groupId=my-group] Request joining group due to: consumer pro-actively leaving the group
2025-01-15T16:16:19.705+03:00  INFO 22228 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully joined group with generation Generation{generationId=35, memberId='consumer-dynamic-group-2-9430bc8c-ba7e-401c-bc8d-0a6b7c2cf74f', protocol='range'}
2025-01-15T16:16:19.709+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics scheduler closed
2025-01-15T16:16:19.710+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-01-15T16:16:19.710+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-01-15T16:16:19.710+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.apache.kafka.common.metrics.Metrics    : Metrics reporters closed
2025-01-15T16:16:19.712+03:00  INFO 22228 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Finished assignment for group at generation 35: {consumer-dynamic-group-2-9430bc8c-ba7e-401c-bc8d-0a6b7c2cf74f=Assignment(partitions=[test-topic-0])}
2025-01-15T16:16:19.718+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-my-group-1 unregistered
2025-01-15T16:16:19.719+03:00  INFO 22228 --- [kafka] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : my-group: Consumer stopped
2025-01-15T16:16:19.725+03:00  INFO 22228 --- [kafka] [SpringApplicationShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2025-01-15T16:16:19.728+03:00  INFO 22228 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Successfully synced group in generation Generation{generationId=35, memberId='consumer-dynamic-group-2-9430bc8c-ba7e-401c-bc8d-0a6b7c2cf74f', protocol='range'}
2025-01-15T16:16:19.728+03:00  INFO 22228 --- [kafka] [noBeanNameSet-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-01-15T16:16:19.729+03:00  INFO 22228 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
2025-01-15T16:16:19.736+03:00  INFO 22228 --- [kafka] [noBeanNameSet-0-C-1] k.c.c.i.ConsumerRebalanceListenerInvoker : [Consumer clientId=consumer-dynamic-group-2, groupId=dynamic-group] Adding newly assigned partitions: test-topic-0
2025-01-15T16:16:19.747+03:00  INFO 22228 --- [kafka] [noBeanNameSet-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : dynamic-group: partitions assigned: [test-topic-0]
2025-01-15T16:16:19.752+03:00  INFO 22228 --- [kafka] [SpringApplicationShutdownHook] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown completed.
